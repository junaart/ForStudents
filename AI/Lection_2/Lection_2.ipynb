{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4d1f4b-b8ad-48d7-b123-cbbf3dc3be19",
   "metadata": {},
   "source": [
    "# Лекция 2. Введение в нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af793834-c968-47bb-864d-6c9d3181643e",
   "metadata": {},
   "source": [
    "1. Биологические нейронные сети.\n",
    "2. Математическая модель нейрона.\n",
    "3. Нейронная сеть. Алгоритмы обучения.\n",
    "4. Метрики качества классификации\n",
    "5. Примеры работы с нейросетями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace738e7-f9c2-420d-817e-b3f16d8381ed",
   "metadata": {},
   "source": [
    "## 1. Биологические нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af4b88-fb91-4f11-88ab-ec4c4fe73fa1",
   "metadata": {},
   "source": [
    "Идея построения искусственных нейронных сетей базируется на моделировании работы человеческого мозга. Элементом клеточной структуры мозга является особая биологическая клетка - **нейрон**.\n",
    "\n",
    "Нейрон состоит из тела клетки или *сомы* (soma), и двух внешних древоподобных ветвей. Это *аксон* (axon), представляющий собой выходной отросток нейрона и *дендриты* (dendrites) - входные элементы.\n",
    "\n",
    "Тело клетки включает *ядро* (nucleus), которое содержит информацию о наследственных свойствах, и *плазму*, обладающую молекулярными средствами для производства необходимых нейрону материалов.\n",
    "\n",
    "Нервная клетка существенно отличается от других биологических клеток по своему функциональному назначению. Нейрон выполняет прием, элементарное преобразование и дальнейшую передачу информации другим нейронам. Информация переносится в виде импульсов нервной активности, имеющей электро-химическую природу.\n",
    "\n",
    "Нейрон воспринимает сигналы (импульсы) от других нейронов через дендриты (приемники) и передает сигналы, сгенерированные телом клетки, вдоль аксона (передатчика), который в конце разветвляется на *волокна* (strands).  На окончании этих волокон находятся *синапсы* (synapses). Синапс соединяет волокно аксона одного нейрона и дендрит другого, обеспечивая передачу электрических импульсов между ними. \n",
    "\n",
    "Когда импульс достигает синаптического окончания нейрона-передатчика, высвобождаются определенные химические вещества, называемые *нейромедиаторами*. В зависимосчти от типа вырабатываемого вещества синапс может обладать возбуждающим или тормозящим действием. Нейромедиаторы проникают через синаптическое соединение, возбуждая или затормаживая способность нейрона-приемника генерировать электрические импульсы. Биологические синапсы могут настраиваться в зависимости от сигналов, проходящих через них. Таким образом, синапсы могут обучаться в зависимости от сигналов, проходящих через них. Эта зависимость от предыстории действует как память.\n",
    "\n",
    "*Биологическая нейронная сеть* представляет собой совокупность нейронов, соединенных между собой связями и образующими сетевую структуру.\n",
    "\n",
    "Кора головного мозга содержит около $10^{11}$ нейронов, каждый нейрон связан с $10^{3} - 10^{4}$ другими нейронами. В целом мозг человека содержит приблизительно от $10^{14}$ до $10^{15}$ взаимосвязей.\n",
    "\n",
    "![image](ris1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24667e-1edf-4338-b83c-d26231b0f244",
   "metadata": {},
   "source": [
    "Нейроны взаимодействуют посредством короткой серии импульсов, как правило, продолжительностью несколько миллисекунд. Частота импульсов может изменяться от нескольких единицдо сотен герц, что в миллион раз медленнее, чем самые быстрые электронные схемы.\n",
    "\n",
    "Однако скорость работы мозга определяет не столько частота импульсов, сколько возможность параллельной обработки информации. \n",
    "\n",
    "Также биологическая нейронная сеть способна к самоорганизации, самообучению, адаптации. \n",
    "\n",
    "Биологическая нейронная сеть обладает способностью к обобщению, классификации, абстрагированию, асоциации и т.д. Это позволяет решать практически все известные задачи.\n",
    "\n",
    "Также биологические нейронные сети обладают высокой надежностью: выход из строя даже 1-% нейроной в нервной системе не прерывает его работы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce62cd4-76dd-42c7-bce2-034a822796e2",
   "metadata": {},
   "source": [
    "# 2. Математическая модель нейрона"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81878e40-6a44-430e-abfc-f0463683dd7a",
   "metadata": {},
   "source": [
    "Искусственный нейрон имитирует в первом приближении свойства биологического нейрона. На вход искусственного нейрона поступает некоторое множество сигналов, каждый из которых является выходом другого нейрона. Каждый вход умножается на соответствующий вес, аналогичный синаптической силе, и все произведения суммируются, определяя уровень активации нейрона. \n",
    "\n",
    "На рисунке представлена модель, реализующая эту идею. \n",
    "\n",
    "![image](ris2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e55be9-46b1-4c84-8388-96ea880fef6a",
   "metadata": {},
   "source": [
    "Хотя сетевые парадигмы весьма разнообразны, в основе почти всех их лежит эта конфигурация. Здесь множество входных сигналов, обозначенных $x_1, x_2, \\ldots, x_n$, поступает на искусственный нейрон. Эти входные сигналы, в совокупности обозначаемые вектором X, соответствуют сигналам, приходящим в синапсы биологического нейрона. Каждый сигнал умножается на соответствующий вес $w_1, w_2,\\ldots, w_n$, и поступает на суммирующий блок, обозначенный $\\Sigma$. Каждый вес соответствует «силе» одной биологической синаптической связи. (Множество весов в совокупности обозначается вектором $W$.) Суммирующий блок, соответствующий телу биологического элемента, складывает взвешенные входы алгебраически, создавая выход, который мы будем называть NET:\n",
    "$$\\text{NET}=w_1x_1+w_2x_2+\\ldots+w_nx_n=\\sum_{i=1}^nw_ix_i$$\n",
    "\n",
    "В векторных обозначениях это может быть компактно записано следующим образом:\n",
    "$$\\text{NET} = X\\cdot W$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078f2a6-5a6a-4d97-b47c-8dd4dc7a8ac9",
   "metadata": {},
   "source": [
    "Но на этом работа в нейроне не заканчивается, выход с сумматора $\\text{NET}$ подается на вход так называемой *активационной функции $F$*.\n",
    "\n",
    "![image](ris3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9267434-8948-4b32-b09a-85b12349b756",
   "metadata": {},
   "source": [
    "На рисунке блок, обозначенный F, принимает сигнал NET и выдает сигнал OUT. Если блок F сужает диапазон изменения величины NET так, что при любых значениях NET значения OUT принадлежат некоторому конечному интервалу, то $F4$ называется «сжимающей» функцией. В качестве «сжимающей» функции часто используется логистическая или «сигмоидальная» (S-образная) функция. Эта функция математически выражается как $F(x) = \\frac{1}{1 + e^{-x}}$. Таким образом,\n",
    "$$\\text{OUT}=\\frac{1}{1+e^{\\text{-NET}}}$$\n",
    "\n",
    "[WolframAlpha](https://www.wolframalpha.com/input?i=1%2F%281%2Be%5E%28-x%29%29).\n",
    "\n",
    "Диапазон выходных значений сигмоиды $\\text{OUT}\\in (0,1)$, рабочая область (в которой нейронная сеть может обучаться): $\\text{NET}\\in[-\\sqrt{3}, \\sqrt{3}]$. \n",
    "                                              \n",
    "\n",
    "В качестве других наиболее часто используемых активационных функций можно рассмотреть следующие:\n",
    " - гиперболический тангенс $\\text{OUT(x)}=tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$ [WolframAlpha](https://www.wolframalpha.com/input?i=tanh%28x%29), диапазон выходных значений: $x\\in[-1;1]$, рабочая область $x\\in [-2, 2]$;\n",
    "- функция ReLu (Rectified linear unit) $ReLu(x)=\\max(0,x)$, диапазон выходных значений и рабочая область: $[0, \\infty)$.\n",
    "\n",
    "Большой перечень активационных функций приведен по [ссылке](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%B0%D1%86%D0%B8%D0%B8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3339db4-4d81-4c64-a6c6-cfa354aff9d7",
   "metadata": {},
   "source": [
    "В последнее время наиболее часто используется именно активационная функция $ReLu$, т.к. она позволяет решить *проблему исчезающего градиента*, которая и опредедляет рабочую область активационных функций - дело в том, что в области насыщения активационной функции обучение нейронной сети становится очень медленным. Функция $ReLu$ не имеет насыщения и является наилучшим выбором для более глубоких архитектур (нейронных сетей, состоящих из большого количества слоев). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a80751c-0610-4c51-bff0-c5999613f4ac",
   "metadata": {},
   "source": [
    "Кроме рассмотренных активационных функций, для нейронов последнего слоя используют специальные активационные функции. Это  требуется для того, чтобы  преобразовать полученные сигналы в заданный условиями задачи диапазон (обычно это вероятность принадлежности к классу).\n",
    "\n",
    "Такая функция активации получила название *функция мягкого максимума для классификации*. Данная функция будет преобразовывать выход предыдущего слоя в вероятностные значения, чтобы выполнить итоговое предсказание класса:\n",
    "$$\\text{softmax}(k,x_1,x_2,\\ldots,x_n)=\\frac{e^{x_k}}{\\sum_{i=1}^ne^{x_i}}$$\n",
    "\n",
    "Данная функция будет принимать значение близкое к единице для такого $k$, при котором $x_k=\\max\\{x_1, x_2, \\ldots\\}$, для других же $k$ она будет принимать значение, близкое к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a92f9-d89f-4509-a4b0-8dd43aa66c7e",
   "metadata": {},
   "source": [
    "Рассмотренная простая модель искусственного нейрона игнорирует многие свойства своего биологического двойника. Например, она не принимает во внимание задержки во времени, которые воздействуют на динамику системы. Входные сигналы сразу же порождают выходной сигнал. И, что более важно, она не учитывает воздействий функции частотной модуляции или синхронизирующей функции биологического нейрона, которые ряд исследователей считают решающими.\n",
    "\n",
    "Несмотря на эти ограничения, сети, построенные из этих нейронов, обнаруживают свойства, сильно напоминающие биологическую систему. Только время и исследования смогут ответить на вопрос, являются ли подобные совпадения случайными или следствием того, что в модели верно схвачены важнейшие черты биологического нейрона."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c857df-f973-47ef-b32b-e6772dc647db",
   "metadata": {},
   "source": [
    "Реализуем класс для создания нейрона. При физической реализации нейрона среди входных сигналов выделяют специальный вход  порогового значения (bias - на него не подаются внешние сигналы, это настраиваемый параметр самого нейрона), который служит для формирования возможного ненулевого выхода нейрона при нулевых входах.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f80a633-8cab-4003-9cb1-286eb34d6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построение нейрона\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Функции активации\n",
    "def sigma(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "  return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0,x)\n",
    "    \n",
    "def softmax(k,x):\n",
    "    return np.exp(x[k])/np.exp(x).sum()\n",
    "\n",
    "#Класс нейрона\n",
    "class Neuron:\n",
    "  def __init__(self, weights, bias, f):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    self.f_active = f\n",
    "\n",
    "  def evaluate(self, inputs):\n",
    "    return self.f_active(np.dot(self.weights, inputs) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5230cff-1025-4406-9985-7ac8302a1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = Neuron(np.array([0.4,0.7,0.1]),-1,sigma)\n",
    "n2 = Neuron(np.array([0.4,0.7,0.1]),-1,tanh)\n",
    "n3 = Neuron(np.array([0.4,0.7,0.1]),-1,relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81fec831-2701-4b83-a931-8c4ff265e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.323004143761477\n",
      "-0.6291451614140355\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(n1.evaluate(np.array([0.3,0.2,0])))\n",
    "print(n2.evaluate(np.array([0.3,0.2,0])))\n",
    "print(n3.evaluate(np.array([0.3,0.2,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c267050-7f86-4823-8ca3-121ceca2ac4c",
   "metadata": {},
   "source": [
    "## 3. Нейронная сеть. Алгоритмы обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6c11e-fe86-4039-b94a-03cc34e99cbd",
   "metadata": {},
   "source": [
    "Сам по себе нейрон пока еще бесполезен без адекватных алгоритмов обучения.\n",
    "\n",
    "Рассмотрим для начала простейшие случаи.\n",
    "\n",
    "**Персептрон (perceptron)** - одна из простейших архитектур, придуманная Фрэнком Розенблаттом в 1957 году. Она основана на несколько отличающемся искусственном нейроне, который называется линейным пороговым элементом (Linear Threshold Unit - LTU). Отличия в использовании в качестве активационной функции - ступенчатой функции Хевисайда:\n",
    "\n",
    "$$f(x)=0, x<0$$\n",
    "$$f(x)=1, x\\geq 0$$\n",
    "\n",
    "Персептрон состоит из единственного слоя элементов, причем каждый нейрон соединяется со всеми входами. Такие связи часто представляются с использованием специальных сквозных нейронов, называемых входными нейронами (input neuron): они просто передают на выход все, что получают на входе. Кроме того, как правило, добавляется дополнительный признак смещения (bias=1).\n",
    "\n",
    "![image](ris4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2775c9-e88c-4595-9dbe-cb69df16cda5",
   "metadata": {},
   "source": [
    "Итак, каким же образом обучается персептрон? Алгоритм обучения персептрона, предложенный Фрэнком Розенблаттом, был в значительной степени навеян правилом Хебба (Hebb's rule). В своей книге \"The Organization of Behavior\" (\"Организация поведения\"), опубликованной в 1949 году, Дональд Хебб предположил, что когда биологический нейрон часто вызывает срабатывание другого нейрона, то связь между этими двумя нейронами усиливается.\n",
    "Позже идея была резюмирована Зигридом Левелем в его легко запоминающейся фразе: \"Клетки, которые срабатывают вместе, связаны вместе\" (\"Cells that fire together, wire together\"). Впоследствии правило стало известным под\n",
    "названием правило Хебба (или обучение по Хеббу (Hebhian learning)). Правило описано в уравнении:\n",
    "$$w_{ij}=w_{ij}+\\eta(y_j-\\bar{y}_j)x_i$$\n",
    "$w_{ij}$ - вес связи между $i$-м входным нейроном и $j$-м выходным нейроном,\n",
    "\n",
    "$x_i$ - $i$-е входное значение текущего обучающего образца,\n",
    "\n",
    "$\\bar{y}_j$ - выход $j$-го выходного нейрона для текущего обучающего образца,\n",
    "\n",
    "$y_j$ - целевой выход $j$-го выходного нейрона для текущего обучающего образца,\n",
    "\n",
    "$\\eta$ - скорость обучения.\n",
    "\n",
    "Граница решений каждого выходного нейрона линейна, так что персептроны неспособны к обучению на сложных паттернах. Тем не менее, если обучающие образцы являются линейно разделяемыми, то алгоритм будет сходиться в решение. Это называется теоремой о сходимости персептрона (perceptron convergence theorem).\n",
    "\n",
    "Попробуем использовать встроенный класс в библиотеке sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75fc0647-c9d5-49fd-b6de-b186698bdf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4820/1753719795.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Y = (iris.target == 0 ).astype(np.int) # ирис щетинистый?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:,(2, 3)] # длина лепестка , ширина лепестка\n",
    "Y = (iris.target == 0 ).astype(np.int) # ирис щетинистый?\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, Y)\n",
    "\n",
    "y_pred = per_clf.predict([[7, 0.5]])\n",
    "print(X)\n",
    "print(Y)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e38cb2-2626-4d66-97be-f06643f9cbcd",
   "metadata": {},
   "source": [
    "Рассмотрим простейший алгоритм обучения перцептрона для реализации функции логического или.\n",
    "\n",
    "|$x_1$|$x_2$|y|\n",
    "|-----|-----|-|\n",
    "|0|0|0|\n",
    "|0|1|1|\n",
    "|1|0|1|\n",
    "|1|1|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c3b8a8f-6641-4cd6-b22d-80a05d78c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#Логическое или\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "Y = np.array([0,1,1,1])\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, Y)\n",
    "\n",
    "print(per_clf.predict([[0,0]]))\n",
    "print(per_clf.predict([[1,0]]))\n",
    "print(per_clf.predict([[0,1]]))\n",
    "print(per_clf.predict([[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "51e1bbed-0505-450d-9654-e216b5006e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Исключающее логическое или\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]) \n",
    "Y = np.array([0,1,1,0])\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, Y)\n",
    "\n",
    "print(per_clf.predict([[0,0]]))\n",
    "print(per_clf.predict([[1,0]]))\n",
    "print(per_clf.predict([[0,1]]))\n",
    "print(per_clf.predict([[1,1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944d30e-4ac4-43aa-8e57-1ad988d97503",
   "metadata": {},
   "source": [
    "Конечно, современные нейронные сети отличаются от простейшей модели перцептрона. Во первых они, как правило, многослойны, во-вторых, требуют огромного количества вычислений. Но к счастью все расчеты проводятся с помощью векторных вычислений.\n",
    "\n",
    "Выполним простое **прямое прохождение сигнала** по нейронной сети с двумя скрытыми слоями, воспользовавшись библиотекой numpy, и применим к конечному результату функцию мягкого максимума softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2efbab04-e3b2-4473-8261-b99a9b302070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "b1 = 0 # узел смещения 1\n",
    "b2 = 0 # узел смещения 2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    l_exp = np.exp(x)\n",
    "    sm = l_exp/np.sum(l_exp,axis=0)\n",
    "    return sm\n",
    "\n",
    "#Входной набор с тремя признаками\n",
    "X = np.array([[0.35, 0.21, 0.33],\n",
    "              [0.2, 0.4, 0.3],\n",
    "              [0.4, 0.34, 0.5],\n",
    "              [0.18, 0.21, 16]])\n",
    "len_X = len(X) #размер тренировочного набора\n",
    "input_dim = 3 #размер входного слоя\n",
    "output_dim = 1 #размер выходного слоя\n",
    "hidden_units = 4\n",
    "np.random.seed(22)\n",
    "\n",
    "# создаем векторы случайных весовых коэффициентов\n",
    "w0 = 2 * np.random.random((input_dim, hidden_units))\n",
    "w1 = 2 * np.random.random((hidden_units, output_dim))\n",
    "\n",
    "# Проход с прямым распространением сигнала\n",
    "d1 = X.dot(w0)+b1\n",
    "l1 = sigmoid(d1)\n",
    "l2 = l1.dot(w1)+b2\n",
    "\n",
    "#Применяем функцию softmax к выходу из конечного слоя\n",
    "output = softmax(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ddd27e9f-c83b-47bb-beb4-04e4ee3eebf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1635402 ],\n",
       "       [0.15942338],\n",
       "       [0.19455826],\n",
       "       [0.48247816]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7393da6-bdc0-48d0-ade3-cb7e9892595f",
   "metadata": {},
   "source": [
    "Рассмотрим теперь **алгоритм обратного распространения ошибки**, который положен в основу обучения нейронных сетей.\n",
    "\n",
    "В алгоритме обратного распространения ошибки можно выделить следующие шаги:\n",
    "\n",
    "1. Прямое прохождение: случайным образом инициализировать весовые векторы и перемножить вход с последующими весовыми векторами вплоть до конечного результата.\n",
    "2. Вычисление ошибки: вычислить меру ошибки на выходе из этапа прямого прохождения. Для этого проще всего использовать так называемую функцию потерь loss. В качестве такой функции часто используют среднюю квадратическую ошибку (MSE - Mean squared error):\n",
    "\n",
    "$$MSE = L(W,B)= \\frac{\\sum_{i=1}^n (y^{predict}_i-y^{true}_i)^2}{n}$$\n",
    "\n",
    "$y^{predict}_i$  - значение выхода нейронной сети для $i$-й входной точки,\n",
    "\n",
    "$y^{true}_i$ - истинное значение на выходе для $i$-й входной точки.\n",
    "\n",
    "3. Обратное распространение вплоть до последнего вкрытого слоя (относительно выхода). Вычислить грардиент этой ошибки и изменить веса по направлению градиента. Это делается путем перемножения весового вектора $w_j$ с полученными градиентами.\n",
    "4. Обновление весов, пока не будет достигнут критерий останова (минимальная ошибка или число раундов тренировки (эпох)):\n",
    "$$w_{ij} = w_{ij}-\\eta\\cdot \\frac{\\partial L(W,B)}{\\partial w_{ij}}$$\n",
    "\n",
    "Попробуем реализовать данный подход с использованием активационной функции сигмоида."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f56d8fa-6842-4a8a-b526-45f5a1b9e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность сети: 0.44647534287546486\n",
      "Точность сети: 0.6253180460223495\n",
      "Точность сети: 0.6392735698278696\n",
      "Точность сети: 0.646973337655459\n",
      "Точность сети: 0.653551004459721\n",
      "Точность сети: 0.6600110603269453\n",
      "Точность сети: 0.6667981323756436\n",
      "Точность сети: 0.6744988603735986\n",
      "Точность сети: 0.6840284753132683\n",
      "Точность сети: 0.6962997389283025\n",
      "Точность сети: 0.7117467993865515\n",
      "Точность сети: 0.730046564032792\n",
      "Точность сети: 0.7500905930592756\n",
      "Точность сети: 0.770419774119299\n",
      "Точность сети: 0.7899718607807951\n",
      "Точность сети: 0.8082630921890804\n",
      "Точность сети: 0.8245660587281408\n",
      "Точность сети: 0.8387375311669389\n",
      "Точность сети: 0.8510261120667642\n",
      "Точность сети: 0.86169131496511\n",
      "Точность сети: 0.8709670698433893\n",
      "Точность сети: 0.8790652813098027\n",
      "Точность сети: 0.8861722933478937\n",
      "Точность сети: 0.8924457761143711\n",
      "Точность сети: 0.8980162262318607\n",
      "Точность сети: 0.9029907766760406\n",
      "Точность сети: 0.9074573181627793\n",
      "Точность сети: 0.9114881651999217\n",
      "Точность сети: 0.915143094557943\n",
      "Точность сети: 0.9184717912991063\n",
      "Точность сети: 0.9215157943560616\n",
      "Точность сети: 0.9243100380768651\n",
      "Точность сети: 0.9268840741520326\n",
      "Точность сети: 0.9292630429116303\n",
      "Точность сети: 0.9314684486177232\n",
      "Точность сети: 0.9335187813569751\n",
      "Точность сети: 0.9354300185444644\n",
      "Точность сети: 0.9372160315700502\n",
      "Точность сети: 0.9388889173538946\n",
      "Точность сети: 0.9404592701568195\n",
      "Точность сети: 0.9419364056045957\n",
      "Точность сети: 0.9433285462877143\n",
      "Точность сети: 0.9446429763004164\n",
      "Точность сети: 0.9458861705404992\n",
      "Точность сети: 0.9470639033958095\n",
      "Точность сети: 0.9481813405120524\n",
      "Точность сети: 0.9492431166077276\n",
      "Точность сети: 0.9502534017287585\n",
      "Точность сети: 0.9512159578823164\n",
      "Точность сети: 0.9521341876294765\n",
      "Точность сети: 0.9530111759291382\n",
      "Точность сети: 0.9538497262953474\n",
      "Точность сети: 0.9546523921446255\n",
      "Точность сети: 0.9554215040597881\n",
      "Точность сети: 0.9561591935747212\n",
      "Точность сети: 0.956867413984998\n",
      "Точность сети: 0.9575479586076132\n",
      "Точность сети: 0.9582024768459577\n",
      "Точность сети: 0.9588324883607009\n",
      "Точность сети: 0.9594393956012706\n",
      "Точность сети: 0.9600244949143729\n",
      "Точность сети: 0.9605889864140484\n",
      "Точность сети: 0.9611339827710235\n",
      "Точность сети: 0.9616605170566224\n",
      "Точность сети: 0.9621695497575591\n",
      "Точность сети: 0.9626619750618928\n",
      "Точность сети: 0.9631386265028512\n",
      "Точность сети: 0.9636002820356497\n",
      "Точность сети: 0.9640476686125845\n",
      "Точность сети: 0.9644814663132474\n",
      "Точность сети: 0.9649023120794694\n",
      "Точность сети: 0.9653108030984038\n",
      "Точность сети: 0.9657074998717903\n",
      "Точность сети: 0.9660929290048292\n",
      "Точность сети: 0.9664675857440885\n",
      "Точность сети: 0.9668319362903919\n",
      "Точность сети: 0.9671864199096236\n",
      "Точность сети: 0.9675314508617519\n",
      "Точность сети: 0.9678674201660785\n",
      "Точность сети: 0.9681946972187084\n",
      "Точность сети: 0.9685136312764869\n",
      "Точность сети: 0.968824552820086\n",
      "Точность сети: 0.9691277748075824\n",
      "Точность сети: 0.969423593828658\n",
      "Точность сети: 0.9697122911685009\n",
      "Точность сети: 0.9699941337895505\n",
      "Точность сети: 0.9702693752383995\n",
      "Точность сети: 0.9705382564844336\n",
      "Точность сети: 0.9708010066961354\n",
      "Точность сети: 0.9710578439603972\n",
      "Точность сети: 0.9713089759496714\n",
      "Точность сети: 0.9715546005413275\n",
      "Точность сети: 0.9717949063931637\n",
      "Точность сети: 0.9720300734786609\n",
      "Точность сети: 0.9722602735852321\n",
      "Точность сети: 0.9724856707784211\n",
      "Точность сети: 0.9727064218347398\n",
      "Точность сети: 0.9729226766455943\n",
      "Точность сети: 0.9731345785945307\n",
      "Точность сети: 0.9733422649098387\n",
      "Точность сети: 0.9735458669943757\n",
      "Точность сети: 0.9737455107343114\n",
      "Точность сети: 0.9739413167883526\n",
      "Точность сети: 0.9741334008588771\n",
      "Точность сети: 0.9743218739462848\n",
      "Точность сети: 0.9745068425877694\n",
      "Точность сети: 0.9746884090816141\n",
      "Точность сети: 0.9748666716980295\n",
      "Точность сети: 0.9750417248774679\n",
      "Точность сети: 0.9752136594172748\n",
      "Точность сети: 0.9753825626474726\n",
      "Точность сети: 0.9755485185964115\n",
      "Точность сети: 0.9757116081469608\n",
      "Точность сети: 0.9758719091838709\n",
      "Точность сети: 0.9760294967328805\n",
      "Точность сети: 0.9761844430921075\n",
      "Точность сети: 0.9763368179562204\n",
      "Точность сети: 0.9764866885338479\n",
      "Точность сети: 0.9766341196586543\n",
      "Точность сети: 0.9767791738944809\n",
      "Точность сети: 0.9769219116349146\n",
      "Точность сети: 0.9770623911976316\n",
      "Точность сети: 0.9772006689138321\n",
      "Точность сети: 0.9773367992130629\n",
      "Точность сети: 0.9774708347037053\n",
      "Точность сети: 0.9776028262493812\n",
      "Точность сети: 0.9777328230415245\n",
      "Точность сети: 0.9778608726683334\n",
      "Точность сети: 0.9779870211803209\n",
      "Точность сети: 0.9781113131526518\n",
      "Точность сети: 0.9782337917444537\n",
      "Точность сети: 0.9783544987552713\n",
      "Точность сети: 0.9784734746788213\n",
      "Точность сети: 0.9785907587542033\n",
      "Точность сети: 0.978706389014701\n",
      "Точность сети: 0.9788204023343088\n",
      "Точность сети: 0.9789328344721063\n",
      "Точность сети: 0.9790437201145951\n",
      "Точность сети: 0.9791530929161105\n",
      "Точность сети: 0.9792609855374029\n",
      "Точность сети: 0.9793674296824939\n",
      "Точность сети: 0.9794724561338897\n",
      "Точность сети: 0.9795760947862399\n",
      "Точность сети: 0.9796783746785237\n",
      "Точность сети: 0.9797793240248323\n",
      "Точность сети: 0.9798789702438252\n",
      "Точность сети: 0.979977339986924\n",
      "Точность сети: 0.9800744591653054\n",
      "Точность сети: 0.980170352975755\n",
      "Точность сети: 0.9802650459254377\n",
      "Точность сети: 0.980358561855636\n",
      "Точность сети: 0.9804509239645076\n",
      "Точность сети: 0.9805421548289099\n",
      "Точность сети: 0.9806322764253331\n",
      "Точность сети: 0.9807213101499888\n",
      "Точность сети: 0.9808092768380889\n",
      "Точность сети: 0.9808961967823567\n",
      "Точность сети: 0.9809820897508039\n",
      "Точность сети: 0.9810669750038077\n",
      "Точность сети: 0.9811508713105191\n",
      "Точность сети: 0.9812337969646366\n",
      "Точность сети: 0.9813157697995675\n",
      "Точность сети: 0.9813968072030118\n",
      "Точность сети: 0.9814769261309887\n",
      "Точность сети: 0.981556143121333\n",
      "Точность сети: 0.9816344743066842\n",
      "Точность сети: 0.981711935426991\n",
      "Точность сети: 0.981788541841552\n",
      "Точность сети: 0.9818643085406107\n",
      "Точность сети: 0.981939250156529\n",
      "Точность сети: 0.9820133809745518\n",
      "Точность сети: 0.9820867149431813\n",
      "Точность сети: 0.9821592656841817\n",
      "Точность сети: 0.9822310465022229\n",
      "Точность сети: 0.9823020703941846\n",
      "Точность сети: 0.9823723500581303\n",
      "Точность сети: 0.9824418979019667\n",
      "Точность сети: 0.9825107260518025\n",
      "Точность сети: 0.9825788463600162\n",
      "Точность сети: 0.9826462704130461\n",
      "Точность сети: 0.9827130095389143\n",
      "Точность сети: 0.9827790748144938\n",
      "Точность сети: 0.9828444770725308\n",
      "Точность сети: 0.9829092269084285\n",
      "Точность сети: 0.9829733346868066\n",
      "Точность сети: 0.9830368105478406\n",
      "Точность сети: 0.9830996644133931\n",
      "Точность сети: 0.9831619059929422\n",
      "Точность сети: 0.9832235447893163\n",
      "Точность сети: 0.9832845901042442\n",
      "Точность сети: 0.9833450510437225\n",
      "Точность сети: 0.9834049365232133\n",
      "Точность сети: 0.9834642552726748\n",
      "Точность сети: 0.9835230158414321\n",
      "Точность сети: 0.9835812266028926\n",
      "Точность сети: 0.9836388957591162\n",
      "Точность сети: 0.9836960313452391\n",
      "Точность сети: 0.9837526412337623\n",
      "Точность сети: 0.9838087331387071\n",
      "Точность сети: 0.9838643146196416\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def softmax(x):\n",
    "    l_exp = np.exp(x)\n",
    "    sm = l_exp/np.sum(l_exp,axis=0)\n",
    "    return sm\n",
    "def deriv_sigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "eta = 0.1\n",
    "\n",
    "#Входной набор с тремя признаками\n",
    "X = np.array([[0.35, 0.21, 0.33],\n",
    "              [0.2, 0.4, 0.3],\n",
    "              [0.4, 0.34, 0.5],\n",
    "              [0.18, 0.21, 16]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# создаем векторы случайных весовых коэффициентов\n",
    "w0 = 2 * np.random.random((3,4)) - 1\n",
    "w1 = 2 * np.random.random((4,1)) - 1\n",
    "\n",
    "for epoch in range(200000):\n",
    "    #Идем вперед\n",
    "    l1 = sigmoid(X.dot(w0))\n",
    "    l2 = sigmoid(l1.dot(w1))\n",
    "    #Считаем ошибку\n",
    "    l2_error = Y-l2\n",
    "    if (epoch % 1000) == 0:\n",
    "        print(\"Точность сети: \" + str(np.mean(1-(np.abs(l2_error)))))\n",
    "    l2_delta = eta * l2_error * deriv_sigmoid(l2)\n",
    "    l1_error = l2_delta.dot(w1.T)\n",
    "    l1_delta = eta * l1_error *deriv_sigmoid(l1)\n",
    "    w1 += l1.T.dot(l2_delta)\n",
    "    w0 += X.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212ea11-0a6f-400f-8ddf-8d941dc845f1",
   "metadata": {},
   "source": [
    "Известная проблема с нейронными сетями состоит в том, что во авремя оптимизации с обратным распространением ошибки градиент может застревать в локальным минимумах. Это происходит, когда процесс минимизации ошибки попал в ловушку и видит минимум там, где это в действительности просто локальная неровность на пути.\n",
    "\n",
    "![image](ris5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b0fca-4b2a-4013-848a-15080f60642d",
   "metadata": {},
   "source": [
    "Еще одна типичная проблема появляется, когда градиентный спуск не попадает по глобальному минимому, что может приводить к плохо работающим моделям. Данная проблема называется *промахом*.\n",
    "\n",
    "Обе проблемы можно решить путем выбора более низкого темпа обучения, когда модель промахивается, лтибо выбора более высокого темпа обучения, когда при застревании в локальным минимумах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f4f26-1b26-40e3-bf47-0ef3be6015c4",
   "metadata": {},
   "source": [
    "# 4. Метрики качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ef4e3-8b8c-4497-a7f2-8849d30f6be2",
   "metadata": {
    "id": "MFNp4JmC-dmk"
   },
   "source": [
    "Рассмотрим такие метрики, как точность (presicion), специфичность (specificity, true negative rate, TNR), F1-мера (F1-score).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ed10d-2a4d-442b-86ba-fcd9879d1ce5",
   "metadata": {
    "id": "7swhKdJOcELe"
   },
   "source": [
    "## Обозначения:   \n",
    "* $\\mathbf{y} = (y_1, ..., y_n)$ — правильные ответы\n",
    "* В задаче бинарной классификации считаем, что $y_i \\in \\{0, 1\\}$ для любого $i \\in [1, n]$\n",
    "* $\\hat{\\mathbf{y}} = (\\hat{y}_1, ..., \\hat{y}_n)$ — предсказания меток классов\n",
    "* $\\mathbf{p_i} = (p_1, ..., p_K)$ — предсказания вероятностей принадлежности к классам $\\{ C_1, \\ldots, C_K\\}$ для любого $i \\in [1, n]$\n",
    "* В задаче бинарной классификации считаем, что \n",
    "$$p_1 = p \\text{ – вероятность принадлежности к классу } 1$$\n",
    "$$p_0 = 1 - p \\text{ – вероятность принадлежности к классу } 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d3091-6796-44c5-854f-b8ac653faeed",
   "metadata": {
    "id": "F3KhXgs7dKnO"
   },
   "source": [
    "## Точность, доля правильных ответов (accuracy)\n",
    "\n",
    "Формула расчета **доли правильных ответов** выглядит одинаково и в случае бинарной, и в случае многоклассовой классификации (единственное, в многоклассовом случае ее можно считать как для всех классов в целом, так и для каждого класса по отдельности, а потом усреднить полученные значения): \n",
    "\n",
    "$$\\text{accuracy} = \\frac{1}{n} \\sum_{i = 1}^{n}{1(\\hat{y}_i == y_i)}, $$\n",
    "где $1(x)$ — **индикаторная функция**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0973ec-d34c-4f58-aa0b-457db7689d62",
   "metadata": {
    "id": "xtoM-V5JDrZ2"
   },
   "source": [
    "---\n",
    "Пусть $A ⊆ X$ — выбранное подмножество произвольного множества $X$. Функция $1_A: X → \\{0, 1\\}$, определенная следующим образом: \n",
    "\n",
    "\\begin{equation*}\n",
    "1_A(x) = \n",
    " \\begin{cases}\n",
    "   1, &\\text{$x \\in A$}\\\\\n",
    "   0, &\\text{$x \\notin A$},\n",
    " \\end{cases}\n",
    "\\end{equation*}  \n",
    "\n",
    "называется **индикатором** множества $A$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcebdc0-a584-4d22-b983-c048db7cd1a2",
   "metadata": {
    "id": "23WdjsL6Dk8e"
   },
   "source": [
    "$+$ Простая в понимании и интерпретации метрика.  \n",
    "$-$ Неприменима, когда выборка несбалансированная (если в выборке будет очень мало объектов, представляющих какой-то класс, то даже константная модель (предсказания которой являются константой) покажет достаточно большую долю правильных ответов.\n",
    "\n",
    "[Реализация в `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17749d-0812-4852-8af1-806a57b8622d",
   "metadata": {
    "id": "cys_ifN5Qo-n"
   },
   "source": [
    "## Точность (presicion) и специфичность (specificity/true negative rate, TNR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4bd64-ad97-476d-8c0e-4e583de9e28e",
   "metadata": {
    "id": "LXqAAonkYS-P"
   },
   "source": [
    "### Бинарная классификация\n",
    "\n",
    "Перед тем как переходить к формулам непосредственно точности и специфичности, введем следующие обозначения, которые лучше всего записать в виде таблицы: \n",
    "\n",
    "|| $y = 0$ | $y = 1$ |\n",
    "| :-----------: | :-----------: | :-----------: |\n",
    "| $\\hat{y} = 0$ | True negative (TN) | False negative (FN) |\n",
    "| $\\hat{y} = 1$ | False positive (FP) | True positive (TP) |\n",
    "\n",
    "В данной таблице приняты следующие обозначения для объектов, попавших в каждую из ячеек:  \n",
    "* TN — модель правильно классифицировала объект, и он принадлежит классу $0$\n",
    "* FN — модель неправильно классифицировала объект, присвоила ему класс $0$, хотя он принадлежит классу $1$\n",
    "* FP — модель неправильно классифицировала объект, присвоила ему класс $1$, хотя он принадлежит классу $0$\n",
    "* TP — модель правильно классифицировала объект, и он принадлежит классу $1$\n",
    "\n",
    "В терминах, введенных выше, **точность (precision)** и **специфичность (specificity/TNR)** выглядят следующим образом:\n",
    "\n",
    "$$ \\text{precision} = \\frac{TP}{TP + FP} – \\text{доля объектов, названных классификатором положительными и при этом действительно являющиеся положительными.}$$ \n",
    "$$ \\text{TNR} = \\frac{TN}{TN + FP}  \n",
    " – \\text{показывает, какую долю объектов класса $0$ из всех объектов класса $0$ нашел алгоритм.}$$ \n",
    "\n",
    "Также весьма распространена метрика **полнота (recall)**, которая является «братом-близнецом» специфичности:\n",
    "$$ \\text{recall} = \\frac{TP}{TP + FN}  \n",
    " – \\text{показывает, какую долю объектов класса $1$ из всех объектов класса $1$ нашел алгоритм.}$$  \n",
    "\n",
    "Визуализировать различие между точностью и полнотой можно следующим образом: \n",
    "\n",
    "![image.png](https://habrastorage.org/web/38e/9d4/892/38e9d4892d9241ea95e1f56e3ef9124c.png)  \n",
    "[Рис. 1. Визуальное сравнение точности и полноты](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "**Точность (precision)** демонстрирует способность отличать класс $1$ от других классов, а **полнота (recall)** — способность алгоритма обнаруживать класс $1$ вообще (**специфичность** делает то же самое, что и полнота, только для класса $0$).\n",
    "\n",
    "Аналогичным образом, через TN, FN, FP и TP, можно ввести и понятие **доли правильных ответов**: $\\text{accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b78605-4af6-40d2-a424-48ff756c0c3d",
   "metadata": {
    "id": "YOtk-LyhcFOz"
   },
   "source": [
    "$+$ Как точность, так и специфичность и полнота не зависят, в отличие от доли правильных ответов, от соотношения классов, поэтому они применимы и для несбалансированных выборок.  \n",
    "$-$ На практике обычно не стоит задача оптимизировать какую-то одну из этих метрик, а необходимо найти баланс между ними. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c7383-c081-42a7-9b35-ebcede5f49d8",
   "metadata": {
    "id": "Grl14LEHcCkR"
   },
   "source": [
    "### Многоклассовая классификация\n",
    "\n",
    "Как и в случае доли правильных ответов, существуют два типа агрегации этих метрик в многоклассовом случае:  \n",
    "* **Микро-** — необходимые значения вычисляются глобально, целиком по всей выборке, рассматривая каждый элемент матрицы показателей метки как метку  \n",
    "* **Макро-** — метрика считается отдельно для каждого класса, и в качестве ответа берется их среднее значение\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bca4ab-f66c-4f5d-aa88-2f3ce67f2a47",
   "metadata": {
    "id": "GQL_5VfKRYcB"
   },
   "source": [
    "## $F_1$-мера\n",
    "\n",
    "Как упоминалось ранее, обычно важна не только точность или полнота, но и баланс между ними. Наиболее распространенной метрикой такого баланса является **$F_1$-мера** — среднее гармоническое точности и полноты:  \n",
    "$$F_1 = \\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$  \n",
    "\n",
    "**$F_1$-мера** достигает максимума при полноте и точности, равных единице, и близка к нулю, если один из аргументов близок к нулю.  \n",
    "\n",
    "\n",
    "Как в случае точности, специфичности и полноты, формула верна и для бинарной, и для многоклассовой классификации, однако в многоклассовом случае необходимо уточнить вид агрегации: микро- (когда показатели считаются глобально для всей выборки) или макро- (точность и полнота считаются отдельно для каждого класса, а в качестве ответа берется среднее).   \n",
    "[Реализация в `sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "Существуют и другие варианты соотношений точности и полноты. В целом **$F_1$-мера** — частный случай **$F_\\beta-$меры**:  \n",
    "\n",
    "$$F_\\beta = (1 + \\beta^2) \\frac{\\text{precision} \\cdot \\text{recall}}{\\beta^2 \\cdot \\text{precision} + \\text{recall}}$$  \n",
    "\n",
    "* $\\beta > 1$ — больше важна точность\n",
    "* $\\beta < 1$ — больше важна полнота\n",
    "\n",
    "$+$ $F_1$-мера учитывает распределение классов (т. е. хорошо работает даже в случае несбалансированных выборок).   \n",
    "$+$ Это одно число, а не два, как с точностью и полнотой.   \n",
    "$-$ Сложно интерпретируемая. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eafc1d-fb33-4cd8-b84f-8327a8976cdd",
   "metadata": {},
   "source": [
    "## 5. Примеры работы с нейросетями"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f426a-46dd-4cce-ad22-f28e61d04300",
   "metadata": {},
   "source": [
    "Рассмотрим использование библиотеки neurolab.\n",
    "\n",
    "Настроим несколько архитектур, чтобы увидеть различия в качестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "82c43ac9-ce88-48c9-a335-47482e5d76b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/juna/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1.13.1-unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/juna/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/juna/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Collecting neurolab\n",
      "  Downloading neurolab-0.3.5.tar.gz (645 kB)\n",
      "\u001b[K     |████████████████████████████████| 645 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: neurolab\n",
      "  Building wheel for neurolab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for neurolab: filename=neurolab-0.3.5-py3-none-any.whl size=22179 sha256=4f581fa1caf9933c22577638b19908003bd2037fed19e92004979b3f972c0fdc\n",
      "  Stored in directory: /home/juna/.cache/pip/wheels/9a/86/fe/9a885ba792ded332e3c7316b612944d3875a5ea932c386fa9f\n",
      "Successfully built neurolab\n",
      "Installing collected packages: neurolab\n",
      "Successfully installed neurolab-0.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install neurolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4550f63e-1af7-4859-a029-00915cd44223",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array([[1,2,3,4],[4,5,7,8],[3,4,5,6]])\n",
    "labels = np.array([[1],[2],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe4577f-f1e6-4fbd-ada5-bd31da8d3058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15; Error: 0.10699182650368955;\n",
      "The goal of learning is reached\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import neurolab as nl\n",
    "# Create train samples\n",
    "#input = np.random.uniform(-0.5, 0.5, (10, 2))\n",
    "#target = (input[:, 0] + input[:, 1]).reshape(10, 1)\n",
    "# Create network with 2 inputs, 5 neurons in input layer and 1 in output layer\n",
    "net.trainf = nl.train.train_gd\n",
    "net = nl.net.newff([[0, 7], [0, 7], [0,7], [0,7]], [5, 1])\n",
    "# Train process\n",
    "err = net.train(input, target, show=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd129bae-1810-4997-80ad-e345c6be2813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14033229, -0.26697043],\n",
       "       [-0.32664498,  0.11151116],\n",
       "       [ 0.29773168, -0.48087788],\n",
       "       [ 0.46583186, -0.09057417],\n",
       "       [-0.4771744 ,  0.25621615],\n",
       "       [-0.33820466, -0.46610012],\n",
       "       [ 0.27495483,  0.18434175],\n",
       "       [-0.48486676,  0.47682594],\n",
       "       [-0.43101645, -0.44472421],\n",
       "       [ 0.11672825,  0.09822935]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389d5f3c-f562-4ffa-bdd8-f10e63ec4d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25664295],\n",
       "       [ 0.51729302],\n",
       "       [-0.55646973],\n",
       "       [-0.17066756],\n",
       "       [ 0.53854489],\n",
       "       [ 0.31431491],\n",
       "       [ 0.25153476],\n",
       "       [-0.47967915],\n",
       "       [-0.14883781],\n",
       "       [-0.29769465]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ab073ed0-a00a-486a-9a80-cf8e82d16d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7079777290221153,\n",
       " 0.23139046622201553,\n",
       " 0.11020513523612915,\n",
       " 0.061511339711487416,\n",
       " 0.04316782926294021,\n",
       " 0.030616025363742513,\n",
       " 0.025976310461819616,\n",
       " 0.020469349158126485,\n",
       " 0.018208580621741723,\n",
       " 0.015643103942090595,\n",
       " 0.011611504846030562,\n",
       " 0.007787167650966865]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1a41b544-3b98-4410-b0cc-e61531c529cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86680265]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "net.sim([[0.5, 0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1af69bdd-c989-4d9e-a2a2-b7c58592159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0826952 ,  0.05868983],\n",
       "       [-0.35961306, -0.30189851],\n",
       "       [ 0.30074457,  0.46826158],\n",
       "       [-0.18657582,  0.19232262],\n",
       "       [ 0.37638915,  0.39460666],\n",
       "       [-0.41495579, -0.46094522],\n",
       "       [-0.33016958,  0.3781425 ],\n",
       "       [-0.40165317, -0.07889237],\n",
       "       [ 0.45788953,  0.03316528],\n",
       "       [ 0.19187711, -0.18448437]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6448f8f2-5a61-4177-9c11-1607f7e26766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02400537],\n",
       "       [-0.66151157],\n",
       "       [ 0.76900614],\n",
       "       [ 0.00574679],\n",
       "       [ 0.77099582],\n",
       "       [-0.87590101],\n",
       "       [ 0.04797292],\n",
       "       [-0.48054554],\n",
       "       [ 0.49105482],\n",
       "       [ 0.00739274]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34a127-5860-481c-8e5f-8d234bfed368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
