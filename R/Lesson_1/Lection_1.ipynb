{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78f202db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <b>Лекция 1. Основные понятия и технологии больших данных</b>\n",
    " \n",
    "- Общие понятия о больших данных, наука о данных. \n",
    "- Понятия дескриптивной, диагностической, прогностической, прескриптивной  аналитик.\n",
    "- Основные этапы жизненного цикла аналитики больших данных.  \n",
    "- Основные методы анализа больших данных. \n",
    "- Обзор основных программных средств анализа больших данных.\n",
    "- Ограничения CAP теоремы\n",
    "- Понятие NoSQL, классификация NoSQL-решений\n",
    "- Распределенные хранилища данных. Технологии шардинга, репликации, MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e083161",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Понятие большие данные (Big Data), наука о данных (Data Science)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a2d86",
   "metadata": {},
   "source": [
    "Скорость роста генерируемых и хранимых данных растет экспоненциально. \n",
    "В статье 1965 года Гордон Мур подсчитал, что плотность транзисторов на интегральной плате увеличивается вдвое каждые два года. Этот темп роста, известный как «закон Мура», был применен ко всем аспектам вычислений,\n",
    "от тактовых частот до памяти. Считается, что темпы роста объемов данных превышают закон Мура.\n",
    "\n",
    "**Объемы данных увеличиваются более чем вдвое каждые восемнадцать месяцев.**\n",
    "\n",
    "Этот взрыв данных создает возможности для новых способов комбинирования и использования данных, а также создает значительные проблемы из-за размера обрабатываемых и анализируемых данных. \n",
    "\n",
    "Одно существенное изменение касается количества неструктурированных данных. Исторически структурированные данные обычно были в центре внимания большей части корпоративной аналитики, и обрабатывалась с помощью реляционной модели данных. \n",
    "\n",
    "В последнее время количество неструктурированных данные, такие как веб-страницы, данные социальных сетей, изображения и видео резко выросли, и тенденция указывает на увеличение использования неструктурированных данных. \n",
    "\n",
    "*Аналитика больших данных - это способность обрабатывать большие объемы и различные типы информации.*\n",
    "    \n",
    "Рассмотрим ключевые этапы в становлении понятия \"большие данные\" \n",
    "    (https://api.bigdata-msu.ru/media/uploads/2021/05/27/2021_05_25_hohlov.pdf): \n",
    "    \n",
    "*2001.* Аналитик Meta Group Даг Лэйни публикует аналитическую записку, в которой дает три основные характеристики: объем, скорость изменений и многообразие больших данных (3V = Volume, Velocity, Variety)\n",
    "    \n",
    "\n",
    "*2013-2014*. Национальный институт стандартизации и технологий США (NIST) создает рабочую группу по большим данным для стандартизации эталонной архитектуры больших данных.  В 2014 г опубликованы первые редакции основных документов NIST https://bigdatawg.nist.gov/V1_output_docs.php\n",
    "    \n",
    "*2015* Международная организация по стандартизации (ИСО) и Международная электротехническая комиссия (МЭК) публикуют предварительный доклад «Большие данные», в котором описаны проблемы и направления международной стандартизации технологий работы с большими данными.\n",
    "\n",
    "*2018* Утверждены первые международные стандарты из серии стандартов ISO/IEC 20547-X, посвященной стандартизации\n",
    "эталонной архитектуры больших данных: часть 2 «Сценарии использования и производные требования» и часть 5 «Направления стандартизации»\n",
    "    \n",
    "\n",
    "    \n",
    "*2020* Утвержден международный стандарт «Организация сетей, ориентированных на большие данные – Требования» [ITU-T Y.3652] и опубликованы рекомендации по внедрению больших данных в развивающихся странах [Y Suppl. 65], утверждены\n",
    "международные стандарты «Большие данные – Эталонная архитектура» [ITU-TY.3605] и «Большие данные – Обзор и требования к сохранению данных» [ITU-T Y.3604]\n",
    "\n",
    "*2021* Подготовлен  к публикации международный стандарт «Организация сетей, ориентированных на большие данные – Функциональная архитектура» [ITU-T Y.3653]\n",
    "    \n",
    "Для  того  чтобы  набор  данных  можно  было  считать  большими  данными,  он  должен  обладать  одной  или  несколькими  специфическими характеристиками. Большинство  этих  характеристик  данных  были  первоначально  определены  Дагом  Лейни  в  начале  2001  года,  когда он  опубликовал  свою  статью,  описывающую  влияние  объема,  скорости  и  многообразия  данных  электронной  коммерции  на  хранилища  данных  предприятия.\n",
    "    \n",
    " На текущий момент согласно NIST SP 1500-1 -- Volume 1: Definitions выделяют следующие характеристики:\n",
    "    \n",
    "**Объем (Volume)**\n",
    "    \n",
    " Предполагаемый  объем  данных,  который  обрабатывается  решениями  для  больших  данных,  является  существенным  и  постоянно  растущим.  Большие  объемы  данных  накладывают  различные  требования  по  хранению  и  обработке  данных,  а  также  к  дополнительной  подготовке  данных,  к  процессам \n",
    "сопровождения  и  управления. \n",
    "    \n",
    "Типичные  источники  данных,  которые  отвечают  за  генерацию  больших  объемов  данных,  могут  включать  в  себя:\n",
    " * онлайн-транзакции,  такие  как  розничные  точки  продаж  и  т.д.\n",
    " * научные  и  исследовательские  эксперименты,  такие  как большой  адронный  коллайдер  и  атакамский  большой  антенный  телескоп\n",
    " * сенсоры,  такие  как  GPS-сенсоры,  RFID,  смарт-счетчики  и телематика\n",
    " * социальные  сети,  такие  как  Facebook  и  Twitter\n",
    "\n",
    " Организации  и  пользователи по  всему  миру  создают  более  2,5  ЕВ  [экзабайтов)  данный  в  день.  В  качестве  сравнения:  в  настоящее  время  библиотека  Конгресса  США  содержит  более  300  ТВ (терабайтов)  данный.\n",
    "    \n",
    "**Скорость (Velocity)**\n",
    "    \n",
    "В  средах  с  большими  данными  последние  могут  поступать  с  высокими  скоростями,  и  при  этом  огромные  массивы  данных  могут  накапливаться  за  очень  короткие  промежутки  времени.  \n",
    "    \n",
    "В  зависимости  от  источника  данных  скорость  может  разной.   В тоже время для сравнения можно указать, что за  минуту  могут  быть  легко  созданы  следующие  объемы  данных:  350  тысяч  твитов,  300  часов  видеоматериалов,  загруженных  на  YouTube,  171  миллион  электронных  писем  и  330  гигабайт  данных  от  сенсоров  реактивного  двигателя.\n",
    "\n",
    "**Многообразие (Variety)**\n",
    "    \n",
    "Многообразие  данных  касается  множества  форматов  и  типов  данных,  которые  должны  поддерживаться  решениями \n",
    "для  больших  данных.  \n",
    "    \n",
    "Многообразие  данных  создает  проблемы   с  точки  зрения  интеграции  данных,  их  трансформации,  обработки  и  хранения.  В качестве примера  многообразия  данных можно указать: структурированные  данные  в  форме  финансовых  транзакций,  слабоструктурированные  данные  в  виде  электронных  писем  и  неструктурированные  данные  в  виде  изображений, текстов,  графики,  видео-  аудио-,  html,  json,  сенсорные  данные  и  метаданные.\n",
    "\n",
    "**Изменчивость (Variability)**\n",
    "    \n",
    "Кроме того, что данные могут быстро накапливаться, уже накопленные могут еще и быстро меняться. Это накладывает существенные ограничения на проектирование распределенных систем хранения данных, поскольку можно не успевать их согласовывать в разных сегментах децентрализованного хранилища: системы электронной коммерции, биржи.\n",
    "    \n",
    "\n",
    "Поскольку все эти термины начинаются на английскую букву V, такой набор часто называют \"V\".\n",
    "\n",
    "В тоже время иногда выделяют дополнительные характеристики: **достоверность (veracity)** (т.е. точность данных), ценность **value** (т.е. ценность аналитики для организации), **волатильность (volatility)** (т. е. склонность структур данных к изменению во времени - меняются форматы хранения, передачи данных) и **валидность (validity)** (т. е. пригодность данных для предполагаемого использования).\n",
    "    \n",
    "Окончательно, под большими данными будем понимать:\n",
    "  \n",
    "**Большие данные - это данные, которые  характеризуются следующими свойствами:  объем, многообразие, скорость, и (или) изменчивость, и требуют масштабируемой архитектуры для эффективного хранения, обработки и анализа.**\n",
    "    \n",
    "Как уже было указано, по многообразию данные можно классифицировать на следующие группы:\n",
    "    \n",
    "   * структурированные данные;\n",
    "   * слабоструктурированные данные;\n",
    "   * неструктурированные данные.\n",
    "\n",
    "*Структурированные  данные*  соответствуют  моделям  или  схемам  данных  и часто  хранятся  в  табличных  формах. \n",
    "\n",
    " Они  используются  для  фиксации  отношений  между  различными  объектами и  поэтому  чаще  всего  хранятся  в  реляционных  базах  данных.  Структурированные  данные  часто  генерируются корпоративными  приложениями  и  информационными  системами,  такими  как  системы  ERP  и  CRM.  Из-за  обилия инструментов  и  баз  данных,  которые  поддерживают  структурированные  данные,  они  редко  нуждаются  в  особых  подходах к  обработке  или  хранению.  Примерами  такого  типа  данных  являются  банковские  операции,  счета-фактуры  и  записи  клиентов.\n",
    "    \n",
    "*Слабоструктурированные  данные*  имеют  определенный  уровень  структуры  и  согласованности,  но  не  являются  реляционными  по  своей  природе.  Они  являются  иерархическими  или  основанными  на  графах.  Такого  рода  данные  обычно хранятся  в  файлах,  содержащих  текст.  Например,  файлы  XML  и  JSON  являются  распространенными  формами  слабоструктурированных  данных.  Ввиду текстовой  природы  этих  данных  и  их  соответствия  определенному  уровню  структуры  они  обрабатываются  легче,  чем  неструктурированные  данные.\n",
    "Примеры  общих  источников  слабоструктурированных  данных  включают  файлы  электронной  системы  документооборота  (EDI),  электронные  таблицы,  RSS-каналы  и  данные  от  сенсоров.  Слабоструктурированные  данные  часто  имеют специальные  требования  к  предварительной  обработке  и  хранению,  особенно  если  основной  формат  не  является  текстовым.  Примером  предварительной  обработки  слабоструктурированных  данных  может  быть  проверка  допустимости XML-файла  с  целью  гарантирования  соответствия  его  определению  схемы. (см. витрины данных https://data.mos.ru/)\n",
    "    \n",
    "*Данные,  которые  не  соответствуют  моделям  или  схемам \n",
    "данных,  называются  *неструктурированными  данными*.  Считается,  что  неструктурированные  данные  составляют  80% данных  любой организации.  Неструктурированные  данные имеют  более  быстрый  темп  роста,  чем  структурированные.   Форма таких данных  является  либо текстовой,  либо  бинарной  и  зачастую  передается  посредством  файлов,  которые  являются  автономными и  нереляционными.  Текстовый  файл  может  вмещать  содержимое  различных  твитов  или  постов  в  блогах.  Бинарные  файлы  чаще  представляют  собой  мультимедийные  файлы,  содержащие  изображения,  аудио-  или  видеоданные.  Технически  и  текстовые,  и  бинарные  файлы  имеют  структуру,  определенную  самим  форматом  файла,  но  этот  аспект  игнорируется,  а  понятие неструктурированное относится  к  формату  данных.\n",
    "    \n",
    "Обычно  для  обработки  и  хранения  неструктурированных  данных  требуется  специализированная  логика.  Например,  для  воспроизведения  видеофайла  важно  наличие  правильного  кодека.  Неструктурированные  данные  нельзя  напрямую  обработать  или  запросить  с  помощью  SQL.  Если  их необходимо  хранить  в  реляционной  базе  данных,  то  они  сохраняются  в  таблице  как  большие  бинарные  объекты  (Binary Large  Object  —  BLOB).\n",
    "\n",
    "\n",
    "Рассмотрим теперь термин **наука о данных**. Согласно NIST:\n",
    "    \n",
    "**Наука о данных - это методология синтеза полезных знаний непосредственно из данных через процесс открытия или формулировки гипотез и проверки гипотез.**\n",
    "    \n",
    "*Специалист по данным (data scientist)* - это практик, обладающий достаточными знаниями в предметной области (Domain Expertise), статистики и машинном обучении (Statistics & Machine Learning), инжиниринга для управления сквозными процессами обработки данных в жизненном цикле данных (Engineering). \n",
    "    \n",
    "Более наглядно этот набор компетенций описан на рисунке в виде диаграммы Вена-Эйлера.\n",
    "\n",
    "<img src=\"ris1.jpg\" height=\"300\" width=\"300\">\n",
    "\n",
    "\n",
    "Хотя этот полный набор навыков в редком случае может присутствовать в отдельном человеке, также возможно, что эти навыки охватываются членами команды."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570c10d-dc8d-41c7-a171-9b118b4e06c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Различные виды аналитик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5644255-f6f8-43ef-b183-6480b7b2f19e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Анализ данных представляет собой процесс исследования данных для поиска фактов, отношений, шаблонов, идей и/или тенденций. Общая цель анализа данных заключается в том, чтобы поддерживать принятие более обоснованных решений. Простым примером анализа данных является анализ данных по продаже мороженого с целью определить, каким образом количество проданных рожков связано со среднесуточной температурой воздуха. Результаты такого анализа могли бы стать обоснованием для решения о количестве мороженого, которое магазин должен заказать в зависимости от прогноза погоды. Выполнение анализа данных помогает установить закономерности и взаимосвязи между анализируемыми данными.\n",
    "\n",
    "Существуют четыре основные категории аналитики, которые различаются производимыми результатами:\n",
    "\n",
    " - дескриптивная аналитика\n",
    " - диагностическая аналитика\n",
    " - прогностическая аналитика\n",
    " - прескриптивная аналитика\n",
    "\n",
    "*Дескриптивная аналитика* \n",
    "\n",
    "Дескриптивная аналитика используется для поиска ответов на вопросы о событиях, которые уже произошли. Эта форма аналитики согласовывает данные с контекстом для генерирования информации.\n",
    "Примеры вопросов дескриптивной аналитики:\n",
    "\n",
    "Каким был объем продаж за последние 12 месяцев?\n",
    "Сколько звонков поступило в службу поддержки, упорядоченных по категориям серьезности и географическому расположению?\n",
    "Какова ежемесячная комиссия, заработанная каждым агентом по продажам?\n",
    "\n",
    "По оценкам, 80 % сгенерированных результатов аналитики являются дескриптивными по своей природе. Ориентированная на смысловую полезность, дескриптивная аналитика обеспечивается с наименьшими затратами и требует относительного базового набора навыков.\n",
    "\n",
    "Дескриптивная аналитика часто выполняется с помощью специальных отчетов или информационных панелей. Отчеты обычно статичны по своей природе и отображают исторические данные, которые представлены в форме таблиц или диаграмм.\n",
    "\n",
    "*Диагностическая аналитика*\n",
    "\n",
    "Диагностическая аналитика направлена на то, чтобы определить причину произошедшего события, используя вопросы, которые фокусируются на причинах этого события. Цель этого типа аналитики — определить, какая информация относится к данному явлению, чтобы дать возможность ответить на вопросы о том, почему это произошло.\n",
    "\n",
    "К таким вопросам относятся:\n",
    "\n",
    "Почему продажи первого предприятия были меньше, чем продажи второго?\n",
    "Почему в службу поддержки поступило больше звонков из восточного региона, чем из западного?\n",
    "Почему увеличилось число повторных госпитализаций пациентов за последние три месяца?\n",
    "\n",
    "Диагностическая аналитика имеет большую значимость, чем дескриптивная, но требует более продвинутого набора навыков. Обычно диагностическая аналитика требует сбора данных из различных источников и хранения их в структуре, которая подвергается детальному анализу и свертыванию.\n",
    "\n",
    "Результаты диагностической аналитики могут быть просмотрены с помощью инструментов интерактивной визуализации, которые позволяют пользователям определять тенденции и шаблоны. Выполнение запросов здесь значительно сложнее, чем в дескриптивной аналитике, и выполняются они на многомерных данных, содержащихся в системах аналитической обработки.\n",
    "\n",
    "*Прогностическая аналитика*\n",
    "\n",
    "Прогностическая аналитика проводится с целью определить результат события, которое может произойти в будущем. С помощью прогностической аналитики информация усиливается смысловым содержанием. Интенсивность и значимость ассоциативных связей формируют основу моделей, которые используются для создания будущих прогнозов на основе прошлых событий.\n",
    "\n",
    "Важно учитывать, что у моделей, которые используются для прогностической аналитики, существуют неявные зависимости от условий, в которых происходили прошлые события. Если лежащие в основе причины изменяются, то и модели прогнозирования должны быть откорректированы.\n",
    "\n",
    "Вопросы обычно формулируются с использованием обоснования «Что, если...», например:\n",
    "\n",
    "Какова вероятность невозвращения клиентом кредита, если пропущен ежемесячный платеж?\n",
    "Каков процент эффективности лечения пациента, если вместо препарата А будет использоваться препарат В?\n",
    "Если клиент приобрел продукты А и В, каковы шансы, что он также купит продукт С?\n",
    "\n",
    "Прогностическая аналитика призвана предсказать исход события, при этом прогнозы делаются на основе шаблонов, тенденций и исключений, найденных в исторических и текущих данных. Это может привести к выявлению как рисков, так и возможностей. Такой вид аналитики предполагает использование больших наборов данных, внутренних и внешних, и различных методов анализа этих данных. Эта аналитика обеспечивает высокую значимость результатов и требует еще более совершенного набора навыков, чем дескриптивная и диагностическая.\n",
    "\n",
    "Как правило, инструменты прогностической аналитики используют лежащие в их основе абстрактные способы решения статистических сложных задач, предоставляя удобные для пользователя внешние интерфейсы.\n",
    "\n",
    "*Прескриптивная аналитика*\n",
    "\n",
    "Прескриптивная аналитика основывается на результатах прогностической аналитики, предписывая меры, которые должны быть предприняты. Акцент делается не только на том, какому предписанному варианту лучше всего следовать, но и почему.\n",
    "\n",
    "Вопросы могут иметь следующий вид:\n",
    "\n",
    "Какой из трех препаратов обеспечивает наилучшие результаты?\n",
    "Когда наилучше время для проведения определенной акции? \n",
    "\n",
    "Прескриптивная аналитика представляет большую значимость, чем любые другие виды аналитики, и, соответственно, требует самого продвинутого набора навыков, а также специализированного программного обеспечения и инструментов.\n",
    "Благодаря ей рассчитываются различные результаты и предлагается оптимальный курс действий для каждого из них. Такая тактика переходит от пояснений к консультациям и может включать в себя моделирование различных сценариев.\n",
    "\n",
    "Этот вид аналитики охватывает внутренние данные одновременно с внешними. Внутренние данные могут включать в себя текущие и исторические данные о продажах, информацию о клиентах, данные о продуктах и бизнес-правила. Внешние же могут содержать данные из социальных сетей, прогнозы погоды и демографические данные, подготовленные правительством."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd0daf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Основные этапы жизненного цикла аналитики больших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95703c05",
   "metadata": {},
   "source": [
    "**Жизненный цикл данных** - это набор процессов в приложении, которые преобразуют необработанные данные в практические знания.\n",
    "\n",
    "Жизненный  цикл  аналитики  больших  данных  можно  разделить  на  следующие  девять  этапов:\n",
    "1. Оценивание ситуации\n",
    "2. Идентификация  данных\n",
    "3. Сбор  и  фильтрация данных\n",
    "4. Извлечение  данных\n",
    "5. Проверка  и  очистка данных\n",
    "6. Агрегирование и  представление  данных\n",
    "7. Анализ  данных\n",
    "8. Визуализация  данных\n",
    "9. Использование результатов  анализа\n",
    "\n",
    "*Оценивание ситуации*\n",
    "\n",
    "Анализ данных связан с формированием цели анализа, определения мотивации.\n",
    "Четкая формулировка цели позволит выбрать основные направления анализа еще до начала  выполнения  реальных  практических  задач  анализа.\n",
    "\n",
    "*Идентификация данных*\n",
    "\n",
    "Этап  идентификации  данных посвящен  определению  наборов  данных,  необходимых  для  аналитических  проектов  и  их  источников.\n",
    " \n",
    " Выявление  наиболее  широкого  спектра  источников  данных  может  увеличить  вероятность  обнаружения  скрытых  закономерностей  и  корреляций.  \n",
    " В  зависимости  от  сферы  деятельности  и  характера  решаемых  бизнес-задач,  требуемые  наборы  данных и  их  источники  могут  быть  внутренними  и/или  внешними для  организации.\n",
    "\n",
    "В  случае  внутренних  наборов  данных,  списки  доступных  наборов  данных  из  внутренних  источников,  таких  как  витрины данных, базы данных,  составляются  и  сравниваются  с  заранее  определенной  спецификацией  набора  данных.\n",
    " \n",
    " При  работе  с  внешними  наборами  данных  составляется  список  возможных  сторонних  поставщиков,  таких  как  витрины данных  и  общедоступные  наборы  данных.  Некоторые формы  внешних  данных  могут  быть  встроены  в  блоги  или другие  типы  веб-сайтов  на  основе  контента,  и  в  этом  случае они,  возможно,  должны  быть  собраны  с  помощью  автоматизированных  инструментов.\n",
    " \n",
    "*Сбор и фильтрация данных*\n",
    "\n",
    "На  этапе  сбора  и  фильтрации  данных данные  собираются  из  всех  источников,  которые были  идентифицированы  на  предыдущем  этапе.  Затем  полученные  данные  подвергаются  автоматизированной  фильтрации  для  удаления  поврежденных  или  таких,  которые  не имеют  особого  значения  для  целей  анализа.\n",
    "\n",
    "В  зависимости  от  типа  источника  данные  могут  поступать как  набор  файлов,  например,  данные,  приобретенные  у  стороннего  поставщика,  или  могут  потребовать  интеграции  API,  например,  с  Twitter.  Во  многих  случаях,  особенно  в  случае  внешних  неструктурированных  данных,  некоторые  или большинство  полученных  данных  могут  быть  нерелевантными  (шумом)  и  могут  быть  отброшены  в  процессе  фильтрации.\n",
    " \n",
    " Данные,  классифицированные  как  «искаженные»,  могут включать  записи  с  отсутствующими  или  бессмысленными \n",
    "значениями  или  недопустимыми  типами  данных.  Данные, отфильтрованные  для  одного  анализа,  могут  быть  значимыми  для  другого  типа  анализа.  Поэтому  рекомендуется сохранить  точную  копию  исходного  набора  данных  перед началом  фильтрации.  Чтобы  свести  к  минимуму  требуемое пространство  для  хранения,  точная  копия  может  быть  сжата.\n",
    "\n",
    "*Извлечение данных*\n",
    "\n",
    "Некоторые  данные,  идентифицированные  как  входные данные  для  анализа,  могут  поступать  в  формате,  несовместимом с алгоритмами обработки данных.  Необходимость  обращаться  к  несопоставимым  типам  данных более  вероятна  при  работе  с  данными  из  внешних  источников.  \n",
    "\n",
    "Этап  жизненного  цикла  извлечения  данных  предназначен  для  извлечения  несопоставимых  данных  и  преобразования  их  в сопоставимый формат,  который  может  использовать  в \n",
    "целях  анализа  данных.\n",
    " \n",
    "Необходимая  степень  извлечения  и  преобразования  зависят  от типов  аналитики.  Например,  извлечение  обязательных  полей  из  текстовых данных  с  разделителями,  таких  как  файлы  журнала  веб-сервера,  может  не  понадобиться,  если  базовые  решения  для  больших  данных  уже  могут  напрямую  обрабатывать  эти  файлы.\n",
    "\n",
    "Аналогично,  извлечение  текста  для  аналитики  текста,  который  требует  сканирования  всех  документов,  упрощается, если  базовое  решение  для  больших  данных  может  напрямую читать  документ  в  его  собственном  формате.\n",
    "\n",
    "*Проверка и очистка данных*\n",
    "\n",
    "Неправильные  данные  могут  искажать  и  фальсифицировать результаты  анализа.  В  отличие  от  традиционных  корпоративных  данных,  где  структура  данных  заранее  определена и  данные  предварительно  проверены,  данные  вводимые  в анализ  больших  данных  могут  быть  неструктурированными,  без  каких-либо  указаний  на достоверность.  Эта  сложность  также  может  затруднить  получение  набора  подходящих  ограничений  проверки.\n",
    " \n",
    "Этап  проверки  и  очистки  данных предназначен  для  создания  зачастую  сложных  правил  проверки  и  удаления  любых  известных  недопустимых  данных.\n",
    "\n",
    "*Агрегирование и представление данных*\n",
    "\n",
    "Данные  могут  быть  распределены  по  нескольким  наборам данных,  требуя  объединения  наборов  данных  через  общие  поля,  например  дату  или  идентификатор  (ID).  \n",
    "\n",
    "Этап  агрегирования  и  представления  данных предназначен  для  интеграции  нескольких  наборов  данных  вместе  для  достижения  унифицированного  представления.\n",
    "\n",
    "\n",
    "*Анализ данных*\n",
    "\n",
    "Этап  анализа  данных  посвящен выполнению  фактической  задачи  анализа,  которая  обычно включает  в  себя  один  или  несколько  типов  аналитики.  Этот этап  может  быть  итеративным  по  своей  природе,  особенно если  анализ  данных  является  разведывательным  —  в  этом случае  анализ  повторяется  до  тех  пор,  пока  не  будет  обнаружен  соответствующий  шаблон  или  корреляция.  \n",
    "\n",
    " В  зависимости  от  типа  требуемого  аналитического  результата  этот  этап  может  быть  таким  же  простым,  как  запрос  к набору  данных,  чтобы  вычислить  агрегирование  для  сравнения.  С  другой  стороны,  это  может  быть  столь  же  сложным, как  комбинирование  интеллектуального  анализа  данных  и сложных  методов статистического  анализа  для  обнаружения  закономерностей  и  аномалий,  или  для  создания  статистической  или  математической  модели,  описывающей  взаимосвязи  между  переменными.\n",
    " \n",
    " Анализ  данных  может  быть  классифицирован  как  *подтверждающий  или  разведывательный  анализ*,  последний  из  которых  связан  с  data  mining. \n",
    " \n",
    " *Подтверждающий  анализ*  данных  является  дедуктивным подходом,  при  котором  причина  исследуемого  явления предлагается  заранее.  Предлагаемая  причина  или  предположение  называется  гипотезой.  Затем  данные  анализируются  для  того,  чтобы  подтвердить  или  опровергнуть  гипотезу  и  дать  окончательные  ответы  на  конкретные  вопросы.\n",
    " \n",
    "*Разведывательный  анализ*  данных  представляет  собой  индуктивный  подход,  который  тесно  связан  с  data  mining.  Ни одна  из  гипотез  или  предположений  не  генерируются  заранее.  Вместо  этого  данные  анализируются  для  развития  понимания  причины  этого  явления.  Хотя анализ  может  и  не  давать  окончательных  ответов,  этот  метод  позволяет сформировать  направление,  которое  может  способствовать  обнаружению  закономерностей  или  аномалий.\n",
    "\n",
    "*Визуализация данных*\n",
    "\n",
    "Способность  анализировать  огромные  объемы  данных  и находить  полезные  идеи  не  имеет  большого  значения,  если единственными,  кто  может  интерпретировать  результаты, являются  аналитики.\n",
    "\n",
    "Этап  визуализации  данных посвящен  использованию  методов  визуализации  данных и  инструментам  графического  представления  результатов анализа  для  их  эффективной  интерпретации  пользователями.\n",
    "\n",
    "*Использование результатов анализа*\n",
    "\n",
    "После  получения  результатов  анализа,  предоставляемых пользователям  для  поддержки  принятия  решений,  они могут располагаться на  информационных  панелях.  Этап  использования  результатов  анализа посвящен  определению  того,  как  и  где \n",
    "обрабатываемые  аналитические  данные  могут  быть  в  дальнейшем  использованы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a1e95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Основные методы анализа больших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123184a",
   "metadata": {},
   "source": [
    "Анализ  больших  данных  сочетает традиционные  методы  статистического  анализа  данных  с  вычислительными  методами.\n",
    "\n",
    "Принято выделять общие и частные методы анализа больших данных. \n",
    "\n",
    "Среди общих методов следует выделить: \n",
    "- количественный  анализ\n",
    "- качественный  анализ\n",
    "- Data  Mining  (интеллектуальный  анализ  данных)\n",
    "\n",
    "Внутри Data  Mining часто говорят про следующие методы:\n",
    " - статистический  анализ\n",
    " - машинное  обучение\n",
    " - семантический  анализ\n",
    " - визуальный  анализ\n",
    "\n",
    "*Количественный  анализ*  —  это  метод  анализа  данных,  который  фокусируется  на  определении  количественных  закономерностей  и  корреляций,  найденных  в  данных.  Основанный на  статистических  методах,  этот  метод  предполагает  анализ  большого  количества  наблюдений  из  набора  данных.  Поскольку  размер  выборки  большой,  результаты  могут  применяться  обобщенно  ко  всему  набору  данных. \n",
    "\n",
    "Результаты  количественного  анализа  носят  абсолютный характер  и  поэтому  могут  использоваться  для  численных сравнений.  Например,  количественный  анализ  по  продажам мороженого  может  обнаружить,  что  увеличение  температуры  на  5  градусов  увеличивает  продажи  мороженого  на  15%.\n",
    "\n",
    "*Качественный  анализ*  —  это  метод  анализа  данных,  который  фокусируется  на  описании  различных  качеств  данных с  использованием  слов.  \n",
    "\n",
    "Он  предполагает  анализ  меньшей  выборки,  но  более  основательнее,  по  сравнению  с  количественным  анализом  данных.  Такие  результаты  анализа  не могут  быть  обобщены  на  весь  набор  данных  из-за  небольшого  размера  выборки.  Их  также  нельзя  численно  измерить или  использовать  для  числовых  сравнений.  Например,  качественный  анализ  по  продажам  мороженого  может  показать,  что  показатели  продаж  в  мае  были  не  такими  высокими,  как  в  июне.  Результаты  анализа  указывают  только  на  то, что  цифры  «не  так  высоки,  как»,  и  не  дают  количественной разницы.  Результатом  качественного  анализа  является  описание  отношения  с  использованием  слов.\n",
    "\n",
    "*Data  Mining*\n",
    "\n",
    " Data  Mining  (интеллектуальный  анализ  данных),  известен  также  как  обнаружение  данных,  представляет  собой  специализированную  форму  анализа  данных,  предназначенную для  больших  наборов  данных.  В  связи  с  анализом  больших данных,  Data  Mining  обычно  относится  к  автоматизированным,  основанным  на  программном  обеспечении  методам, которые  просеивают  массивные  наборы  данных  для  выявления  закономерностей  и  тенденций.\n",
    " \n",
    " В  частности,  это  предполагает  извлечение  скрытых  или  неизвестных  шаблонов  в  данных  с  целью  идентификации ранее  неизвестных  закономерностей. Data  Mining  является  основой  для  прогностической аналитики  и  Business Intelligence  (BI).\n",
    " \n",
    "*Статистический  анализ*\n",
    "\n",
    " Статистический  анализ  использует  статистические  методы, основанные  на  теории вероятностей и математической статистики  в  качестве  средства  для  анализа  данных.  Статистический  анализ  чаще  всего является  количественным,  но  может  быть  и  качественным. \n",
    "\n",
    "Этот  тип  анализа  обычно  используется  для  описания  наборов  данных  посредством  итогового  обобщения,  например, предоставление  среднего  значения.\n",
    " Среди важных методов можно указать  следующие  виды  статистического  анализа:\n",
    " * проверка статистических гипотез\n",
    " * корреляционный анализ\n",
    " * регрессионный анализ\n",
    "\n",
    "*Машинное обучение*\n",
    "\n",
    "Люди  хорошо  разбираются  в  шаблонах  и  отношениях  внутри  данных.  К  сожалению,  мы  не  можем  обработать  большие объемы  данных  очень  быстро.  Машины,  с  другой  стороны, очень  искусны  в  быстрой  обработке больших  объемов  данных,  но  только  тогда,  когда знают  как.\n",
    "\n",
    "Если  знания  человека  сочетать  со  скоростью  обработки  машины,  то  последние  смогут  обрабатывать  большие  объемы данных,  без  особого  вмешательства  человека.  Это  и  является основной  концепцией  машинного обучения.\n",
    "\n",
    "В  Data  Mining  можно выделить следующие важные  методы  машинного  обучения:\n",
    " * Классификация\n",
    " * Кластеризация\n",
    " * Обнаружение  выбросов\n",
    " * Фильтрация\n",
    "\n",
    "*Классификация*  —  это  метод  обучения  с  учителем,  при  котором  данные  классифицируются  на  релевантные,  заранее исследованные  категории.  \n",
    "Данный метод состоит  из  двух  этапов:\n",
    " 1. Система  получает  обучающие  данные,  которые  уже  классифицированы  или  промаркированы,  для  возможности развития  понимания  различных  категорий.\n",
    " 2. Система  получает  неизвестные,  но  аналогичные  данные для  классификации,  и  на  основании  понимания,  которое она  получила  из  обучающих  данных,  алгоритм  классифицирует  немаркированные  данные.\n",
    " \n",
    " Примером  применения  этого  метода  является  фильтрация спама  электронной  почты.  \n",
    " \n",
    " Классификация  может  проводиться  по  двум  или  более  категориям.\n",
    " \n",
    " Примеры вопросов, которые можно решать с использованием классификации:\n",
    " \n",
    " * Следует  ли  принять  или  отклонить  заявку  претендента на  выдачу  кредитной  карты  на  основании  других  принятых или  отклоненных  заявок?\n",
    " * Является  ли  томат  фруктом  или  овощем,  основываясь  на известных  примерах  фруктов  и  овощей?\n",
    " * Результаты  клинических  испытаний  для  пациентов  свидетельствуют  об  угрозе  сердечного  приступа?\n",
    " \n",
    " Для классификации используют целый арсенал математических подходов (методов, алгоритмов): опорные вектора, градиентный бустинг, деревья решений, нейронные сети и т.д.\n",
    " \n",
    "*Кластеризация*  —  это  метод  обучения  без  учителя,  при  котором  данные  делятся  на  разные  группы,  так  что  данные  в  каждой  группе  имеют  схожие  свойства.  Не  требуется  никакого предварительного  изучения  требуемых  категорий.  Вместо этого  категории  генерируются  неявно  на  основе  группирования  данных.  Способ  группирования  данных  зависит  от типа  используемого  алгоритма.  Каждый  алгоритм  использует  различную  технику  для  определения  кластеров.\n",
    " \n",
    " Кластеризация  обычно  используется  в  Data  Mining  для  получения  представления  о  свойствах  заданного  набора  данных. После  разработки  такого  понимания  классификация  может использоваться  для  более  точного  прогнозирования  подобных,  но  новых  или  ненаблюдаемых  данных.\n",
    "\n",
    "Кластеризация  может  применяться  для  категоризации  неизвестных  документов  и  персонализированных  маркетинговых  кампаний,  группируя  клиентов  со  схожим  поведением.\n",
    "\n",
    "Вопросы, решаемые в рамках кластеризации,   могут  включать  в  себя:\n",
    " * Сколько  существует  различных  видов  деревьев,  основываясь на  сходстве  между  деревьями?\n",
    " * Сколько  существует  групп  клиентов,  основываясь  на  истории  аналогичных  покупок?\n",
    " * Какие  различные  группы  вирусов,  основываясь  на  их  характеристиках?\n",
    " \n",
    " Для кластеризации также используют целый ряд математических методов и алгоритмов: k-means, иерархическая кластеризация, генетические алгоритмы, нейронные сети и т.д.\n",
    " \n",
    " *Обнаружение  выбросов*  —  это  процесс  поиска  данных,  которые  существенно  отличаются  от  других  данных  в  заданном наборе  данных  или  несовместимы  с  ними.  Этот  метод  машинного  обучения  используется  для  выявления  аномалий, патологий  и  отклонений,  которые  либо следует исключить из рассмотрения при анализе или наоборот, обратить на них пристальное внимание при анализе возможных рисков.\n",
    " \n",
    " Обнаружение  выбросов  тесно  связано  с  концепцией  классификации  и  кластеризации,  хотя  алгоритмы  и  сосредоточены на  поиске  аномальных  значений.  Процесс  может  основываться  либо  на  контролируемом,  либо  на  неконтролируемом обучении.  \n",
    " \n",
    " Приложения  для  обнаружения  выбросов  включают  обнаружение  мошенничества,  медицинскую  диагностику,  анализ  сетевых  данных  и  анализ  данных  датчиков.\n",
    " \n",
    " Примеры  вопросов  могут  включать  в  себя  следующее:\n",
    " * Является  ли  спортсмен  тем,  кто  использовал  допинговые препараты?\n",
    " * Имеются  ли  неправильно  идентифицированные  фрукты  и овощи  в  учебном  наборе  данных,  который используется  для задачи  классификации?\n",
    " * Существует  ли  особый  штамм  вируса,  который  не  реагирует  на  лекарства?\n",
    " \n",
    "*Фильтрация* - это  автоматизированный  процесс  поиска  релевантных  элементов  из  совокупности  всех  элементов.  Элементы  могут  быть  отфильтрованы  либо  на  основе  собственного  поведения  пользователя,  либо  путем  сопоставления поведения  нескольких  пользователей.  \n",
    " \n",
    " Фильтрация  обычно применяется  с  помощью  следующих  двух  подходов:\n",
    "* коллаборативной  фильтрации\n",
    "* фильтрации  на  основе  контента\n",
    " \n",
    " Широко  распространённым  средством  для  осуществления фильтрации,  является  использование  рекомендационных \n",
    "систем.  \n",
    "\n",
    "Коллаборативная  фильтрация  — это один из методов построения прогнозов (рекомендаций) в рекомендательных системах, использующий известные предпочтения (оценки) группы пользователей для прогнозирования неизвестных предпочтений другого пользователя.\n",
    "\n",
    "Фильтрация  на  основе  контента  —  это  метод  фильтрации элементов,  сфокусированный  на  сходстве  между  пользователями  и  элементами.  Профиль  пользователя  создается  на основе  предыдущего  поведения  пользователя,  например, его  предпочтений,  оценок  и  истории  покупок.  Сходства,  выявленные  между  профилем  пользователя  и  атрибутами  различных  элементов,  приводит  к  элементам,  которые  фильтруются  для  пользователя.  В  противовес  коллаборативной фильтрации,  фильтрация  на  основе  контента  предназначена  исключительно  для  индивидуальных  предпочтений пользователя  и  не  требует  данных  о  других  пользователях. Данный метод, например, используется для реализации контент-фильтра - устройство или программное обеспечение для фильтрации сайтов по их содержимому, не позволяющее получить доступ к определённым сайтам или услугам сети Интернет. Система позволяет блокировать веб-сайты с содержимым, не предназначенным для просмотра.\n",
    "\n",
    "Примеры  вопросов  могут  включать  в  себя:\n",
    " * Как  можно  отображать  только  те  новостные  статьи,  которые  интересуют  пользователя?\n",
    " * Какие  места  для  отдыха  можно  порекомендовать,  основываясь  на  истории  путешествий  отдыхающего?\n",
    " * Каких  других  новых  пользователей  можно  предложить  в  качестве  друзей  на  основе  текущего  профиля  пользователя?\n",
    "\n",
    "*Семантический анализ (Text Mining)*\n",
    "\n",
    "Семантический анализ  представляет  собой  практический  метод  извлечения значимой  информации  из  текстовых  и  речевых  данных.\n",
    "\n",
    "Среди основные методов можно указать:\n",
    "\n",
    "* распознавание голоса\n",
    "* автоматизированное формирование резюме по тексту\n",
    "* тематическая близость текстов\n",
    "* распознавание эмоциональной окраски\n",
    "\n",
    "*Визуальный анализ* является  формой  анализа  данных,  которая  содержит  графическое  представление  данных  для  обеспечения  или  улучшения  их  визуального  восприятия.  Основываясь  на  предположении,  что  людям  проще  и  быстрее  понять  и  сделать  выводы  из  графиков,  чем  из  текстов,  визуальный  анализ  действует  как инструмент  обнаружения  знаний  в  области  больших  данных.\n",
    " \n",
    " Цель  заключается  в  использовании  графических  представлений  для  более  глубокого  понимания  анализируемых  данных.  В  частности,  это  помогает  выявлять  и  подчеркивать скрытые  закономерности,  корреляции  и аномалии.  Визуальный  анализ  также  непосредственно  связан  с  пробным анализом  данных,  поскольку  он  подходит  с разных  сторон  к формулированию  вопросов.\n",
    " \n",
    " Среди методов визуального анализа можно указать:\n",
    " * Цветовые  карты\n",
    " * Временные  ряды\n",
    " * Сетевые  графики\n",
    " * Сопоставление  пространственных  данных\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c4649-c1e0-47ee-9daa-5ae3c2551438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Обзор программных средств анализа больших данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b6b086-cbf1-48e7-ac68-fcb0251973dd",
   "metadata": {},
   "source": [
    "При обзоре программных средств следует говорить про два взаимосвязанных, но самостоятельных направления:\n",
    "1. программные средства создания и содержания инфраструктуры больших данных;\n",
    "2. программные средсва анализа больших данных.\n",
    "\n",
    "В первом направлении нужно выделить фреймворк Hadoop (https://hadoop.apache.org/) для разработки и выполнения распределённых программ, работающих на кластерах из сотен и тысяч узлов. \n",
    "\n",
    "Все началось в 2004 году, когда компания Google опубликовала статью, посвященную процессу MapReduce. Каркас MapReduce использует модель параллельной обработки\n",
    "очень большого объема данных. В рамках MapReduce запросы разделяются и\n",
    "распределяются по параллельным узлам, при этом они обрабатываются\n",
    "параллельно (этап распределения (Мар)). Затем осуществляется сбор и\n",
    "доставка результатов (этап преобразования (Reduce)). \n",
    "\n",
    "\n",
    "Hadoop на данный момент является «де-факто» стандартом создания и обслуживания инфраструктры больших данных. Hadoop представляет собой фреймворк, на основе которого\n",
    "разрабатываются приложения для анализа и визуализации больших данных.\n",
    "Хранение данных в данном фреймворке осуществяляется с помощью\n",
    "специальной распределённой файловой системы HDFS (Hadoop Distributed\n",
    "File System), которая лежит в основе Hadoop и позволяет хранить и\n",
    "предоставлять доступ к данным сразу на нескольких узлах кластера. Таким\n",
    "образом, если один или несколько узлов кластера выходят из строя, то риск\n",
    "потери информации сводится к минимуму и кластер продолжает работу в\n",
    "штатном режиме.\n",
    "\n",
    "В рамках второго направления существует огромное количество различных инструментов. Остановимся на ключевых из них:\n",
    "\n",
    "**WEKA**\n",
    "\n",
    "Weka ( https://www.cs.waikato.ac.nz/~ml/weka/ ), программное обеспечение с открытым исходным кодом, представляет собой набор алгоритмов машинного обучения для задач интеллектуального анализа данных. Алгоритмы могут быть применены непосредственно к набору данных или вызваны из вашего собственного Java-кода. Он также хорошо подходит для разработки новых схем машинного обучения, поскольку полностью реализован на языке программирования Java, а также поддерживает несколько стандартных задач интеллектуального анализа данных. Weka с ее графическим интерфейсом, обеспечивает самый простой переход в мир Data Science. Для пользователей с опытом программирования на Java есть возможность встраивать в библиотеку свой собственный код.\n",
    "\n",
    "<img src=\"ris2.jpg\">\n",
    "\n",
    "**Язык общего назначения Python (https://www.python.org/)**\n",
    "\n",
    "Основные преимущества: \n",
    " - Простой, но выразительный синтаксис.\n",
    " - Богатый выбор библиотек. И речь не только о библиотеках алгоритмов машинного обучения — на Python разрабатывают облачные хранилища, стриминговые сервисы.\n",
    " Ниже перечислены некоторые из библиотек:\n",
    "   - Pandas — библиотека для манипулирования данными с огромными возможностями. Позволяет очень быстро провести исследование новых данных, протестировать гипотезы, получить отчёт. Одно из главных преимуществ Python.\n",
    "   - Scikit-learn — большая библиотека алгоритмов машинного обучения и обработки данных. \n",
    "   - Keras и PyTorch — библиотеки, используемые для обучения глубоких нейронных сетей. Подходят для задач, связанных с изображениями, аудио и видео файлами.\n",
    "   - IPython Notebook — рассказывая о Python нельзя не упомянуть о нём. Стандартная среда разработки не совсем подходит data scientist’у в процессе исследования данных. Есть потребность в таком формате, который позволил бы, например, запустить затратный алгоритм, а когда он завершится — поиграть немного с результатами, исследовать их и построить графики. Здесь на помощь и приходит формат ноутбука. Это графический интерфейс, который открывается в обычном браузере и представляет из себя последовательность ячеек, где можно писать и исполнять код, используя при этом общую память для хранения данных.\n",
    " - Высокая культура документации. Сам Python прекрасно документирован, и обычно библиотеки на нём продолжают эту традицию.\n",
    "    \n",
    "**Язык статистических вычислений R (https://www.r-project.org/)** \n",
    "\n",
    "В 2020 году язык R остаётся одним из самых популярных для Data Science и статистики, стабильно завоёвывая всё большую долю просмотров в соответствующих разделах StackOverflow. При этом, со значительным перевесом лидируют вопросы академического характера: в первую очередь, R — это язык с богатым набором библиотек по машинному обучению и статистике, что особенно важно в исследовательских целях.\n",
    "\n",
    "Основные преимущества:\n",
    " - Огромное количество библиотек статистических методов. R особенно популярен в академической среде, что и приводит к тому, что часто новые методы впервые имплементируются именно на нём.\n",
    " - Достаточно удобная проприетарная среда разработки RStudio, с которой будет легко разобраться.\n",
    " - Необычный синтаксис, заточенный под нужды статистики. \n",
    " - Нативная поддержка векторных вычислений. \n",
    " \n",
    "**Anaconda (https://www.anaconda.com/)**\n",
    " \n",
    "Anaconda — дистрибутив языков программирования Python и R, включающий набор популярных свободных библиотек, объединённых проблематиками науки о данных и машинного обучения. Основная цель — поставка единым согласованным комплектом наиболее востребованных соответствующим кругом пользователей тематических модулей (таких как NumPy, SciPy, Astropy и других) с разрешением возникающих зависимостей и конфликтов.\n",
    "\n",
    "<img src=\"ris3.png\">\n",
    " \n",
    "\n",
    "**Сравнение Python и R**\n",
    "\n",
    "Оба языка обладают своими достоинствами и недостатками. Подойти может любой из них, всё зависит от ваших задач. Вот некоторые моменты, которые могут помочь с выбором:\n",
    "\n",
    "- При программировании  Python гораздо более привычен.\n",
    " - Python больше приближен к продакшену и чаще применяется в коммерческих проектах. В то же время, в академических кругах большей популярностью пользуется R.\n",
    "- Хотите ли вы расширить кругозор в методах машинного обучения? Или вам достаточно будет ознакомиться с несколькими наиболее популярными методами и больше времени посвятить, например, алгоритмам обработки больших данных? В первом случае вам однозначно нужен R, во втором — больше возможностей вы найдёте в Python.\n",
    " - Хотите ли вы заниматься внедрением своих разработок, и программировать что-либо? Если да, то Python вам подойдёт лучше, но скорее всего понадобится и что-то ещё (например Java, Scala или C++)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d08cfd-8048-40e6-9768-f9b0a1331daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "16"
      ],
      "text/latex": [
       "16"
      ],
      "text/markdown": [
       "16"
      ],
      "text/plain": [
       "[1] 16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "7+9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad60bf-728f-4ce6-b06b-e6e83fb57710",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ограничения CAP теоремы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514040d9-60d3-45e3-8a5a-b294a5ace16b",
   "metadata": {},
   "source": [
    "Несмотря на широкое использованием реляционной модели данных, практика ее повсеместного внедрения позволила выявить и ее недостатки. При этом ее основные достоинства, как ни странно, оказываются и ее важными ограничениями и недостатками при использовании реляционной модели в ряде предметных областей, набирающих в последнее время существенную популярность. В первую очередь это касается области больших данных с требованием их распределенного хранения. Также особые сложности возникают при сопровождении реляционной базы данных в течение длительного времени с требованием сохранить хронологию изменений, или требованиями изменения самой структуры реляционной базы данных. \n",
    "\n",
    "Рассмотрим возникающие ограничения по использованию реляционной модели более подробно. \n",
    "Термин Big Data появился в конце 2000 годов и одна из его составляющих означает массив структурированных и неструктурированных данных значительного объема (принципиально не может поместиться за раз не только в оперативной памяти отдельного компьютера, но и на винчестере одного компьютера). В этой ситуации приходится каким-то способом дробить данные на части, хранить их распределенно, синхронизировать их при выдаче. Часто при этом данные подвержены очень быстрым изменениям. Все это накладывает определенные требования на информационные системы, служащие для обработки таких данных. \n",
    "\n",
    "Обычно к информационной системе в отношении обработки данных предъявляют следующий состав требований:\n",
    "\n",
    "- **Согласованность (consistency - С) данных**  - означает, что данные не противоречат друг другу;\n",
    "- **Доступность (availability - A)** – каждый запрос к информационной системе дает ответ без существенной задержки по времени;\n",
    "- **Масштабируемость (partition - P)** – имеется принципиальная возможность сегментации данных для хранения их на удаленных сегментах распределенной информационной системы.\n",
    "\n",
    "Возможность совместного удовлетворения этих требований показано на рисунке. На данном рисунке проведена аналогия с тремя другими требованиями к разработке любой сложной технической системе: качество, время, стоимость. Практика построения таких систем показывает, что, как правило, всегда удается добиться только любых двух из этих трех требований.\n",
    "\n",
    "![ris1.png](ris1.png)\n",
    "\n",
    "В 2000 году Эрик Брюер сформулировал аналогичное эмпирическое утверждение относительно совместного достижения требований C, A, P к возможностям хранения и обработки данных в информационных системах. Данное утверждение известно как \n",
    "\n",
    "**CAP-теорема: на практике всегда можно добиться одновременного выполнения только любых двух из этих трех требований: C - согласованность, A - доступность, P – масштабируемость.**\n",
    "\n",
    "В дальнейшем некоторые ученые подвергали это утверждение критике за его недостаточно строгую формулировку.\n",
    "Однако в 2002 году ученые из MIT подобрали формальные модели асинхронных и синхронных распределённых вычислений, в которых CAP теорема выполняется. После чего многие авторы ссылаются на CAP гипотезу как на доказанную теорему.\n",
    "\n",
    "Таким образом, в соответствии с CAP теоремой возможно построение трех классов систем: CA системы, AP системы, CP системы.\n",
    "\n",
    "Как видно из рисунка, информационные системы, построенные с использованием реляционной модели данных (MySQL, MariaDB, MS SQL Server и др.), относятся именно к CA системам, а значит на пути их масштабирования всегда будут возникать сложности.\n",
    "\n",
    "В этой связи, описывая два других класса систем CP, AP, принято использовать обозначение NoSQL, указывающего на отказ в той или иной степени от реляционной модели данных. Концепция NoSQL использует CAP теорему, как обоснование отказа от попыток добиться полной согласованности данных, или их доступности при их распределенном хранении. \n",
    "\n",
    "Следующий вид ограничений, которые сопровождают реляционную модель связан с ее структурой, учитывающей требования нормализации данных.\n",
    "\n",
    "В таблицах реляционной модели целесообразно исключать атрибуты, в которых большинство записей имеют значение NULL, обозначающее отсутствие значения. Если мы хотим указать, что один из ста служащих имеет особую квалификацию, для хранения этой информации не следует добавлять в таблицу еще один столбец, поскольку для остальных 99 работников значением столбца будет NULL. Вместо этого следует добавить новую таблицу, в которой будут храниться только кодовые номера и информация о квалификации тех работников, которых это касается. Решение перечисленных проблем состоит в разделении данных и связей, что и обеспечивается процедурой нормализации. Однако в условиях длительной эксплуатации базы данных такие ситуации неизбежно возникают, что приводит к необходимости добавления новых таблиц. Их количество со временем может увеличиться до неприемлемого значения. \n",
    "\n",
    "Но еще большая проблема связана с фиксированной жёсткой структурой базы данных. Дело в том, что в процессе эксплуатации базы данных меняются не только значения атрибутов, но и их состав, и, самое главное, тип связей между сущностями. Добавление новых атрибутов в таблицу требует возобновить анализ на выявление зависимостей этого атрибута с другими атрибутами, требует, как правило, переработки программных приложений, обслуживающих базу данных. Это приводит к потере соответствия между моделью представления данных в предметной области и моделью в реляционном подходе. \n",
    "\n",
    "Еще более затратно изменение структуры базы данных. Допустим, две таблицы были связаны в отношении один ко многим, однако в дальнейшем отношение поменялось на многие ко многим (например, в базе данных учебного отдела таблица группа связана с таблицей студенты в отношении один ко многим, однако в дальнейшем выяснилось, что один и тот же студент может учиться в разных группах по разным направлениям и формам обучения). Обычно это решается добавлением новой таблицы, с которой две указанные связываются отношением один ко многим, а также существенной переработкой программных приложений, обслуживающих базу данных. Но при этом изменение структуры происходит без возможности ее отката назад. Иными словами, хронология структуры базы данных в реляционной модели никак не учитывается. Она в каждый момент времени остается жесткой и фиксированной.\n",
    "\n",
    "Также в настоящее время к данным из БД применяются различные методы многомерного анализа данных, которые приводят к использованию методов Data Mining, OLAP-технологий. При этом многие исследователи показываю, что при проведении OLAP анализа часто требуется расширять и дополнять данные, делать их избыточными, что может приводить к потере согласованности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f88efd-0e0d-42a7-920d-fac870b8a5cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Понятие NoSQL, классификация NoSQL-решений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f0909-909c-4862-8d30-56cf9d36db6c",
   "metadata": {},
   "source": [
    "За многолетнюю историю развития СУБД было выработано несколько моделей представления структурированной информации: иерархическая, сетевая, реляционная, объектно-реляционная и т. д. Наибольшее распространение получила реляционная модель, предложенная Эдгаром Коддом в 1970 году, которая надолго стала стандартом представления структурированной информации. \n",
    "\n",
    "Причины этого вытекают из сформулированных требований к СУБД. Во-первых, реляционная модель оказалась достаточно простой, с точки зрения прикладного программиста. Во-вторых, она обладает достаточной гибкостью и позволяет представлять информацию из самых разных предметных областей. В-третьих, в рамках этой модели используется мощный и удобный подход к манипулированию данными (реляционная алгебра), впоследствии оформленный в виде языка SQL. В-четвертых, простота и естественность реляционной алгебры позволили создать высокопроизводительные и универсальные алгоритмы выполнения запросов, удовлетворяющие большую часть потребностей разработчиков прикладных систем. Наконец, к сегодняшнему дню создано множество инструментальных средств, ориентированных на обработку реляционных данных, что дает еще одно преимущество реляционных СУБД.\n",
    "Итак, при разработке прикладного ПО из исходных требований следует выбор модели представления информации, а уже из нее следует выбор конкретной СУБД. Эта схема может показаться абстрактной в силу того, что на текущий момент доминирующей является реляционная модель, а выбор СУБД производится из достаточно ограниченного списка, который заранее продиктован другими факторами, такими как интеграция, простота поддержки, стоимость и т. д.\n",
    "\n",
    "Реляционная модель предполагает оперирование только атомарными данными и исключает обработку какой-либо неструктурированной информации, а это приводит к тому, что хранение графических, аудио- или видеоданных становится невозможным. Более того, невозможно даже хранение текстовых документов произвольной длины. Поэтому, первым отступлением от классической реляционной модели в сторону удобства разработки прикладного ПО можно считать введение типа BLOB (Binary Large Object — «бинарные данные большого объема»), который сегодня поддерживается большинством современных СУБД и закреплен в стандартах языка SQL. Каждая СУБД, помимо BLOB, может иметь свои собственные «дополнительные» типы данных, которые обычно требуют введения дополнительных поисковых операций (например, полнотекстовый поиск по BLOB). Это еще дальше уводит от классической реляционной модели.\n",
    "\n",
    "По мере роста объемов хранимой информации и сложности ее внутренних взаимосвязей стали возникать новые проблемы. Увеличение сложности SQL-запросов привело к тому, что СУБД уже далеко не всегда способны выработать оптимальный план выполнения поступившего запроса. На практике это производится двумя способами. Первый — в тексте SQL указываются «подсказки», помогающие оптимизатору запросов выработать наиболее эффективный план. Второй — вместо запроса используется хранимая процедура. Причина хорошей применимости обоих способов кроется в том, что встроенные в СУБД оптимизаторы запросов располагают только статистической информацией о содержимом базы и строят планы выполнения запросов только на ее основе, а разработчик прикладного ПО всегда обладает гораздо большими сведениями.\n",
    "\n",
    "Оба отступления от базовых принципов реляционной модели (введение неатомарных типов данных и тонкое управление алгоритмами обработки) напрямую следуют из особенностей самой модели, провозглашающей универсальность способов хранения информации и алгоритмов ее обработки. За этой универсальностью скрывается невозможность учета специфики данных, позволяющей существенно оптимизировать работу алгоритмов.\n",
    "\n",
    "Допустим, требуется организовать хранение большого множества чисел и быстро выполнять операции поиска, добавления и удаления числа. Для этого в реляционной базе создается таблица с одним числовым полем, а по этому полю формируется поисковый индекс на основе B-дерева. Такое решение работает быстро, но предполагает дублирование информации (числа хранятся и в самой таблице, и в поисковом индексе). Более эффективным было бы использование только B-дерева, но реляционные СУБД не всегда способны это определить.\n",
    "\n",
    "Данный пример может показаться слишком простым, но похожая ситуация реализована в Triple Storages (хранение графов семантических сетей в семантическом Web), отвечающих за хранение и обработку графов семантических связей. Каждая дуга графа представляет собой три числа (два кода вершин и один код окраски). Множество всех дуг можно было бы хранить в виде реляционной таблицы с необходимыми поисковыми индексами, но вместо этого хранят только поисковые индексы, поскольку каждый из них содержит всю информацию о каждой дуге.\n",
    "\n",
    "Третий пример — полнотекстовые поисковые системы Интернета. Их база данных должна содержать поисковые слова и коды документов, в которых эти слова являются ключевыми. Это несложно сделать с помощью реляционной таблицы, состоящей из двух полей — поисковое слово и код документа. Если слово встречается в нескольких документах или документ имеет несколько ключевых слов, то таблица содержит несколько записей.\n",
    "Возможна существенная оптимизация рассмотренного примера: скажем, вместо текстового представления слова можно хранить его целочисленный код, а вместо кода документа хранить BLOB, где будут содержаться коды всех документов, имеющих отношение к данному слову. Дальнейшая оптимизация в рамках реляционного подхода невозможна, но ее можно провести посредством так называемого инвертированного индекса. Коды документов сортируются по возрастанию, и в полученном списке вычисляются разницы между кодами соседних документов. Получается последовательность, состоящая из кода первого документа и множества чисел небольшой величины. Она хорошо сжимается алгоритмами энтропийной компрессии (например, алгоритмом Лемпеля — Зива-Велча), в результате чего значительно сокращается объем ввода/вывода, ускоряется загрузка информации с диска и в конечном счете растет скорость полнотекстового поиска.\n",
    "\n",
    "Термин **NoSQL** стал использоваться в конце 90-х годов, но реальный смысл в том виде, как он используется сейчас, приобрел только в середине 2009 года. Изначально так называлась открытая база данных, созданная Карло Строззи, которая хранила все данные как текстовые файлы и использовала shell скрипты вместо SQL для доступа к данным. \n",
    "\n",
    "Таким образом, термин NoSQL имеет стихийное происхождение и скорее характеризует вектор развития IT-сферы в сторону от реляционных баз данных. Расшифровывается как Not Only SQL, хотя есть сторонники и прямого определения No SQL.\n",
    "\n",
    "NoSQL стал общим термином для различных баз данных и хранилищ, но он не обозначает какую-либо одну конкретную технологию или продукт.\n",
    "\n",
    "Выделим основные тренды в формировании представлений о NoSQL.\n",
    "\n",
    "1. Первый тренд — увеличение объемов хранимых данных. Сегодня хранилища достигли таких невероятных размеров, nолько за 2009 и 2010 годы в базах было сохранено больше информации, чем за всю предыдущую историю человечества.\n",
    "2. Второй тренд — взаимосвязанность данных. Информация перестала быть изолированной. Каждый кусочек знаний как-то связан с данными в других хранилищах информации. Страницы в интернете ссылаются на другие страницы. Тэги связывают помеченную информацию из разных источников. Онтологии устанавливают взаимосвязи между различными терминами, и т.д.\n",
    "3. Третий тренд — использование слабоструктурированной информации. Возьмем простой пример: описание товара в магазине. Если раньше было достаточно 5-6 полей, чтобы описать одежду (размер, цвет, материал, фотография товара, …), то теперь количество параметров может доходить до нескольких десятков. Причем, для разных типов одежды будет использован разный набор параметров. В таких условиях становится крайне сложно заранее определить структуру таблицы, в которой хранится описание товара.\n",
    "4. Четвертый тренд — архитектура. В 80-х годах прошлого века типичная архитектура использовала один большой компьютер (mainframe) и одну базу данных. В 90-х, распространение получила клинт-серверная архитектура. В наше время активно используются web-сервисы, каждый со своим backend-ом (со своей базой данных) и  распределенными решениями.\n",
    "\n",
    "В таких условиях у реляционных баз данных резко падает производительность. И если для большинства web-сайтов производительности еще хватает, то для таких приложений как современные социальные сети или поисковые сервисы, SQL базы данных оказались несостоятельны.\n",
    "\n",
    "Для большего понимания проведем сравнение реляционных и NoSQL решений:\n",
    "\n",
    "- Стуруктуры данных и их типы - реляционные БД используют строгие схемы данных, NoSQL БД допускают любой тип данных.\n",
    "- Запросы - вне зависимости от типа лицензии, реляционные базы данных в той или иной мере соответствуют стандартам SQL, поэтому данные из них можно получать при помощи языка SQL. NoSQL БД используют специфические способы запросов к данным.\n",
    "- Масштабируемость - оба эти типа СУБД довольно легко поддаются вертикальному масштабированию (т.е. увеличение системных ресурсов). Тем не менее, так как NoSQL это более современный продукт, именно такие СУБД предлагают более простые способы горизонтального масштабирования (т.е. создание кластера из нескольких машин).\n",
    "- Надежность - когда дело доходит до сохранности данных и гарантии выполнения транзакций реляционные БД по прежнему занимают лидирующие позиции.\n",
    "- Поддержка - Реляционные СУБД имеют не малую историю за плечами. Они очень популярны и предлагают как платные, так и бесплатные решения. При возникновении проблем, все же гораздо проще найти ответ, если дело касается реляционных систем, чем NoSQL, особенно если решение довольно сложное по своей природе (например MongoDB).\n",
    "- Хранение и доступ к данных - изначально реляционные системы предполагали быструю работу с данными, именно поэтому они превосходят остальные решения по производительности.\n",
    "\n",
    "Как было показано на рисунке, существует два класса NoSQL систем: AP системы: CouchDB, Cassandra, Riak  и др.; CP системы: MongoDB, Redis, HBase и др.\n",
    "\n",
    "Внутри каждого класса систем проводят также классификацию по принципам реализации хранилища данных.\n",
    "Всего выделяют 4 типа: \n",
    "\n",
    "- 1 тип – хранилище по принципу ключ-значение; \n",
    "- 2 тип  - документно-ориентированное хранилище; \n",
    "- 3 тип - графовое хранилище; \n",
    "- 4 тип - колоночное хранилище.\n",
    "\n",
    "**Хранилище по принципу ключ-значение**\n",
    "\n",
    "Ключ может быть синтетическим или автосгенерированным, а значение может быть представлено строкой, JSON, блобом (BLOB, Binary Large Object) и т.д.\n",
    "\n",
    "Такие базы данных как правило используют хеш-таблицу, в которой находится уникальный ключ и указатель на конкретный объект данных. Существует понятие блока (bucket) — логической группы ключей, которые не группируют данные физически. В разных блоках могут быть идентичные ключи.\n",
    "\n",
    "Производительность сильно вырастает за счёт кеширующих механизмов, которые работают на основе маппингов. Чтобы прочитать значение, вам нужно знать как ключ, так и блок, поскольку на самом деле ключ является хешем (блок + ключ).\n",
    "\n",
    "<img src=\"ris22.png\" width=500 hight=500>\n",
    "\n",
    "База данных такого типа позволяет читать и записывать значения с помощью ключа следующим образом:\n",
    "- Get(key) возвращает значение, связанное с переданным ключом;\n",
    "- Put(key, value) связывает значение с ключом;\n",
    "- Multi-get(key1, key2, ..., keyN) возвращает список значений, связанных с переданным ключами;\n",
    "- Delete(key) удаляет запись для ключа из хранилища.\n",
    "\n",
    "И хотя базы данных типа «ключ-значение» могут пригодиться в определённых ситуациях, они не лишены недостатков. Первый заключается в том, что модель не предоставляет стандартные возможности баз данных вроде атомарности транзакций или согласованности данных при одновременном выполнении нескольких транзакций. Такие возможности должны предоставляться самим приложением.\n",
    "Второй недостаток в том, что при увеличении объёмов данных, поддержание уникальных ключей может стать проблемой. Для её решения необходимо как-то усложнять процесс генерации строк, чтобы они оставались уникальными среди очень большого набора ключей.\n",
    "\n",
    "Такие системы, как правило, относятся к классу AP систем.\n",
    "\n",
    "Наиболее популярны в этой области: [Redis](https://ru.wikipedia.org/wiki/Redis), [Riak](https://ru.wikipedia.org/wiki/Riak), [Amazon DynamoDB](https://ru.wikipedia.org/wiki/DynamoDB).\n",
    "\n",
    "**Документно-ориентированное хранилище** позволяет эффективно хранить и индексировать иерархические структуры данных. Такие системы в основном используются в документированных информационных системах.\n",
    "\n",
    "В принципе здесь используется тот же самый подход ключ-значение, однако в качестве значений могут выступать структуры, которые также включают в себя наборы ключ-значение. Такие значения рассмотривают как документы. Поддерживаются различные форматы: XML, JSON и BSON — некоторые из наиболее распространенных.\n",
    "В следующем примере можно увидеть данные в виде «документа» который отображает названия определённых магазинов. Обратите внимание, что, хотя все три примера содержат местоположение, они отображают его по-разному:\n",
    "\n",
    "{officeName: \"3Pillar Noida\", \n",
    "   {Street: \"B-25\", City: \"Noida\",\n",
    "   State: \"UP\", Pincode: \"201301\"}\n",
    "}\n",
    "{officeName: \"3Pillar Timisoara\",\n",
    "   {Boulevard: \"Coriolan Brediceanu No. 10\",\n",
    "   Block: \"B, Ist Floor\",\n",
    "   City: \"Timisoara\", Pincode: \"300011\"}\n",
    "}\n",
    "{officeName: \"3Pillar Cluj\",\n",
    "   {Latitude: \"40.748328\",\n",
    "   Longitude: \"-73.985560\"}\n",
    "}\n",
    "\n",
    "Одним из ключевых различий между хранилищем «ключ-значение» и документоориентированным является то, что последний включает метаданные, связанные с хранимым содержимым, что даёт возможность делать запросы на основе содержимого. Например, в указанном примере можно попробовать найти все документы, в которых «City» равно «Noida», что вернёт все документы, связанные с магазинами в этом городе.\n",
    "\n",
    "Как правило, такие хранилища относятся к CP системам.\n",
    "\n",
    "Наиболее популярны в этой области: [CouchDB](https://ru.wikipedia.org/wiki/CouchDB), [MongoDB](https://ru.wikipedia.org/wiki/MongoDB)\n",
    "\n",
    "\n",
    "**Графовые хранилища** наиболее подходят для хранения сложных бинарных отношений, возникающих между элементами данных информационной системы. \n",
    "\n",
    "Графовые структуры используются вместе с рёбрами, узлами и свойствами, что обеспечивает безиндексную смежность. При использовании графового хранилища данные могут быть легко преобразованы из одной модели в другую.\n",
    "\n",
    "<img src=\"ris33.png\" width=500 height=500>\n",
    "\n",
    "Примерами графовых баз можно назвать: [Neo4j](https://ru.wikipedia.org/wiki/Neo4j), [Amazon Neptune](https://ru.wikipedia.org/wiki/Amazon_Neptune).\n",
    "\n",
    "**Колоночные хранилища** представляют данные в виде некоторой разряженной матрицы, в которой строки и столбцы являются ключами с временными метками. Этот подход применяется для высоконагруженных систем, к которым предъявляется требование высокой скорости отклика. \n",
    "\n",
    "В колоночных NoSQL базах данных данные хранятся в ячейках, сгруппированных в колонки, а не в строки данных. Колонки логически группируются в колоночные семейства. Колоночные семейства могут состоять из практически неограниченного количества колонок, которые могут создаваться во время работы программы или во время определения схемы. Чтение и запись происходит с использованием колонок, а не строк.\n",
    "В сравнении с хранением данных в строках, как в большинстве реляционных баз данных, преимущества хранения в колонках заключаются в быстром поиске/доступе и агрегации данных. Реляционные базы данных хранят каждую строку как непрерывную запись на диске. Разные строки хранятся в разных местах на диске, в то время как колоночные базы данных хранят все ячейки, относящиеся к колонке, как непрерывную запись, что делает операции поиска/доступа быстрее.\n",
    "\n",
    "Пример: получение списка заголовков нескольких миллионов статей будет трудоёмкой задачей при использовании реляционных баз данных, так как для извлечения заголовков придётся проходить по каждой записи. А можно получить все заголовки с помощью только одной операции доступа к диску.\n",
    "\n",
    "Двумерная таблица, состоящая из строк и колонок, является частью реляционной системы баз данных.\n",
    "\n",
    "<img src=\"ris44.png\" height=500 width=500>\n",
    "\n",
    "Эту таблицу можно представить в виде BigTable-сопоставления следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0ac75-e556-4d22-9fe4-16bd87172b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "   3PillarNoida: {\n",
    "      city: Noida\n",
    "      pincode: 201301\n",
    "   },\n",
    "   details: {\n",
    "      strength: 250\n",
    "      projects: 20\n",
    "   }\n",
    "}\n",
    "{\n",
    "   3PillarCluj: {\n",
    "      address: {\n",
    "         city: Cluj\n",
    "         pincode: 400606\n",
    "      }, \n",
    "      details: {\n",
    "         strength: 200\n",
    "         projects: 15\n",
    "      }\n",
    "   },\n",
    "{\n",
    "   3PillarTimisoara: {\n",
    "      address: {\n",
    "         city: Timisoara\n",
    "         pincode: 300011\n",
    "      },\n",
    "      details: {\n",
    "         strength: 150\n",
    "         projects: 10\n",
    "      }\n",
    "   }\n",
    "{\n",
    "   3PillarFairfax : {\n",
    "      address: {\n",
    "         city: Fairfax\n",
    "         pincode: VA 22033\n",
    "      }, \n",
    "      details: {\n",
    "         strength: 100\n",
    "         projects: 5\n",
    "      }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd50c3d-0fc6-4a27-800d-5684ebbf81f4",
   "metadata": {},
   "source": [
    "- Внешние ключи «3PillarNoida», «3PillarCluj», «3PillarTimisoara» и «3PillarFairfax» являются аналогами строк.\n",
    "- «address» и «details» — колоночные семейства.\n",
    "- В колоночном семействе «address» есть колонки «city» и «pincode».\n",
    "- В колоночном семействе «details» есть колонки «strength» и «projects».\n",
    "На колонки можно ссылаться с помощью колоночного семейства.\n",
    "\n",
    "Самыми известными системами в этом классе являются: [BigTable](https://ru.wikipedia.org/wiki/BigTable), [Cassandra](https://ru.wikipedia.org/wiki/Apache_Cassandra), [HBase](https://ru.wikipedia.org/wiki/HBase).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb46023-ae08-4029-87b7-35b9d184f5b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Распределенные хранилища данных. Технологии шардинга, репликации, MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbea6e2-0805-480d-a1f0-2e8f7a834367",
   "metadata": {},
   "source": [
    "При организации распределенного хранилища используется ряд специфических понятий: кластер, шардинг, репликация, технология MapReduce. \n",
    "\n",
    "При  вычислении  **кластер**  представляет  собой  тесно  связанную  совокупность  серверов  или  узлов.  Эти  серверы,  как правило,  имеют  одинаковую  аппаратную  спецификацию  и  соединены  вместе  через  сеть  для  работы  в  качестве  единого  блока.  Каждый  узел  в  кластере  имеет  свои  собственные  выделенные  ресурсы,  такие  как  память,  процессор  и  жесткий  диск.  Кластер  может  выполнять  задачу,  разбивая  ее  на  мелкие  кусочки  и  распределяя  их  выполнение  на  разных  компьютерах,  принадлежащих  кластеру.\n",
    "\n",
    "**Файловая  система**  —  это  способ  хранения  и  организации  данных  на  запоминающих  устройствах,  таких  как  флэш-накопители,  DVD-диски  и  жесткие  диски.  Файл  —  это  неделимая  единица  хранения,  используемая файловой  системой  для  хранения  данных. \n",
    "Файловая  система  обеспечивает  логическое представление  данных,  хранящихся  на  запоминающем  устройстве,  и  представляет  его в  виде  древовидной  структуры  каталогов  и \n",
    "файлов.  Операционные  системы  используют  файловые  системы  для  хранения  и  считывания  данных \n",
    "по  поручению  приложений.  Каждая  операционная  система поддерживает  одну  или  несколько  файловых  систем,  например  NTFS  в  Microsoft  Windows  и  ext  в  Linux.\n",
    "\n",
    "**Распределенная  файловая  система**  —  файловая  система,  которая  может  хранить  большие  файлы,  распределенные  по узлам  кластера.  Для  клиента  файлы  выглядят  как  локальные  объекты;  однако  это  только  логическое  представление, так  как  физически  файлы распределены  по  всему  кластеру.  Эти  локальные  отображаемые  элементы  представлены  благодаря  распределенной файловой  системе,  которая  позволяет  осуществлять  доступ к  файлам  из  разнообразных  мест.  Примерами  могут  быть  файловая  система  Google  ([GFS](https://ru.wikipedia.org/wiki/Google_File_System_))  и  распределенная  файловая система  Hadoop  ([HDFS](https://ru.wikipedia.org/wiki/Hadoop)).\n",
    "\n",
    "**Шардинг**—  это  процесс  горизонтального  разделения  большого  массива  данных  на  коллекцию  меньших,  которые  являются  более  управляемыми  и  называются  **шардами**.  \n",
    "\n",
    "Шарды  распределены  среди  множества  узлов,  где  узел  —  это  сервер или  компьютер.  \n",
    "\n",
    "Каждый  сегмент  совместно  использует одну  и  ту  же  логическую  структуру  в  базе  данных,  и  все  вместе  шарды  представляют  собой  полный  набор  данных.\n",
    "\n",
    "<img src=\"ris55.png\" width=500 height=500>\n",
    "\n",
    "Шардинг  позволяет  распределять  нагрузки  по  обработке  между  несколькими  узлами  для  достижения  горизонтальной  масштабируемости.\n",
    "\n",
    "Горизонтальное  масштабирование  —  это  способ  увеличения  производительности  системы  за  счет  добавления  аналогичных  или  более  мощных  ресурсов  наряду  с  существующими  ресурсами.  Поскольку  каждый  узел  отвечает  только  за  часть  всего  массива  данных,  то  при  этом  время  чтения/записи \n",
    "значительно  улучшается.\n",
    "\n",
    "Каждый  сегмент  может  независимо  обслуживать  операции  чтения  и  записи  для  определенного  подмножества данных,  за  которые  он  отвечает.\n",
    "\n",
    "В  зависимости  от  запроса  данные  могут  быть  выбраны  из обоих  сегментов.\n",
    "\n",
    "Преимуществом  шардинга  является  то,  что  оно  обеспечивает  частичную  отказоустойчивость  к  сбоям.  В  случае  сбоя узла,  только  данные  хранящиеся  на  этом  узле  подвержены влиянию.  Что  касается  разбиения  данных,  то  необходимо  учитывать  шаблоны  запросов,  чтобы  сами  сегменты  не  становились  узкими  местами  производительности. Например,  запросы,  требующие  данные  из  нескольких  сегментов,  накладывают  ограничение  на  производительность. \n",
    "\n",
    "Локальное  размещение  данных  позволяет  хранить  общедоступные  данные,  расположенные  в  одном  сегменте,  и  помогает  противостоять  таким  проблемам  с  производительностью.\n",
    "\n",
    "**Репликация**  хранит  множество  копий  наборов  данных,  известных  как  «реплики»,  на  нескольких  узлах.\n",
    "\n",
    "Репликация  обеспечивает  масштабируемость  и  доступность  благодаря  тому  факту,  что  одни  и  те  же  данные  реплицируются  на  разных  узлах.  При  этом  также  достигается  отказоустойчивость,  поскольку  избыточность  данных  гарантирует, что  данные  не  будут  потеряны  в  случае,  когда  отдельный \n",
    "узел  выходит  из  строя.  \n",
    "\n",
    "Для  реализации  репликации  используются  два  разных  метода:\n",
    "- ведущий-ведомый\n",
    "- равный  к  равному  (пиринговый)\n",
    "\n",
    "Во  время  однонаправленного  режима  репликации  «**ведущий-ведомый**»  узлы  размещаются  в  конфигурации  ведущий-ведомый,  и  все  данные  записываются  в  главный  узел. \n",
    "\n",
    "После  сохранения  данные  реплицируются  на  несколько  подчиненных  узлов.  Все  внешние  запросы  на  запись,  включая вставку,  обновление  и  удаление  данных,  происходят  на  главном  узле,  в  то  время,  как  запросы  на  чтение  могут  выполняться  любым  подчиненным  узлом.\n",
    "\n",
    "Если  ведущий  узел  выйдет  из  строя,  то  чтение  по-прежнему  будет  возможным  через  любой  из  ведомых узлов.\n",
    "\n",
    "Ведомый  узел  может  быть  сконфигурирован  как  резервный узел  для  ведущего  узла.  И  в  случае  сбоя  основного  узла  запись  не  будет  поддерживаться  до  тех  пор,  пока  ведущий  узел \n",
    "не  будет  восстановлен.\n",
    " \n",
    "Ведущий  узел  восстанавливается  либо  из  резервной  копии  ведущего  узла,  либо  из  подчиненных  узлов  выбирается  новый  ведущий  узел.  Одна  из  проблем,  связанных  с  однонаправленным  режимом  репликации  «ведущий-ведомый»,— это  несогласованность  чтения,  что  может  быть  проблемой,  если  ведомый  узел  читается  до  обновления  ведущего,  который  его  копирует.\n",
    " \n",
    "Для  обеспечения  согласованности  чтения,  там  может  быть  реализована  система  голосования  в  том  случае,  если  чтение объявлено  согласованным,  а  большинство  ведомых  узлов содержат  одну  и  ту  же  версию  записи.  Для  внедрения  такой  системы  голосования  требуется  надежный  и  быстрый  механизм  обмена  данными  между  ведомыми  узлами.\n",
    "\n",
    "При  репликации  в  режиме  «**одноранговый**»  все  узлы  работают  на  одном  уровне.  Другими  словами,  между  узлами  нет  отношения  «ведущий-ведомый».  Каждый  узел,  известный  как  одноранговый  узел,  в  равной  степени  способен  обрабатывать операции  чтения  и  записи.  Каждая  запись  копируется  во  все \n",
    "одноранговые  узлы.\n",
    "\n",
    "Одноранговая  репликация  подвержена  ошибкам  записи,  возникающим  в  результате  одновременного  обновления  одних тех  же  данных  несколькими  одноранговыми  узлами.  Эту  проблему  можно  решить  путем  реализации  либо  пессимистической,  либо  оптимистической  стратегии  распараллеливания.\n",
    "\n",
    "- Пессимистический  параллелизм  представляет  собой  упреждающую  стратегию,  которая  предотвращает  несогласованность.  Она  использует  блокировки  для  гарантии  того,  что  только  одно  обновление  на  запись  может  происходить  одновременно.  Однако  это  наносит  ущерб  доступности,  поскольку  обновляемая  запись  базы  данных  остается  недоступной  до  тех  пор,  пока  не  будут  освобождены  все  блокировки.\n",
    "- Оптимистичный  параллелизм  —  это  реактивная  стратегия,  которая  не  использует  блокировки.  Вместо  этого  она допускает  несогласованность,  предполагая,  что  в  конечном  итоге  согласованность  будет  достигнута  после  того, как  все  обновления  распространятся.\n",
    "\n",
    "При  оптимистичном  параллелизме  одноранговые  узлы  могут  оставаться  несогласованными  в  течение периода  времени,  прежде  чем  достигнут  согласованности.  \n",
    "Однако  база  данных  остается  доступной,  поскольку  блокировка не  задействована.  Подобно  режиму  однонаправленной  репликации  «ведущий-ведомый»,  чтение  может  быть  несогласованным  в  течение  периода  времени,  когда  некоторые  из одноранговых  узлов  завершили  свои  обновления,  в  то  время как  другие  еще  выполняют  свои  обновления.  Тем  не  менее, чтение  в  конечном  итоге  становится  согласованным,  когда обновления  были  выполнены  для  всех одноранговых узлов.\n",
    "\n",
    "Для  обеспечения  согласованности  чтения,  там  может  быть реализована  система  голосования  в  том  случае,  если  чтение  объявлено  согласованным,  а  большинство  одноранговых  узлов  содержат  одну  и  ту  же  версию  записи.  Как  отмечалось ранее,  внедрение  такой  системы  голосования  требует  надежного  и  быстрого  механизма  обмена  данными  между  одноранговыми  узлами.\n",
    "\n",
    "Для  улучшения  ограниченной  отказоустойчивости,  обеспечиваемую  с  помощью  использования  приемов  шардинга,  а  также  для  получения  дополнительных  преимуществ  от повышенной  доступности  и масштабируемости  репликации,  можно  комбинировать как шардинг,  так  и  репликацию.\n",
    "\n",
    "\n",
    "Возможности обработки запросов NoSQL базы данных напрямую следуют из ее модели распределения. \n",
    "\n",
    "Популярное решения для распределенных вычислений кластеров NoSQL базы данных – **MapReduce**. \n",
    "\n",
    "MapReduce работает в два шага:\n",
    "1. Map-шаг: предварительная обработка полученных данных. На этом шаге Мастер или один из Мастер-серверов получает данные задачи, разделяет их на части и передает для предварительной обработки другим узлам.\n",
    "2. Reduce-шаг: свертка обработанных данных. Мастер получает от прочих узлов подготовленные данные, формирует и отдает ответ, который является решением входной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b47ddd-ccf4-4d1b-a3fb-922ffdc39616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
