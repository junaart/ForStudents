{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d09e67-e647-4b83-b464-33079b92d6dd",
   "metadata": {},
   "source": [
    "# Лекция 6. Обучение с учителем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884f221-1fc5-477e-8efd-e66a638ff1f3",
   "metadata": {},
   "source": [
    "- Регрессионный анализ, многомерная регрессия \n",
    "- Дискриминантный анализ\n",
    "- Метод опорных векторов\n",
    "- Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0d12c-3cbd-40c7-8e63-100f19dfb52d",
   "metadata": {},
   "source": [
    "## Регрессионный анализ, многомерная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0a766-67ca-4940-bc9c-c12fd3971e50",
   "metadata": {},
   "source": [
    "Кроме решения задач кластеризации часто возникают и задачи классификации - отнесение объекта к одному из нескольких известных классов. Для этого обычно имеется некоторый набор данных, по которому нам известны характеристики некоторых объектов, а также принадлежность их к классам. Тогда требуется построить некоторую математическую модель, которая по заданным характеристикам объекта относит его к тому или иному классу. Фактически построение такой модели с целью минимизации ошибки и называется обучением.\n",
    "\n",
    "Наиболее традиционным и в тоже время более тонким методом является **регрессионный анализ**, в котором по заданному вектору значений характеристик объекта (влияющих факторов) $X_j=(x_{j1},x_{j2},\\ldots,x_{jn}), j=\\overline{1\\ldots k}$, а также по вектору значений результативного фактора $Y_j, j=\\overline{1\\ldots k}$ строиться функций $F(X_j)\\sim Y_j$, которая минимизирует функционал:\n",
    "$$\\sum_{j=1}^k\\left(Y_j-F(X_j)\\right)^2\\rightarrow\\min$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d09b4cd-bc19-4a1e-ae59-5484502c6235",
   "metadata": {},
   "source": [
    "В качестве такой функции часто выбырают линейную функцию:\n",
    "$F(X_j)=a_0+\\sum_{i=1}^n a_i\\cdot X_{ji}, \\forall i=\\overline{0,\\ldots,n}: a_i=const$.\n",
    "\n",
    "Можно рассмотреть частные случаи: $F(X_j)=a_0+a_1\\cdot x_{j1}$, для которых используется метод наименьших квадратов.\n",
    "\n",
    "$$G(a_0,a_1)=\\sum_{j=1}^{k}\\left(a_0+a_1\\cdot x_{j1}-Y_j\\right)^2\\rightarrow\\min$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d92d1-2d47-4bf2-a187-b9da08272420",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{G(a_0,a_1)}}{\\partial{a_0}}=\\sum_{j=1}^{k}a_0+a_1\\cdot\\sum_{j=1}^{k}x_{j1}-\\sum_{j=1}^{k}Y_j=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7198e595-6c13-46b2-bff5-867260fefa19",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{G(a_0,a_1)}}{\\partial{a_1}}=\\sum_{j=1}^{k}a_0x_{j1}+a_1\\cdot\\sum_{j=1}^{k}x_{j1}^2-\\sum_{j=1}^{k}Y_jx_{j1}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd24c9-bfff-4d86-a399-4c6ff992d93f",
   "metadata": {},
   "source": [
    "$$k\\cdot a_0+\\left(\\sum_{j=1}^{k}x_{j1}\\right)\\cdot a_1=\\sum_{j=1}^{k}Y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8afa8-ad7b-45b1-b4cc-50639855958d",
   "metadata": {},
   "source": [
    "$$\\left(\\sum_{j=1}^kx_{j1}\\right)\\cdot a_0+\\left(\\sum_{j=1}^{k}x_{j1}^2\\right)\\cdot a_1=\\sum_{j=1}^{k}Y_jx_{j1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473b65c-9e3c-4727-9768-d738c6d02d0d",
   "metadata": {},
   "source": [
    "$$a_0=\\frac{\\begin{vmatrix}\n",
    "\\sum_{j=1}^{k}Y_j & \\sum_{j=1}^{k}x_{j1}\\\\ \n",
    "\\sum_{j=1}^{k}Y_jx_{j1} & \\sum_{j=1}^{k}x_{j1}^2 \\notag\n",
    "\\end{vmatrix}}{\\begin{vmatrix}\n",
    "k & \\sum_{j=1}^{k}x_{j1}\\\\ \n",
    "\\sum_{j=1}^kx_{j1} & \\sum_{j=1}^{k}x_{j1}^2 \\notag\n",
    "\\end{vmatrix}}$$\n",
    "$$a_1=\\frac{\\begin{vmatrix}\n",
    "k & \\sum_{j=1}^{k}Y_j\\\\ \n",
    "\\sum_{j=1}^{k}x_{j1} & \\sum_{j=1}^{k}Y_jx_{j1} \\notag\n",
    "\\end{vmatrix}}{\\begin{vmatrix}\n",
    "k & \\sum_{j=1}^{k}x_{j1}\\\\ \n",
    "\\sum_{j=1}^kx_{j1} & \\sum_{j=1}^{k}x_{j1}^2 \\notag\n",
    "\\end{vmatrix}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1defc-e095-4d1f-adb6-009c2863629a",
   "metadata": {},
   "source": [
    "Рассмотрим использование данного метода на примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d8cb69-48ed-4b56-aaf8-d51f01cd1fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Устанавливаю пакет в ‘/mnt/38fd9072-993c-442e-b6f8-1d98878f17c7/juna/R/x86_64-pc-linux-gnu-library/4.1’\n",
      "(потому что ‘lib’ не определено)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"readxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4b5457-9c93-4d7d-9c71-83e48b62542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A tibble: 30 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>Номер банка</th><th scope=col>Капитал</th><th scope=col>Работающие активы</th><th scope=col>Уставной капитал</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>20.7</td><td> 11.7</td><td> 2.4</td></tr>\n",
       "\t<tr><td> 2</td><td>19.9</td><td> 19.8</td><td>17.5</td></tr>\n",
       "\t<tr><td> 3</td><td> 9.3</td><td>  2.6</td><td> 2.7</td></tr>\n",
       "\t<tr><td> 4</td><td>59.3</td><td> 43.6</td><td> 2.1</td></tr>\n",
       "\t<tr><td> 5</td><td>24.7</td><td> 29.0</td><td>23.1</td></tr>\n",
       "\t<tr><td> 6</td><td>47.7</td><td> 98.5</td><td>18.7</td></tr>\n",
       "\t<tr><td> 7</td><td>24.2</td><td> 25.6</td><td> 5.3</td></tr>\n",
       "\t<tr><td> 8</td><td> 7.8</td><td>  6.2</td><td> 2.2</td></tr>\n",
       "\t<tr><td> 9</td><td>38.3</td><td> 79.8</td><td> 6.8</td></tr>\n",
       "\t<tr><td>10</td><td>10.3</td><td> 10.1</td><td> 3.5</td></tr>\n",
       "\t<tr><td>11</td><td>35.7</td><td> 30.0</td><td>13.6</td></tr>\n",
       "\t<tr><td>12</td><td>20.7</td><td> 21.2</td><td> 8.9</td></tr>\n",
       "\t<tr><td>13</td><td> 8.2</td><td> 16.7</td><td> 2.2</td></tr>\n",
       "\t<tr><td>14</td><td>10.2</td><td>  9.1</td><td> 9.0</td></tr>\n",
       "\t<tr><td>15</td><td>23.5</td><td> 31.7</td><td> 3.6</td></tr>\n",
       "\t<tr><td>16</td><td>55.8</td><td> 54.4</td><td> 7.5</td></tr>\n",
       "\t<tr><td>17</td><td>10.3</td><td> 21.4</td><td> 4.3</td></tr>\n",
       "\t<tr><td>18</td><td>16.7</td><td> 41.1</td><td> 5.1</td></tr>\n",
       "\t<tr><td>19</td><td>15.8</td><td> 29.8</td><td> 9.9</td></tr>\n",
       "\t<tr><td>20</td><td> 6.8</td><td> 10.9</td><td> 2.9</td></tr>\n",
       "\t<tr><td>21</td><td>22.4</td><td> 53.4</td><td>13.4</td></tr>\n",
       "\t<tr><td>22</td><td>13.6</td><td> 22.6</td><td> 4.8</td></tr>\n",
       "\t<tr><td>23</td><td> 9.9</td><td> 11.7</td><td> 5.0</td></tr>\n",
       "\t<tr><td>24</td><td>24.0</td><td> 27.3</td><td> 6.1</td></tr>\n",
       "\t<tr><td>25</td><td>23.0</td><td> 70.2</td><td> 5.9</td></tr>\n",
       "\t<tr><td>26</td><td>75.1</td><td>124.2</td><td>17.2</td></tr>\n",
       "\t<tr><td>27</td><td>56.2</td><td> 90.4</td><td>20.5</td></tr>\n",
       "\t<tr><td>28</td><td>60.7</td><td>101.7</td><td>10.7</td></tr>\n",
       "\t<tr><td>29</td><td>14.8</td><td> 18.2</td><td> 2.9</td></tr>\n",
       "\t<tr><td>30</td><td>41.5</td><td>127.7</td><td>12.1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 30 × 4\n",
       "\\begin{tabular}{llll}\n",
       " Номер банка & Капитал & Работающие активы & Уставной капитал\\\\\n",
       " <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t  1 & 20.7 &  11.7 &  2.4\\\\\n",
       "\t  2 & 19.9 &  19.8 & 17.5\\\\\n",
       "\t  3 &  9.3 &   2.6 &  2.7\\\\\n",
       "\t  4 & 59.3 &  43.6 &  2.1\\\\\n",
       "\t  5 & 24.7 &  29.0 & 23.1\\\\\n",
       "\t  6 & 47.7 &  98.5 & 18.7\\\\\n",
       "\t  7 & 24.2 &  25.6 &  5.3\\\\\n",
       "\t  8 &  7.8 &   6.2 &  2.2\\\\\n",
       "\t  9 & 38.3 &  79.8 &  6.8\\\\\n",
       "\t 10 & 10.3 &  10.1 &  3.5\\\\\n",
       "\t 11 & 35.7 &  30.0 & 13.6\\\\\n",
       "\t 12 & 20.7 &  21.2 &  8.9\\\\\n",
       "\t 13 &  8.2 &  16.7 &  2.2\\\\\n",
       "\t 14 & 10.2 &   9.1 &  9.0\\\\\n",
       "\t 15 & 23.5 &  31.7 &  3.6\\\\\n",
       "\t 16 & 55.8 &  54.4 &  7.5\\\\\n",
       "\t 17 & 10.3 &  21.4 &  4.3\\\\\n",
       "\t 18 & 16.7 &  41.1 &  5.1\\\\\n",
       "\t 19 & 15.8 &  29.8 &  9.9\\\\\n",
       "\t 20 &  6.8 &  10.9 &  2.9\\\\\n",
       "\t 21 & 22.4 &  53.4 & 13.4\\\\\n",
       "\t 22 & 13.6 &  22.6 &  4.8\\\\\n",
       "\t 23 &  9.9 &  11.7 &  5.0\\\\\n",
       "\t 24 & 24.0 &  27.3 &  6.1\\\\\n",
       "\t 25 & 23.0 &  70.2 &  5.9\\\\\n",
       "\t 26 & 75.1 & 124.2 & 17.2\\\\\n",
       "\t 27 & 56.2 &  90.4 & 20.5\\\\\n",
       "\t 28 & 60.7 & 101.7 & 10.7\\\\\n",
       "\t 29 & 14.8 &  18.2 &  2.9\\\\\n",
       "\t 30 & 41.5 & 127.7 & 12.1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 30 × 4\n",
       "\n",
       "| Номер банка &lt;dbl&gt; | Капитал &lt;dbl&gt; | Работающие активы &lt;dbl&gt; | Уставной капитал &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "|  1 | 20.7 |  11.7 |  2.4 |\n",
       "|  2 | 19.9 |  19.8 | 17.5 |\n",
       "|  3 |  9.3 |   2.6 |  2.7 |\n",
       "|  4 | 59.3 |  43.6 |  2.1 |\n",
       "|  5 | 24.7 |  29.0 | 23.1 |\n",
       "|  6 | 47.7 |  98.5 | 18.7 |\n",
       "|  7 | 24.2 |  25.6 |  5.3 |\n",
       "|  8 |  7.8 |   6.2 |  2.2 |\n",
       "|  9 | 38.3 |  79.8 |  6.8 |\n",
       "| 10 | 10.3 |  10.1 |  3.5 |\n",
       "| 11 | 35.7 |  30.0 | 13.6 |\n",
       "| 12 | 20.7 |  21.2 |  8.9 |\n",
       "| 13 |  8.2 |  16.7 |  2.2 |\n",
       "| 14 | 10.2 |   9.1 |  9.0 |\n",
       "| 15 | 23.5 |  31.7 |  3.6 |\n",
       "| 16 | 55.8 |  54.4 |  7.5 |\n",
       "| 17 | 10.3 |  21.4 |  4.3 |\n",
       "| 18 | 16.7 |  41.1 |  5.1 |\n",
       "| 19 | 15.8 |  29.8 |  9.9 |\n",
       "| 20 |  6.8 |  10.9 |  2.9 |\n",
       "| 21 | 22.4 |  53.4 | 13.4 |\n",
       "| 22 | 13.6 |  22.6 |  4.8 |\n",
       "| 23 |  9.9 |  11.7 |  5.0 |\n",
       "| 24 | 24.0 |  27.3 |  6.1 |\n",
       "| 25 | 23.0 |  70.2 |  5.9 |\n",
       "| 26 | 75.1 | 124.2 | 17.2 |\n",
       "| 27 | 56.2 |  90.4 | 20.5 |\n",
       "| 28 | 60.7 | 101.7 | 10.7 |\n",
       "| 29 | 14.8 |  18.2 |  2.9 |\n",
       "| 30 | 41.5 | 127.7 | 12.1 |\n",
       "\n"
      ],
      "text/plain": [
       "   Номер банка Капитал Работающие активы Уставной капитал\n",
       "1   1          20.7     11.7              2.4            \n",
       "2   2          19.9     19.8             17.5            \n",
       "3   3           9.3      2.6              2.7            \n",
       "4   4          59.3     43.6              2.1            \n",
       "5   5          24.7     29.0             23.1            \n",
       "6   6          47.7     98.5             18.7            \n",
       "7   7          24.2     25.6              5.3            \n",
       "8   8           7.8      6.2              2.2            \n",
       "9   9          38.3     79.8              6.8            \n",
       "10 10          10.3     10.1              3.5            \n",
       "11 11          35.7     30.0             13.6            \n",
       "12 12          20.7     21.2              8.9            \n",
       "13 13           8.2     16.7              2.2            \n",
       "14 14          10.2      9.1              9.0            \n",
       "15 15          23.5     31.7              3.6            \n",
       "16 16          55.8     54.4              7.5            \n",
       "17 17          10.3     21.4              4.3            \n",
       "18 18          16.7     41.1              5.1            \n",
       "19 19          15.8     29.8              9.9            \n",
       "20 20           6.8     10.9              2.9            \n",
       "21 21          22.4     53.4             13.4            \n",
       "22 22          13.6     22.6              4.8            \n",
       "23 23           9.9     11.7              5.0            \n",
       "24 24          24.0     27.3              6.1            \n",
       "25 25          23.0     70.2              5.9            \n",
       "26 26          75.1    124.2             17.2            \n",
       "27 27          56.2     90.4             20.5            \n",
       "28 28          60.7    101.7             10.7            \n",
       "29 29          14.8     18.2              2.9            \n",
       "30 30          41.5    127.7             12.1            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(readxl)\n",
    "D<-read_excel(\"data.xlsx\")\n",
    "View(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66e56e8-951b-44d0-875e-d132fcc3b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=D$`Работающие активы`\n",
    "X<-D$`Капитал`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be758326-b7d2-46d4-b003-731298c4abe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] -0.5429375  1.5572892\n"
     ]
    }
   ],
   "source": [
    "#Выполним расчет определителей\n",
    "chis1<-matrix(0,nrow=2,ncol=2)\n",
    "znam<-matrix(0,nrow=2,ncol=2)\n",
    "chis2<-matrix(0,nrow=2,ncol=2)\n",
    "chis1[1,1]<-sum(Y);chis1[1,2]<-sum(X);chis1[2,1]<-sum(Y*X);chis1[2,2]<-sum(X^2)\n",
    "znam[1,1]<-length(Y);znam[1,2]<-sum(X);znam[2,1]<-sum(X);znam[2,2]<-sum(X^2)\n",
    "chis2[1,1]<-length(Y);chis2[1,2]<-sum(Y);chis2[2,1]<-sum(X);chis2[2,2]<-sum(X*Y)\n",
    "\n",
    "a0<-det(chis1)/det(znam)\n",
    "a1<-det(chis2)/det(znam)\n",
    "print(c(a0,a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350e108-5d2d-4f0d-ae8c-d25fc3c5d09a",
   "metadata": {},
   "source": [
    "Конечно, такие расчеты в среде R проводить каждый раз не нужно. Есть встроенная функция lm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f00fe1-7837-486b-8af1-8ce8b0664f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for lm {stats}\"><tr><td>lm {stats}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Fitting Linear Models</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p><code>lm</code> is used to fit linear models.\n",
       "It can be used to carry out regression,\n",
       "single stratum analysis of variance and\n",
       "analysis of covariance (although <code>aov</code> may provide a more\n",
       "convenient interface for these).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "lm(formula, data, subset, weights, na.action,\n",
       "   method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n",
       "   singular.ok = TRUE, contrasts = NULL, offset, ...)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>formula</code></td>\n",
       "<td>\n",
       "<p>an object of class <code>\"formula\"</code> (or one that\n",
       "can be coerced to that class): a symbolic description of the\n",
       "model to be fitted.  The details of model specification are given\n",
       "under &lsquo;Details&rsquo;.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "<p>an optional data frame, list or environment (or object\n",
       "coercible by <code>as.data.frame</code> to a data frame) containing\n",
       "the variables in the model.  If not found in <code>data</code>, the\n",
       "variables are taken from <code>environment(formula)</code>,\n",
       "typically the environment from which <code>lm</code> is called.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>subset</code></td>\n",
       "<td>\n",
       "<p>an optional vector specifying a subset of observations\n",
       "to be used in the fitting process.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>an optional vector of weights to be used in the fitting\n",
       "process.  Should be <code>NULL</code> or a numeric vector.\n",
       "If non-NULL, weighted least squares is used with weights\n",
       "<code>weights</code> (that is, minimizing <code>sum(w*e^2)</code>); otherwise\n",
       "ordinary least squares is used.  See also &lsquo;Details&rsquo;,</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.action</code></td>\n",
       "<td>\n",
       "<p>a function which indicates what should happen\n",
       "when the data contain <code>NA</code>s.  The default is set by\n",
       "the <code>na.action</code> setting of <code>options</code>, and is\n",
       "<code>na.fail</code> if that is unset.  The &lsquo;factory-fresh&rsquo;\n",
       "default is <code>na.omit</code>.  Another possible value is\n",
       "<code>NULL</code>, no action.  Value <code>na.exclude</code> can be useful.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>method</code></td>\n",
       "<td>\n",
       "<p>the method to be used; for fitting, currently only\n",
       "<code>method = \"qr\"</code> is supported; <code>method = \"model.frame\"</code> returns\n",
       "the model frame (the same as with <code>model = TRUE</code>, see below).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>model, x, y, qr</code></td>\n",
       "<td>\n",
       "<p>logicals.  If <code>TRUE</code> the corresponding\n",
       "components of the fit (the model frame, the model matrix, the\n",
       "response, the QR decomposition) are returned.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>singular.ok</code></td>\n",
       "<td>\n",
       "<p>logical. If <code>FALSE</code> (the default in S but\n",
       "not in <span style=\"font-family: Courier New, Courier; color: #666666;\"><b>R</b></span>) a singular fit is an error.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>contrasts</code></td>\n",
       "<td>\n",
       "<p>an optional list. See the <code>contrasts.arg</code>\n",
       "of <code>model.matrix.default</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>this can be used to specify an <em>a priori</em> known\n",
       "component to be included in the linear predictor during fitting.\n",
       "This should be <code>NULL</code> or a numeric vector of length equal to\n",
       "the number of cases.  One or more <code>offset</code> terms can be\n",
       "included in the formula instead or as well, and if more than one are\n",
       "specified their sum is used.  See <code>model.offset</code>.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>additional arguments to be passed to the low level\n",
       "regression fitting functions (see below).</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>Models for <code>lm</code> are specified symbolically.  A typical model has\n",
       "the form <code>response ~ terms</code> where <code>response</code> is the (numeric)\n",
       "response vector and <code>terms</code> is a series of terms which specifies a\n",
       "linear predictor for <code>response</code>.  A terms specification of the form\n",
       "<code>first + second</code> indicates all the terms in <code>first</code> together\n",
       "with all the terms in <code>second</code> with duplicates removed.  A\n",
       "specification of the form <code>first:second</code> indicates the set of\n",
       "terms obtained by taking the interactions of all terms in <code>first</code>\n",
       "with all terms in <code>second</code>.  The specification <code>first*second</code>\n",
       "indicates the <em>cross</em> of <code>first</code> and <code>second</code>.  This is\n",
       "the same as <code>first + second + first:second</code>.\n",
       "</p>\n",
       "<p>If the formula includes an <code>offset</code>, this is evaluated and\n",
       "subtracted from the response.\n",
       "</p>\n",
       "<p>If <code>response</code> is a matrix a linear model is fitted separately by\n",
       "least-squares to each column of the matrix.\n",
       "</p>\n",
       "<p>See <code>model.matrix</code> for some further details.  The terms in\n",
       "the formula will be re-ordered so that main effects come first,\n",
       "followed by the interactions, all second-order, all third-order and so\n",
       "on: to avoid this pass a <code>terms</code> object as the formula (see\n",
       "<code>aov</code> and <code>demo(glm.vr)</code> for an example).\n",
       "</p>\n",
       "<p>A formula has an implied intercept term.  To remove this use either\n",
       "<code>y ~ x - 1</code> or <code>y ~ 0 + x</code>.  See <code>formula</code> for\n",
       "more details of allowed formulae.\n",
       "</p>\n",
       "<p>Non-<code>NULL</code> <code>weights</code> can be used to indicate that\n",
       "different observations have different variances (with the values in\n",
       "<code>weights</code> being inversely proportional to the variances); or\n",
       "equivalently, when the elements of <code>weights</code> are positive\n",
       "integers <i>w_i</i>, that each response <i>y_i</i> is the mean of\n",
       "<i>w_i</i> unit-weight observations (including the case that there\n",
       "are <i>w_i</i> observations equal to <i>y_i</i> and the data have been\n",
       "summarized). However, in the latter case, notice that within-group\n",
       "variation is not used.  Therefore, the sigma estimate and residual\n",
       "degrees of freedom may be suboptimal; in the case of replication\n",
       "weights, even wrong. Hence, standard errors and analysis of variance\n",
       "tables should be treated with care.\n",
       "</p>\n",
       "<p><code>lm</code> calls the lower level functions <code>lm.fit</code>, etc,\n",
       "see below, for the actual numerical computations.  For programming\n",
       "only, you may consider doing likewise.\n",
       "</p>\n",
       "<p>All of <code>weights</code>, <code>subset</code> and <code>offset</code> are evaluated\n",
       "in the same way as variables in <code>formula</code>, that is first in\n",
       "<code>data</code> and then in the environment of <code>formula</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p><code>lm</code> returns an object of <code>class</code> <code>\"lm\"</code> or for\n",
       "multiple responses of class <code>c(\"mlm\", \"lm\")</code>.\n",
       "</p>\n",
       "<p>The functions <code>summary</code> and <code>anova</code> are used to\n",
       "obtain and print a summary and analysis of variance table of the\n",
       "results.  The generic accessor functions <code>coefficients</code>,\n",
       "<code>effects</code>, <code>fitted.values</code> and <code>residuals</code> extract\n",
       "various useful features of the value returned by <code>lm</code>.\n",
       "</p>\n",
       "<p>An object of class <code>\"lm\"</code> is a list containing at least the\n",
       "following components:\n",
       "</p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>coefficients</code></td>\n",
       "<td>\n",
       "<p>a named vector of coefficients</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>residuals</code></td>\n",
       "<td>\n",
       "<p>the residuals, that is response minus fitted values.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fitted.values</code></td>\n",
       "<td>\n",
       "<p>the fitted mean values.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>rank</code></td>\n",
       "<td>\n",
       "<p>the numeric rank of the fitted linear model.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>weights</code></td>\n",
       "<td>\n",
       "<p>(only for weighted fits) the specified weights.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>df.residual</code></td>\n",
       "<td>\n",
       "<p>the residual degrees of freedom.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>call</code></td>\n",
       "<td>\n",
       "<p>the matched call.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>terms</code></td>\n",
       "<td>\n",
       "<p>the <code>terms</code> object used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>contrasts</code></td>\n",
       "<td>\n",
       "<p>(only where relevant) the contrasts used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>xlevels</code></td>\n",
       "<td>\n",
       "<p>(only where relevant) a record of the levels of the\n",
       "factors used in fitting.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>offset</code></td>\n",
       "<td>\n",
       "<p>the offset used (missing if none were used).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>if requested, the response used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>if requested, the model matrix used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>model</code></td>\n",
       "<td>\n",
       "<p>if requested (the default), the model frame used.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.action</code></td>\n",
       "<td>\n",
       "<p>(where relevant) information returned by\n",
       "<code>model.frame</code> on the special handling of <code>NA</code>s.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "<p>In addition, non-null fits will have components <code>assign</code>,\n",
       "<code>effects</code> and (unless not requested) <code>qr</code> relating to the linear\n",
       "fit, for use by extractor functions such as <code>summary</code> and\n",
       "<code>effects</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Using time series</h3>\n",
       "\n",
       "<p>Considerable care is needed when using <code>lm</code> with time series.\n",
       "</p>\n",
       "<p>Unless <code>na.action = NULL</code>, the time series attributes are\n",
       "stripped from the variables before the regression is done.  (This is\n",
       "necessary as omitting <code>NA</code>s would invalidate the time series\n",
       "attributes, and if <code>NA</code>s are omitted in the middle of the series\n",
       "the result would no longer be a regular time series.)\n",
       "</p>\n",
       "<p>Even if the time series attributes are retained, they are not used to\n",
       "line up series, so that the time shift of a lagged or differenced\n",
       "regressor would be ignored.  It is good practice to prepare a\n",
       "<code>data</code> argument by <code>ts.intersect(..., dframe = TRUE)</code>,\n",
       "then apply a suitable <code>na.action</code> to that data frame and call\n",
       "<code>lm</code> with <code>na.action = NULL</code> so that residuals and fitted\n",
       "values are time series.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p>Offsets specified by <code>offset</code> will not be included in predictions\n",
       "by <code>predict.lm</code>, whereas those specified by an offset term\n",
       "in the formula will be.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>The design was inspired by the S function of the same name described\n",
       "in Chambers (1992).  The implementation of model formula by Ross Ihaka\n",
       "was based on Wilkinson &amp; Rogers (1973).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Chambers, J. M. (1992)\n",
       "<em>Linear models.</em>\n",
       "Chapter 4 of <em>Statistical Models in S</em>\n",
       "eds J. M. Chambers and T. J. Hastie, Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "<p>Wilkinson, G. N. and Rogers, C. E. (1973).\n",
       "Symbolic descriptions of factorial models for analysis of variance.\n",
       "<em>Applied Statistics</em>, <b>22</b>, 392&ndash;399.\n",
       "doi: <a href=\"http://doi.org/10.2307/2346786\">10.2307/2346786</a>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>summary.lm</code> for summaries and <code>anova.lm</code> for\n",
       "the ANOVA table; <code>aov</code> for a different interface.\n",
       "</p>\n",
       "<p>The generic functions <code>coef</code>, <code>effects</code>,\n",
       "<code>residuals</code>, <code>fitted</code>, <code>vcov</code>.\n",
       "</p>\n",
       "<p><code>predict.lm</code> (via <code>predict</code>) for prediction,\n",
       "including confidence and prediction intervals;\n",
       "<code>confint</code> for confidence intervals of <em>parameters</em>.\n",
       "</p>\n",
       "<p><code>lm.influence</code> for regression diagnostics, and\n",
       "<code>glm</code> for <b>generalized</b> linear models.\n",
       "</p>\n",
       "<p>The underlying low level functions,\n",
       "<code>lm.fit</code> for plain, and <code>lm.wfit</code> for weighted\n",
       "regression fitting.\n",
       "</p>\n",
       "<p>More <code>lm()</code> examples are available e.g., in\n",
       "<code>anscombe</code>, <code>attitude</code>, <code>freeny</code>,\n",
       "<code>LifeCycleSavings</code>, <code>longley</code>,\n",
       "<code>stackloss</code>, <code>swiss</code>.\n",
       "</p>\n",
       "<p><code>biglm</code> in package <a href=\"https://CRAN.R-project.org/package=biglm\"><span class=\"pkg\">biglm</span></a> for an alternative\n",
       "way to fit linear models to large datasets (especially those with many\n",
       "cases).\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "require(graphics)\n",
       "\n",
       "## Annette Dobson (1990) \"An Introduction to Generalized Linear Models\".\n",
       "## Page 9: Plant Weight Data.\n",
       "ctl &lt;- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)\n",
       "trt &lt;- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)\n",
       "group &lt;- gl(2, 10, 20, labels = c(\"Ctl\",\"Trt\"))\n",
       "weight &lt;- c(ctl, trt)\n",
       "lm.D9 &lt;- lm(weight ~ group)\n",
       "lm.D90 &lt;- lm(weight ~ group - 1) # omitting intercept\n",
       "\n",
       "anova(lm.D9)\n",
       "summary(lm.D90)\n",
       "\n",
       "opar &lt;- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))\n",
       "plot(lm.D9, las = 1)      # Residuals, Fitted, ...\n",
       "par(opar)\n",
       "\n",
       "### less simple examples in \"See Also\" above\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>stats</em> version 3.5.3 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{lm}{Fitting Linear Models}{lm}\n",
       "\\keyword{regression}{lm}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "\\code{lm} is used to fit linear models.\n",
       "It can be used to carry out regression,\n",
       "single stratum analysis of variance and\n",
       "analysis of covariance (although \\code{\\LinkA{aov}{aov}} may provide a more\n",
       "convenient interface for these).\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "lm(formula, data, subset, weights, na.action,\n",
       "   method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n",
       "   singular.ok = TRUE, contrasts = NULL, offset, ...)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{formula}] an object of class \\code{\"\\LinkA{formula}{formula}\"} (or one that\n",
       "can be coerced to that class): a symbolic description of the\n",
       "model to be fitted.  The details of model specification are given\n",
       "under `Details'.\n",
       "\n",
       "\\item[\\code{data}] an optional data frame, list or environment (or object\n",
       "coercible by \\code{\\LinkA{as.data.frame}{as.data.frame}} to a data frame) containing\n",
       "the variables in the model.  If not found in \\code{data}, the\n",
       "variables are taken from \\code{environment(formula)},\n",
       "typically the environment from which \\code{lm} is called.\n",
       "\n",
       "\\item[\\code{subset}] an optional vector specifying a subset of observations\n",
       "to be used in the fitting process.\n",
       "\n",
       "\\item[\\code{weights}] an optional vector of weights to be used in the fitting\n",
       "process.  Should be \\code{NULL} or a numeric vector.\n",
       "If non-NULL, weighted least squares is used with weights\n",
       "\\code{weights} (that is, minimizing \\code{sum(w*e\\textasciicircum{}2)}); otherwise\n",
       "ordinary least squares is used.  See also `Details',\n",
       "\n",
       "\\item[\\code{na.action}] a function which indicates what should happen\n",
       "when the data contain \\code{NA}s.  The default is set by\n",
       "the \\code{na.action} setting of \\code{\\LinkA{options}{options}}, and is\n",
       "\\code{\\LinkA{na.fail}{na.fail}} if that is unset.  The `factory-fresh'\n",
       "default is \\code{\\LinkA{na.omit}{na.omit}}.  Another possible value is\n",
       "\\code{NULL}, no action.  Value \\code{\\LinkA{na.exclude}{na.exclude}} can be useful.\n",
       "\n",
       "\\item[\\code{method}] the method to be used; for fitting, currently only\n",
       "\\code{method = \"qr\"} is supported; \\code{method = \"model.frame\"} returns\n",
       "the model frame (the same as with \\code{model = TRUE}, see below).\n",
       "\n",
       "\\item[\\code{model, x, y, qr}] logicals.  If \\code{TRUE} the corresponding\n",
       "components of the fit (the model frame, the model matrix, the\n",
       "response, the QR decomposition) are returned.\n",
       "\n",
       "\n",
       "\\item[\\code{singular.ok}] logical. If \\code{FALSE} (the default in S but\n",
       "not in \\R{}) a singular fit is an error.\n",
       "\n",
       "\\item[\\code{contrasts}] an optional list. See the \\code{contrasts.arg}\n",
       "of \\code{\\LinkA{model.matrix.default}{model.matrix.default}}.\n",
       "\n",
       "\\item[\\code{offset}] this can be used to specify an \\emph{a priori} known\n",
       "component to be included in the linear predictor during fitting.\n",
       "This should be \\code{NULL} or a numeric vector of length equal to\n",
       "the number of cases.  One or more \\code{\\LinkA{offset}{offset}} terms can be\n",
       "included in the formula instead or as well, and if more than one are\n",
       "specified their sum is used.  See \\code{\\LinkA{model.offset}{model.offset}}.\n",
       "\n",
       "\\item[\\code{...}] additional arguments to be passed to the low level\n",
       "regression fitting functions (see below).\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "Models for \\code{lm} are specified symbolically.  A typical model has\n",
       "the form \\code{response \\textasciitilde{} terms} where \\code{response} is the (numeric)\n",
       "response vector and \\code{terms} is a series of terms which specifies a\n",
       "linear predictor for \\code{response}.  A terms specification of the form\n",
       "\\code{first + second} indicates all the terms in \\code{first} together\n",
       "with all the terms in \\code{second} with duplicates removed.  A\n",
       "specification of the form \\code{first:second} indicates the set of\n",
       "terms obtained by taking the interactions of all terms in \\code{first}\n",
       "with all terms in \\code{second}.  The specification \\code{first*second}\n",
       "indicates the \\emph{cross} of \\code{first} and \\code{second}.  This is\n",
       "the same as \\code{first + second + first:second}.\n",
       "\n",
       "If the formula includes an \\code{\\LinkA{offset}{offset}}, this is evaluated and\n",
       "subtracted from the response.\n",
       "\n",
       "If \\code{response} is a matrix a linear model is fitted separately by\n",
       "least-squares to each column of the matrix.\n",
       "\n",
       "See \\code{\\LinkA{model.matrix}{model.matrix}} for some further details.  The terms in\n",
       "the formula will be re-ordered so that main effects come first,\n",
       "followed by the interactions, all second-order, all third-order and so\n",
       "on: to avoid this pass a \\code{terms} object as the formula (see\n",
       "\\code{\\LinkA{aov}{aov}} and \\code{demo(glm.vr)} for an example).\n",
       "\n",
       "A formula has an implied intercept term.  To remove this use either\n",
       "\\code{y \\textasciitilde{} x - 1} or \\code{y \\textasciitilde{} 0 + x}.  See \\code{\\LinkA{formula}{formula}} for\n",
       "more details of allowed formulae.\n",
       "\n",
       "Non-\\code{NULL} \\code{weights} can be used to indicate that\n",
       "different observations have different variances (with the values in\n",
       "\\code{weights} being inversely proportional to the variances); or\n",
       "equivalently, when the elements of \\code{weights} are positive\n",
       "integers \\eqn{w_i}{}, that each response \\eqn{y_i}{} is the mean of\n",
       "\\eqn{w_i}{} unit-weight observations (including the case that there\n",
       "are \\eqn{w_i}{} observations equal to \\eqn{y_i}{} and the data have been\n",
       "summarized). However, in the latter case, notice that within-group\n",
       "variation is not used.  Therefore, the sigma estimate and residual\n",
       "degrees of freedom may be suboptimal; in the case of replication\n",
       "weights, even wrong. Hence, standard errors and analysis of variance\n",
       "tables should be treated with care.\n",
       "\n",
       "\\code{lm} calls the lower level functions \\code{\\LinkA{lm.fit}{lm.fit}}, etc,\n",
       "see below, for the actual numerical computations.  For programming\n",
       "only, you may consider doing likewise.\n",
       "\n",
       "All of \\code{weights}, \\code{subset} and \\code{offset} are evaluated\n",
       "in the same way as variables in \\code{formula}, that is first in\n",
       "\\code{data} and then in the environment of \\code{formula}.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "\\code{lm} returns an object of \\code{\\LinkA{class}{class}} \\code{\"lm\"} or for\n",
       "multiple responses of class \\code{c(\"mlm\", \"lm\")}.\n",
       "\n",
       "The functions \\code{summary} and \\code{\\LinkA{anova}{anova}} are used to\n",
       "obtain and print a summary and analysis of variance table of the\n",
       "results.  The generic accessor functions \\code{coefficients},\n",
       "\\code{effects}, \\code{fitted.values} and \\code{residuals} extract\n",
       "various useful features of the value returned by \\code{lm}.\n",
       "\n",
       "An object of class \\code{\"lm\"} is a list containing at least the\n",
       "following components:\n",
       "\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{coefficients}] a named vector of coefficients\n",
       "\\item[\\code{residuals}] the residuals, that is response minus fitted values.\n",
       "\\item[\\code{fitted.values}] the fitted mean values.\n",
       "\\item[\\code{rank}] the numeric rank of the fitted linear model.\n",
       "\\item[\\code{weights}] (only for weighted fits) the specified weights.\n",
       "\\item[\\code{df.residual}] the residual degrees of freedom.\n",
       "\\item[\\code{call}] the matched call.\n",
       "\\item[\\code{terms}] the \\code{\\LinkA{terms}{terms}} object used.\n",
       "\\item[\\code{contrasts}] (only where relevant) the contrasts used.\n",
       "\\item[\\code{xlevels}] (only where relevant) a record of the levels of the\n",
       "factors used in fitting.\n",
       "\\item[\\code{offset}] the offset used (missing if none were used).\n",
       "\\item[\\code{y}] if requested, the response used.\n",
       "\\item[\\code{x}] if requested, the model matrix used.\n",
       "\\item[\\code{model}] if requested (the default), the model frame used.\n",
       "\\item[\\code{na.action}] (where relevant) information returned by\n",
       "\\code{\\LinkA{model.frame}{model.frame}} on the special handling of \\code{NA}s.\n",
       "\n",
       "\\end{ldescription}\n",
       "In addition, non-null fits will have components \\code{assign},\n",
       "\\code{effects} and (unless not requested) \\code{qr} relating to the linear\n",
       "fit, for use by extractor functions such as \\code{summary} and\n",
       "\\code{\\LinkA{effects}{effects}}.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Section}{Using time series}\n",
       "Considerable care is needed when using \\code{lm} with time series.\n",
       "\n",
       "Unless \\code{na.action = NULL}, the time series attributes are\n",
       "stripped from the variables before the regression is done.  (This is\n",
       "necessary as omitting \\code{NA}s would invalidate the time series\n",
       "attributes, and if \\code{NA}s are omitted in the middle of the series\n",
       "the result would no longer be a regular time series.)\n",
       "\n",
       "Even if the time series attributes are retained, they are not used to\n",
       "line up series, so that the time shift of a lagged or differenced\n",
       "regressor would be ignored.  It is good practice to prepare a\n",
       "\\code{data} argument by \\code{\\LinkA{ts.intersect}{ts.intersect}(..., dframe = TRUE)},\n",
       "then apply a suitable \\code{na.action} to that data frame and call\n",
       "\\code{lm} with \\code{na.action = NULL} so that residuals and fitted\n",
       "values are time series.\n",
       "\\end{Section}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "Offsets specified by \\code{offset} will not be included in predictions\n",
       "by \\code{\\LinkA{predict.lm}{predict.lm}}, whereas those specified by an offset term\n",
       "in the formula will be.\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "The design was inspired by the S function of the same name described\n",
       "in Chambers (1992).  The implementation of model formula by Ross Ihaka\n",
       "was based on Wilkinson \\& Rogers (1973).\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Chambers, J. M. (1992)\n",
       "\\emph{Linear models.}\n",
       "Chapter 4 of \\emph{Statistical Models in S}\n",
       "eds J. M. Chambers and T. J. Hastie, Wadsworth \\& Brooks/Cole.\n",
       "\n",
       "Wilkinson, G. N. and Rogers, C. E. (1973).\n",
       "Symbolic descriptions of factorial models for analysis of variance.\n",
       "\\emph{Applied Statistics}, \\bold{22}, 392--399.\n",
       "doi:\\nobreakspace{}\\Rhref{http://doi.org/10.2307/2346786}{10.2307\\slash{}2346786}.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{summary.lm}{summary.lm}} for summaries and \\code{\\LinkA{anova.lm}{anova.lm}} for\n",
       "the ANOVA table; \\code{\\LinkA{aov}{aov}} for a different interface.\n",
       "\n",
       "The generic functions \\code{\\LinkA{coef}{coef}}, \\code{\\LinkA{effects}{effects}},\n",
       "\\code{\\LinkA{residuals}{residuals}}, \\code{\\LinkA{fitted}{fitted}}, \\code{\\LinkA{vcov}{vcov}}.\n",
       "\n",
       "\\code{\\LinkA{predict.lm}{predict.lm}} (via \\code{\\LinkA{predict}{predict}}) for prediction,\n",
       "including confidence and prediction intervals;\n",
       "\\code{\\LinkA{confint}{confint}} for confidence intervals of \\emph{parameters}.\n",
       "\n",
       "\\code{\\LinkA{lm.influence}{lm.influence}} for regression diagnostics, and\n",
       "\\code{\\LinkA{glm}{glm}} for \\bold{generalized} linear models.\n",
       "\n",
       "The underlying low level functions,\n",
       "\\code{\\LinkA{lm.fit}{lm.fit}} for plain, and \\code{\\LinkA{lm.wfit}{lm.wfit}} for weighted\n",
       "regression fitting.\n",
       "\n",
       "More \\code{lm()} examples are available e.g., in\n",
       "\\code{\\LinkA{anscombe}{anscombe}}, \\code{\\LinkA{attitude}{attitude}}, \\code{\\LinkA{freeny}{freeny}},\n",
       "\\code{\\LinkA{LifeCycleSavings}{LifeCycleSavings}}, \\code{\\LinkA{longley}{longley}},\n",
       "\\code{\\LinkA{stackloss}{stackloss}}, \\code{\\LinkA{swiss}{swiss}}.\n",
       "\n",
       "\\code{biglm} in package \\Rhref{https://CRAN.R-project.org/package=biglm}{\\pkg{biglm}} for an alternative\n",
       "way to fit linear models to large datasets (especially those with many\n",
       "cases).\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "require(graphics)\n",
       "\n",
       "## Annette Dobson (1990) \"An Introduction to Generalized Linear Models\".\n",
       "## Page 9: Plant Weight Data.\n",
       "ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)\n",
       "trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)\n",
       "group <- gl(2, 10, 20, labels = c(\"Ctl\",\"Trt\"))\n",
       "weight <- c(ctl, trt)\n",
       "lm.D9 <- lm(weight ~ group)\n",
       "lm.D90 <- lm(weight ~ group - 1) # omitting intercept\n",
       "\n",
       "anova(lm.D9)\n",
       "summary(lm.D90)\n",
       "\n",
       "opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))\n",
       "plot(lm.D9, las = 1)      # Residuals, Fitted, ...\n",
       "par(opar)\n",
       "\n",
       "### less simple examples in \"See Also\" above\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "lm                    package:stats                    R Documentation\n",
       "\n",
       "_\bF_\bi_\bt_\bt_\bi_\bn_\bg _\bL_\bi_\bn_\be_\ba_\br _\bM_\bo_\bd_\be_\bl_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     ‘lm’ is used to fit linear models.  It can be used to carry out\n",
       "     regression, single stratum analysis of variance and analysis of\n",
       "     covariance (although ‘aov’ may provide a more convenient interface\n",
       "     for these).\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     lm(formula, data, subset, weights, na.action,\n",
       "        method = \"qr\", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,\n",
       "        singular.ok = TRUE, contrasts = NULL, offset, ...)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       " formula: an object of class ‘\"formula\"’ (or one that can be coerced to\n",
       "          that class): a symbolic description of the model to be\n",
       "          fitted.  The details of model specification are given under\n",
       "          ‘Details’.\n",
       "\n",
       "    data: an optional data frame, list or environment (or object\n",
       "          coercible by ‘as.data.frame’ to a data frame) containing the\n",
       "          variables in the model.  If not found in ‘data’, the\n",
       "          variables are taken from ‘environment(formula)’, typically\n",
       "          the environment from which ‘lm’ is called.\n",
       "\n",
       "  subset: an optional vector specifying a subset of observations to be\n",
       "          used in the fitting process.\n",
       "\n",
       " weights: an optional vector of weights to be used in the fitting\n",
       "          process.  Should be ‘NULL’ or a numeric vector.  If non-NULL,\n",
       "          weighted least squares is used with weights ‘weights’ (that\n",
       "          is, minimizing ‘sum(w*e^2)’); otherwise ordinary least\n",
       "          squares is used.  See also ‘Details’,\n",
       "\n",
       "na.action: a function which indicates what should happen when the data\n",
       "          contain ‘NA’s.  The default is set by the ‘na.action’ setting\n",
       "          of ‘options’, and is ‘na.fail’ if that is unset.  The\n",
       "          ‘factory-fresh’ default is ‘na.omit’.  Another possible value\n",
       "          is ‘NULL’, no action.  Value ‘na.exclude’ can be useful.\n",
       "\n",
       "  method: the method to be used; for fitting, currently only ‘method =\n",
       "          \"qr\"’ is supported; ‘method = \"model.frame\"’ returns the\n",
       "          model frame (the same as with ‘model = TRUE’, see below).\n",
       "\n",
       "model, x, y, qr: logicals.  If ‘TRUE’ the corresponding components of\n",
       "          the fit (the model frame, the model matrix, the response, the\n",
       "          QR decomposition) are returned.\n",
       "\n",
       "singular.ok: logical. If ‘FALSE’ (the default in S but not in R) a\n",
       "          singular fit is an error.\n",
       "\n",
       "contrasts: an optional list. See the ‘contrasts.arg’ of\n",
       "          ‘model.matrix.default’.\n",
       "\n",
       "  offset: this can be used to specify an _a priori_ known component to\n",
       "          be included in the linear predictor during fitting.  This\n",
       "          should be ‘NULL’ or a numeric vector of length equal to the\n",
       "          number of cases.  One or more ‘offset’ terms can be included\n",
       "          in the formula instead or as well, and if more than one are\n",
       "          specified their sum is used.  See ‘model.offset’.\n",
       "\n",
       "     ...: additional arguments to be passed to the low level regression\n",
       "          fitting functions (see below).\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     Models for ‘lm’ are specified symbolically.  A typical model has\n",
       "     the form ‘response ~ terms’ where ‘response’ is the (numeric)\n",
       "     response vector and ‘terms’ is a series of terms which specifies a\n",
       "     linear predictor for ‘response’.  A terms specification of the\n",
       "     form ‘first + second’ indicates all the terms in ‘first’ together\n",
       "     with all the terms in ‘second’ with duplicates removed.  A\n",
       "     specification of the form ‘first:second’ indicates the set of\n",
       "     terms obtained by taking the interactions of all terms in ‘first’\n",
       "     with all terms in ‘second’.  The specification ‘first*second’\n",
       "     indicates the _cross_ of ‘first’ and ‘second’.  This is the same\n",
       "     as ‘first + second + first:second’.\n",
       "\n",
       "     If the formula includes an ‘offset’, this is evaluated and\n",
       "     subtracted from the response.\n",
       "\n",
       "     If ‘response’ is a matrix a linear model is fitted separately by\n",
       "     least-squares to each column of the matrix.\n",
       "\n",
       "     See ‘model.matrix’ for some further details.  The terms in the\n",
       "     formula will be re-ordered so that main effects come first,\n",
       "     followed by the interactions, all second-order, all third-order\n",
       "     and so on: to avoid this pass a ‘terms’ object as the formula (see\n",
       "     ‘aov’ and ‘demo(glm.vr)’ for an example).\n",
       "\n",
       "     A formula has an implied intercept term.  To remove this use\n",
       "     either ‘y ~ x - 1’ or ‘y ~ 0 + x’.  See ‘formula’ for more details\n",
       "     of allowed formulae.\n",
       "\n",
       "     Non-‘NULL’ ‘weights’ can be used to indicate that different\n",
       "     observations have different variances (with the values in\n",
       "     ‘weights’ being inversely proportional to the variances); or\n",
       "     equivalently, when the elements of ‘weights’ are positive integers\n",
       "     w_i, that each response y_i is the mean of w_i unit-weight\n",
       "     observations (including the case that there are w_i observations\n",
       "     equal to y_i and the data have been summarized). However, in the\n",
       "     latter case, notice that within-group variation is not used.\n",
       "     Therefore, the sigma estimate and residual degrees of freedom may\n",
       "     be suboptimal; in the case of replication weights, even wrong.\n",
       "     Hence, standard errors and analysis of variance tables should be\n",
       "     treated with care.\n",
       "\n",
       "     ‘lm’ calls the lower level functions ‘lm.fit’, etc, see below, for\n",
       "     the actual numerical computations.  For programming only, you may\n",
       "     consider doing likewise.\n",
       "\n",
       "     All of ‘weights’, ‘subset’ and ‘offset’ are evaluated in the same\n",
       "     way as variables in ‘formula’, that is first in ‘data’ and then in\n",
       "     the environment of ‘formula’.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     ‘lm’ returns an object of ‘class’ ‘\"lm\"’ or for multiple responses\n",
       "     of class ‘c(\"mlm\", \"lm\")’.\n",
       "\n",
       "     The functions ‘summary’ and ‘anova’ are used to obtain and print a\n",
       "     summary and analysis of variance table of the results.  The\n",
       "     generic accessor functions ‘coefficients’, ‘effects’,\n",
       "     ‘fitted.values’ and ‘residuals’ extract various useful features of\n",
       "     the value returned by ‘lm’.\n",
       "\n",
       "     An object of class ‘\"lm\"’ is a list containing at least the\n",
       "     following components:\n",
       "\n",
       "coefficients: a named vector of coefficients\n",
       "\n",
       "residuals: the residuals, that is response minus fitted values.\n",
       "\n",
       "fitted.values: the fitted mean values.\n",
       "\n",
       "    rank: the numeric rank of the fitted linear model.\n",
       "\n",
       " weights: (only for weighted fits) the specified weights.\n",
       "\n",
       "df.residual: the residual degrees of freedom.\n",
       "\n",
       "    call: the matched call.\n",
       "\n",
       "   terms: the ‘terms’ object used.\n",
       "\n",
       "contrasts: (only where relevant) the contrasts used.\n",
       "\n",
       " xlevels: (only where relevant) a record of the levels of the factors\n",
       "          used in fitting.\n",
       "\n",
       "  offset: the offset used (missing if none were used).\n",
       "\n",
       "       y: if requested, the response used.\n",
       "\n",
       "       x: if requested, the model matrix used.\n",
       "\n",
       "   model: if requested (the default), the model frame used.\n",
       "\n",
       "na.action: (where relevant) information returned by ‘model.frame’ on\n",
       "          the special handling of ‘NA’s.\n",
       "     In addition, non-null fits will have components ‘assign’,\n",
       "     ‘effects’ and (unless not requested) ‘qr’ relating to the linear\n",
       "     fit, for use by extractor functions such as ‘summary’ and\n",
       "     ‘effects’.\n",
       "\n",
       "_\bU_\bs_\bi_\bn_\bg _\bt_\bi_\bm_\be _\bs_\be_\br_\bi_\be_\bs:\n",
       "\n",
       "     Considerable care is needed when using ‘lm’ with time series.\n",
       "\n",
       "     Unless ‘na.action = NULL’, the time series attributes are stripped\n",
       "     from the variables before the regression is done.  (This is\n",
       "     necessary as omitting ‘NA’s would invalidate the time series\n",
       "     attributes, and if ‘NA’s are omitted in the middle of the series\n",
       "     the result would no longer be a regular time series.)\n",
       "\n",
       "     Even if the time series attributes are retained, they are not used\n",
       "     to line up series, so that the time shift of a lagged or\n",
       "     differenced regressor would be ignored.  It is good practice to\n",
       "     prepare a ‘data’ argument by ‘ts.intersect(..., dframe = TRUE)’,\n",
       "     then apply a suitable ‘na.action’ to that data frame and call ‘lm’\n",
       "     with ‘na.action = NULL’ so that residuals and fitted values are\n",
       "     time series.\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     Offsets specified by ‘offset’ will not be included in predictions\n",
       "     by ‘predict.lm’, whereas those specified by an offset term in the\n",
       "     formula will be.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     The design was inspired by the S function of the same name\n",
       "     described in Chambers (1992).  The implementation of model formula\n",
       "     by Ross Ihaka was based on Wilkinson & Rogers (1973).\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Chambers, J. M. (1992) _Linear models._ Chapter 4 of _Statistical\n",
       "     Models in S_ eds J. M. Chambers and T. J. Hastie, Wadsworth &\n",
       "     Brooks/Cole.\n",
       "\n",
       "     Wilkinson, G. N. and Rogers, C. E. (1973).  Symbolic descriptions\n",
       "     of factorial models for analysis of variance.  _Applied\n",
       "     Statistics_, *22*, 392-399.  doi: 10.2307/2346786 (URL:\n",
       "     http://doi.org/10.2307/2346786).\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘summary.lm’ for summaries and ‘anova.lm’ for the ANOVA table;\n",
       "     ‘aov’ for a different interface.\n",
       "\n",
       "     The generic functions ‘coef’, ‘effects’, ‘residuals’, ‘fitted’,\n",
       "     ‘vcov’.\n",
       "\n",
       "     ‘predict.lm’ (via ‘predict’) for prediction, including confidence\n",
       "     and prediction intervals; ‘confint’ for confidence intervals of\n",
       "     _parameters_.\n",
       "\n",
       "     ‘lm.influence’ for regression diagnostics, and ‘glm’ for\n",
       "     *generalized* linear models.\n",
       "\n",
       "     The underlying low level functions, ‘lm.fit’ for plain, and\n",
       "     ‘lm.wfit’ for weighted regression fitting.\n",
       "\n",
       "     More ‘lm()’ examples are available e.g., in ‘anscombe’,\n",
       "     ‘attitude’, ‘freeny’, ‘LifeCycleSavings’, ‘longley’, ‘stackloss’,\n",
       "     ‘swiss’.\n",
       "\n",
       "     ‘biglm’ in package ‘biglm’ for an alternative way to fit linear\n",
       "     models to large datasets (especially those with many cases).\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     require(graphics)\n",
       "     \n",
       "     ## Annette Dobson (1990) \"An Introduction to Generalized Linear Models\".\n",
       "     ## Page 9: Plant Weight Data.\n",
       "     ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)\n",
       "     trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)\n",
       "     group <- gl(2, 10, 20, labels = c(\"Ctl\",\"Trt\"))\n",
       "     weight <- c(ctl, trt)\n",
       "     lm.D9 <- lm(weight ~ group)\n",
       "     lm.D90 <- lm(weight ~ group - 1) # omitting intercept\n",
       "     \n",
       "     anova(lm.D9)\n",
       "     summary(lm.D90)\n",
       "     \n",
       "     opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))\n",
       "     plot(lm.D9, las = 1)      # Residuals, Fitted, ...\n",
       "     par(opar)\n",
       "     \n",
       "     ### less simple examples in \"See Also\" above\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae5db9e-e851-4a15-b826-c7ac507a9645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ X)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)            X  \n",
       "    -0.5429       1.5573  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res<-lm(Y~X)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3663ba-4fbd-438d-b855-76670b0ea2e5",
   "metadata": {},
   "source": [
    "Как видно, получились те же самые коэффициенты. Попробуем использовать результаты построенной модели. Для этих целей в среде R есть встроенная функция predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6d3a3e8-cc56-4d73-8ce2-e6eb1c0b49bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for predict {stats}\"><tr><td>predict {stats}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Model Predictions</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p><code>predict</code> is a generic function for predictions from the results of\n",
       "various model fitting functions.  The function invokes particular\n",
       "<em>methods</em> which depend on the <code>class</code> of\n",
       "the first argument.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "predict (object, ...)\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>object</code></td>\n",
       "<td>\n",
       "<p>a model object for which prediction is desired.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>additional arguments affecting the predictions produced.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>Most prediction methods which are similar to those for linear models\n",
       "have an argument <code>newdata</code> specifying the first place to look for\n",
       "explanatory variables to be used for prediction.  Some considerable\n",
       "attempts are made to match up the columns in <code>newdata</code> to those\n",
       "used for fitting, for example that they are of comparable types and\n",
       "that any factors have the same level set in the same order (or can be\n",
       "transformed to be so).\n",
       "</p>\n",
       "<p>Time series prediction methods in package <span class=\"pkg\">stats</span> have an argument\n",
       "<code>n.ahead</code> specifying how many time steps ahead to predict.\n",
       "</p>\n",
       "<p>Many methods have a logical argument <code>se.fit</code> saying if standard\n",
       "errors are to returned.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>The form of the value returned by <code>predict</code> depends on the\n",
       "class of its argument.  See the documentation of the\n",
       "particular methods for details of what is produced by that method.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Chambers, J. M. and Hastie, T. J. (1992)\n",
       "<em>Statistical Models in S</em>.\n",
       "Wadsworth &amp; Brooks/Cole.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>predict.glm</code>,\n",
       "<code>predict.lm</code>,\n",
       "<code>predict.loess</code>,\n",
       "<code>predict.nls</code>,\n",
       "<code>predict.poly</code>,\n",
       "<code>predict.princomp</code>,\n",
       "<code>predict.smooth.spline</code>.\n",
       "</p>\n",
       "<p>SafePrediction for prediction from (univariable) polynomial and\n",
       "spline fits.\n",
       "</p>\n",
       "<p>For time-series prediction,\n",
       "<code>predict.ar</code>,\n",
       "<code>predict.Arima</code>,\n",
       "<code>predict.arima0</code>,\n",
       "<code>predict.HoltWinters</code>,\n",
       "<code>predict.StructTS</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "\n",
       "require(utils)\n",
       "\n",
       "## All the \"predict\" methods found\n",
       "## NB most of the methods in the standard packages are hidden.\n",
       "## Output will depend on what namespaces are (or have been) loaded.\n",
       "## IGNORE_RDIFF_BEGIN\n",
       "for(fn in methods(\"predict\"))\n",
       "   try({\n",
       "       f &lt;- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))\n",
       "       cat(fn, \":\\n\\t\", deparse(args(f)), \"\\n\")\n",
       "       }, silent = TRUE)\n",
       "## IGNORE_RDIFF_END\n",
       "\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>stats</em> version 3.5.3 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{predict}{Model Predictions}{predict}\n",
       "\\keyword{methods}{predict}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "\\code{predict} is a generic function for predictions from the results of\n",
       "various model fitting functions.  The function invokes particular\n",
       "\\emph{methods} which depend on the \\code{\\LinkA{class}{class}} of\n",
       "the first argument.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "predict (object, ...)\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{object}] a model object for which prediction is desired.\n",
       "\\item[\\code{...}] additional arguments affecting the predictions produced.\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "Most prediction methods which are similar to those for linear models\n",
       "have an argument \\code{newdata} specifying the first place to look for\n",
       "explanatory variables to be used for prediction.  Some considerable\n",
       "attempts are made to match up the columns in \\code{newdata} to those\n",
       "used for fitting, for example that they are of comparable types and\n",
       "that any factors have the same level set in the same order (or can be\n",
       "transformed to be so).\n",
       "\n",
       "Time series prediction methods in package \\pkg{stats} have an argument\n",
       "\\code{n.ahead} specifying how many time steps ahead to predict.\n",
       "\n",
       "Many methods have a logical argument \\code{se.fit} saying if standard\n",
       "errors are to returned.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "The form of the value returned by \\code{predict} depends on the\n",
       "class of its argument.  See the documentation of the\n",
       "particular methods for details of what is produced by that method.\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "Chambers, J. M. and Hastie, T. J. (1992)\n",
       "\\emph{Statistical Models in S}.\n",
       "Wadsworth \\& Brooks/Cole.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{predict.glm}{predict.glm}},\n",
       "\\code{\\LinkA{predict.lm}{predict.lm}},\n",
       "\\code{\\LinkA{predict.loess}{predict.loess}},\n",
       "\\code{\\LinkA{predict.nls}{predict.nls}},\n",
       "\\code{\\LinkA{predict.poly}{predict.poly}},\n",
       "\\code{\\LinkA{predict.princomp}{predict.princomp}},\n",
       "\\code{\\LinkA{predict.smooth.spline}{predict.smooth.spline}}.\n",
       "\n",
       "\\LinkA{SafePrediction}{SafePrediction} for prediction from (univariable) polynomial and\n",
       "spline fits.\n",
       "\n",
       "For time-series prediction,\n",
       "\\code{\\LinkA{predict.ar}{predict.ar}},\n",
       "\\code{\\LinkA{predict.Arima}{predict.Arima}},\n",
       "\\code{\\LinkA{predict.arima0}{predict.arima0}},\n",
       "\\code{\\LinkA{predict.HoltWinters}{predict.HoltWinters}},\n",
       "\\code{\\LinkA{predict.StructTS}{predict.StructTS}}.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "\n",
       "require(utils)\n",
       "\n",
       "## All the \"predict\" methods found\n",
       "## NB most of the methods in the standard packages are hidden.\n",
       "## Output will depend on what namespaces are (or have been) loaded.\n",
       "## IGNORE_RDIFF_BEGIN\n",
       "for(fn in methods(\"predict\"))\n",
       "   try({\n",
       "       f <- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))\n",
       "       cat(fn, \":\\n\\t\", deparse(args(f)), \"\\n\")\n",
       "       }, silent = TRUE)\n",
       "## IGNORE_RDIFF_END\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "predict                 package:stats                  R Documentation\n",
       "\n",
       "_\bM_\bo_\bd_\be_\bl _\bP_\br_\be_\bd_\bi_\bc_\bt_\bi_\bo_\bn_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     ‘predict’ is a generic function for predictions from the results\n",
       "     of various model fitting functions.  The function invokes\n",
       "     particular _methods_ which depend on the ‘class’ of the first\n",
       "     argument.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     predict (object, ...)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "  object: a model object for which prediction is desired.\n",
       "\n",
       "     ...: additional arguments affecting the predictions produced.\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     Most prediction methods which are similar to those for linear\n",
       "     models have an argument ‘newdata’ specifying the first place to\n",
       "     look for explanatory variables to be used for prediction.  Some\n",
       "     considerable attempts are made to match up the columns in\n",
       "     ‘newdata’ to those used for fitting, for example that they are of\n",
       "     comparable types and that any factors have the same level set in\n",
       "     the same order (or can be transformed to be so).\n",
       "\n",
       "     Time series prediction methods in package ‘stats’ have an argument\n",
       "     ‘n.ahead’ specifying how many time steps ahead to predict.\n",
       "\n",
       "     Many methods have a logical argument ‘se.fit’ saying if standard\n",
       "     errors are to returned.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     The form of the value returned by ‘predict’ depends on the class\n",
       "     of its argument.  See the documentation of the particular methods\n",
       "     for details of what is produced by that method.\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Chambers, J. M. and Hastie, T. J. (1992) _Statistical Models in\n",
       "     S_.  Wadsworth & Brooks/Cole.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘predict.glm’, ‘predict.lm’, ‘predict.loess’, ‘predict.nls’,\n",
       "     ‘predict.poly’, ‘predict.princomp’, ‘predict.smooth.spline’.\n",
       "\n",
       "     SafePrediction for prediction from (univariable) polynomial and\n",
       "     spline fits.\n",
       "\n",
       "     For time-series prediction, ‘predict.ar’, ‘predict.Arima’,\n",
       "     ‘predict.arima0’, ‘predict.HoltWinters’, ‘predict.StructTS’.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     require(utils)\n",
       "     \n",
       "     ## All the \"predict\" methods found\n",
       "     ## NB most of the methods in the standard packages are hidden.\n",
       "     ## Output will depend on what namespaces are (or have been) loaded.\n",
       "     ## IGNORE_RDIFF_BEGIN\n",
       "     for(fn in methods(\"predict\"))\n",
       "        try({\n",
       "            f <- eval(substitute(getAnywhere(fn)$objs[[1]], list(fn = fn)))\n",
       "            cat(fn, \":\\n\\t\", deparse(args(f)), \"\\n\")\n",
       "            }, silent = TRUE)\n",
       "     ## IGNORE_RDIFF_END\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64511193-c565-403e-83fb-b9318766a0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 30 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>X</th><th scope=col>Y</th><th scope=col>g</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>20.7</td><td> 11.7</td><td> 31.69295</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>19.9</td><td> 19.8</td><td> 30.44712</td></tr>\n",
       "\t<tr><th scope=row>3</th><td> 9.3</td><td>  2.6</td><td> 13.93985</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>59.3</td><td> 43.6</td><td> 91.80431</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>24.7</td><td> 29.0</td><td> 37.92211</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>47.7</td><td> 98.5</td><td> 73.73976</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>24.2</td><td> 25.6</td><td> 37.14346</td></tr>\n",
       "\t<tr><th scope=row>8</th><td> 7.8</td><td>  6.2</td><td> 11.60392</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>38.3</td><td> 79.8</td><td> 59.10124</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>10.3</td><td> 10.1</td><td> 15.49714</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>35.7</td><td> 30.0</td><td> 55.05229</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>20.7</td><td> 21.2</td><td> 31.69295</td></tr>\n",
       "\t<tr><th scope=row>13</th><td> 8.2</td><td> 16.7</td><td> 12.22683</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>10.2</td><td>  9.1</td><td> 15.34141</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>23.5</td><td> 31.7</td><td> 36.05336</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>55.8</td><td> 54.4</td><td> 86.35380</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>10.3</td><td> 21.4</td><td> 15.49714</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>16.7</td><td> 41.1</td><td> 25.46379</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>15.8</td><td> 29.8</td><td> 24.06223</td></tr>\n",
       "\t<tr><th scope=row>20</th><td> 6.8</td><td> 10.9</td><td> 10.04663</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>22.4</td><td> 53.4</td><td> 34.34034</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>13.6</td><td> 22.6</td><td> 20.63620</td></tr>\n",
       "\t<tr><th scope=row>23</th><td> 9.9</td><td> 11.7</td><td> 14.87423</td></tr>\n",
       "\t<tr><th scope=row>24</th><td>24.0</td><td> 27.3</td><td> 36.83200</td></tr>\n",
       "\t<tr><th scope=row>25</th><td>23.0</td><td> 70.2</td><td> 35.27471</td></tr>\n",
       "\t<tr><th scope=row>26</th><td>75.1</td><td>124.2</td><td>116.40948</td></tr>\n",
       "\t<tr><th scope=row>27</th><td>56.2</td><td> 90.4</td><td> 86.97672</td></tr>\n",
       "\t<tr><th scope=row>28</th><td>60.7</td><td>101.7</td><td> 93.98452</td></tr>\n",
       "\t<tr><th scope=row>29</th><td>14.8</td><td> 18.2</td><td> 22.50494</td></tr>\n",
       "\t<tr><th scope=row>30</th><td>41.5</td><td>127.7</td><td> 64.08456</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & X & Y & g\\\\\n",
       "  & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 20.7 &  11.7 &  31.69295\\\\\n",
       "\t2 & 19.9 &  19.8 &  30.44712\\\\\n",
       "\t3 &  9.3 &   2.6 &  13.93985\\\\\n",
       "\t4 & 59.3 &  43.6 &  91.80431\\\\\n",
       "\t5 & 24.7 &  29.0 &  37.92211\\\\\n",
       "\t6 & 47.7 &  98.5 &  73.73976\\\\\n",
       "\t7 & 24.2 &  25.6 &  37.14346\\\\\n",
       "\t8 &  7.8 &   6.2 &  11.60392\\\\\n",
       "\t9 & 38.3 &  79.8 &  59.10124\\\\\n",
       "\t10 & 10.3 &  10.1 &  15.49714\\\\\n",
       "\t11 & 35.7 &  30.0 &  55.05229\\\\\n",
       "\t12 & 20.7 &  21.2 &  31.69295\\\\\n",
       "\t13 &  8.2 &  16.7 &  12.22683\\\\\n",
       "\t14 & 10.2 &   9.1 &  15.34141\\\\\n",
       "\t15 & 23.5 &  31.7 &  36.05336\\\\\n",
       "\t16 & 55.8 &  54.4 &  86.35380\\\\\n",
       "\t17 & 10.3 &  21.4 &  15.49714\\\\\n",
       "\t18 & 16.7 &  41.1 &  25.46379\\\\\n",
       "\t19 & 15.8 &  29.8 &  24.06223\\\\\n",
       "\t20 &  6.8 &  10.9 &  10.04663\\\\\n",
       "\t21 & 22.4 &  53.4 &  34.34034\\\\\n",
       "\t22 & 13.6 &  22.6 &  20.63620\\\\\n",
       "\t23 &  9.9 &  11.7 &  14.87423\\\\\n",
       "\t24 & 24.0 &  27.3 &  36.83200\\\\\n",
       "\t25 & 23.0 &  70.2 &  35.27471\\\\\n",
       "\t26 & 75.1 & 124.2 & 116.40948\\\\\n",
       "\t27 & 56.2 &  90.4 &  86.97672\\\\\n",
       "\t28 & 60.7 & 101.7 &  93.98452\\\\\n",
       "\t29 & 14.8 &  18.2 &  22.50494\\\\\n",
       "\t30 & 41.5 & 127.7 &  64.08456\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 3\n",
       "\n",
       "| <!--/--> | X &lt;dbl&gt; | Y &lt;dbl&gt; | g &lt;dbl&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | 20.7 |  11.7 |  31.69295 |\n",
       "| 2 | 19.9 |  19.8 |  30.44712 |\n",
       "| 3 |  9.3 |   2.6 |  13.93985 |\n",
       "| 4 | 59.3 |  43.6 |  91.80431 |\n",
       "| 5 | 24.7 |  29.0 |  37.92211 |\n",
       "| 6 | 47.7 |  98.5 |  73.73976 |\n",
       "| 7 | 24.2 |  25.6 |  37.14346 |\n",
       "| 8 |  7.8 |   6.2 |  11.60392 |\n",
       "| 9 | 38.3 |  79.8 |  59.10124 |\n",
       "| 10 | 10.3 |  10.1 |  15.49714 |\n",
       "| 11 | 35.7 |  30.0 |  55.05229 |\n",
       "| 12 | 20.7 |  21.2 |  31.69295 |\n",
       "| 13 |  8.2 |  16.7 |  12.22683 |\n",
       "| 14 | 10.2 |   9.1 |  15.34141 |\n",
       "| 15 | 23.5 |  31.7 |  36.05336 |\n",
       "| 16 | 55.8 |  54.4 |  86.35380 |\n",
       "| 17 | 10.3 |  21.4 |  15.49714 |\n",
       "| 18 | 16.7 |  41.1 |  25.46379 |\n",
       "| 19 | 15.8 |  29.8 |  24.06223 |\n",
       "| 20 |  6.8 |  10.9 |  10.04663 |\n",
       "| 21 | 22.4 |  53.4 |  34.34034 |\n",
       "| 22 | 13.6 |  22.6 |  20.63620 |\n",
       "| 23 |  9.9 |  11.7 |  14.87423 |\n",
       "| 24 | 24.0 |  27.3 |  36.83200 |\n",
       "| 25 | 23.0 |  70.2 |  35.27471 |\n",
       "| 26 | 75.1 | 124.2 | 116.40948 |\n",
       "| 27 | 56.2 |  90.4 |  86.97672 |\n",
       "| 28 | 60.7 | 101.7 |  93.98452 |\n",
       "| 29 | 14.8 |  18.2 |  22.50494 |\n",
       "| 30 | 41.5 | 127.7 |  64.08456 |\n",
       "\n"
      ],
      "text/plain": [
       "   X    Y     g        \n",
       "1  20.7  11.7  31.69295\n",
       "2  19.9  19.8  30.44712\n",
       "3   9.3   2.6  13.93985\n",
       "4  59.3  43.6  91.80431\n",
       "5  24.7  29.0  37.92211\n",
       "6  47.7  98.5  73.73976\n",
       "7  24.2  25.6  37.14346\n",
       "8   7.8   6.2  11.60392\n",
       "9  38.3  79.8  59.10124\n",
       "10 10.3  10.1  15.49714\n",
       "11 35.7  30.0  55.05229\n",
       "12 20.7  21.2  31.69295\n",
       "13  8.2  16.7  12.22683\n",
       "14 10.2   9.1  15.34141\n",
       "15 23.5  31.7  36.05336\n",
       "16 55.8  54.4  86.35380\n",
       "17 10.3  21.4  15.49714\n",
       "18 16.7  41.1  25.46379\n",
       "19 15.8  29.8  24.06223\n",
       "20  6.8  10.9  10.04663\n",
       "21 22.4  53.4  34.34034\n",
       "22 13.6  22.6  20.63620\n",
       "23  9.9  11.7  14.87423\n",
       "24 24.0  27.3  36.83200\n",
       "25 23.0  70.2  35.27471\n",
       "26 75.1 124.2 116.40948\n",
       "27 56.2  90.4  86.97672\n",
       "28 60.7 101.7  93.98452\n",
       "29 14.8  18.2  22.50494\n",
       "30 41.5 127.7  64.08456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g<-predict(res,data.frame(X,Y))\n",
    "View(data.frame(X,Y,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a85db79c-d8ca-460e-a095-554dc17ed70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94\nAAAgAElEQVR4nOzde3zPheLH8fd3d7NbZpjjXmIbxqw0d0kuHUMhKqSbO5EudBnFkqJccimV\nk1xSlBpdXHM35DYsJiG5bmY2s+v394d+5bIJ2fezz2ev51/nfD7fM++dR6fz6vP9fj5fm91u\nFwAAAMzPyegBAAAAuDUIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrAD\nAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAI\nwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAA\nwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7\nAAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACL\nIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAA\nACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKw\nAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACw\nCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4A\nAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCII\nOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAA\niyDsAAAALIKwAwAAsAgXoweYw44dO7Kzs41eAQAACgUXF5fQ0FCjV+SBsPtnW7Zsueuuu4xe\nAQAACpHNmzeHh4cbveJKhN0/y8zMlJSRkeHm5mb0FgAAYLDMzEx3d/eLeVDY8Bk7AAAAiyDs\nAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAs\ngrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCJcjB4AACZ06JA++US7dikl\nRSEh6tBBTZoYvQkAuGIHADdq3jwFBembb1SunO6+W/v3q3lzPfOMcnONXgagqOOKHQDciG3b\n1L27xozRkCF/H4yNVatWqlRJw4cbtwwAuGIHADdk7Fi1bn1Z1Um6+26NHau331ZWlkGzAEAi\n7ADgxqxZowcfzOP4gw8qOVm7djl8EAD8jbADgBtx7pxKlMjjuJ+fnJyUkuLwQQDwN8IOAG5E\nuXJKSMjj+IEDys1V+fIOHwQAfyPsAOBGtG+v6dOVnn7l8QkTVKOGbr/diE0A8CfCDgBuxNCh\nysxUmzbav//PIykpeuklffCBJkwwdBkAEHYAcENuu00rV0rSnXcqMFB33qkSJTR7thYt0r33\nGj0OQFHHc+wA4AZVrKiVK7Vnj3bu1LlzCglReLjc3IyeBQCEHQDcnOBgBQcbPQIALsNbsQAA\nABZB2AEAAFgEYQcAAGARhB0AAIBFcPMEAACwtP37NXeu4uIkqUYNde2qqlWN3lRQuGIHAACs\na+JEhYTom29UsqRKltQ33ygkRBMnGj2roHDFDgAAWNS33+q55zRzph599O+Dn32mnj1VpYr+\n+1/jlhUUrtgBAACLeuMN9et3WdVJeuwx9eun1183aFPBIuwAAIAVpaVpyxZ17pzHqU6dtGWL\nzp93+KYCR9gBAAArOntWdrtKlszjVECA7HadPevwTQWOsAMAAFZUsqTc3fXrr3mcOnBA7u7y\n93f4pgJH2AEAACtyc1PLlnr/fdntlx232zVlilq2lJubQcsKkGnDzp6dlnTs6PHkjFyjlwAA\ngMIpOlqrVqlXL5058+eRpCQ984xWrVJ0tKHLCoqpws6e9tvqmVHdm4ZUCPD2cPf2L1susEQx\nd6+S5YMadX35458Onbf/888AAABFRUiIfvhBK1aodGmFhCgkRGXKaOVK/fijQkKMHlcgzPMc\nu/PbJz8cOWTxkSw5ewdWrVon2L+kn6f9fHJSUuIfBzZ9Hr3287dHRk6ImdunZjGjpwIAgEKi\nfn3Fx2v9eu3eLUkhIapfXy7m6Z8bZJZfLHNLdLfnFh8r1Wrk5NHPtKpTxuOys1mJe1fNHjXg\npbmDu71Vb/OIMFeDVgIAgELHxUWNG6txY6N3OIJJ3orN3jZ/frzCo7776rX2V1adJFf/oBYD\nZy2b1MZjz7z527KNWAgAAGA0k4RdzoljJ+z+dSOquef/GqfAiIjbdezo8RzH7QIAACg8TBJ2\nzqXLlrElbotNyMz/Nbmntm75TWXKlnZ23C4AAIDCwyRh5xLWqXOQYqNad4yOiTt1Vd1lJyes\nnNKzeZ9FqdU7dwozy+cGAQAAbimzRJBr+LBZ43e0HRzzctuYKN/yQdUqlPL39/O0pZ9NSjp9\ndP+eXxMz5VquzbhPh4Vz5wQAACiazBJ2kmdov0V7H1g9f8YHnyz4aXfcxrj0bLtdNpuLh1+p\n8uGdn+nZ66kuTat42YzeCQAAYBDzhJ0kW/FKTXqOatJzlKTcjJTTp1Ls3gEBvu4meT8ZAACg\nQJkq7CQpN/3krwfPFi9fOdCnVDmfy0+dTzx+JsPDv2yJq56HAgAAYH0mutiVe2bzlG6hJX3L\nVA258z8lKzTo/fGO1EvP20/NfqRKudt7x2QYtRAAAMBIprlil7F9TJt7X9l0oURQ8451/FO2\nL13xwdP37U9ZseTZmtd4th0AAEDRYZIrdvYz34wZF5tx59MLd+5Y+sVn837YGf/jC+EZq155\n5t1d13i0HQAAQBFikrDL2rZq7VnP1sNH/7fsxWuMTgHNRs0a1cQpduzLc47mGrwOAACgMDBJ\n2NlTklPkX7my7yVPM3G585kx/YNTl4x6a3Vq/v/Jf/Drr7+6ubnZrqlBgwaSsrP5DloAAFCo\nmeQzdk6lygToxPZtv+c2qvx3i3rcNXRMt9ntPhz8Vrf1b9x1U7fCVq5cecWKFRcuXLjGa2Ji\nYiZMmJCby4VBAABQqJkk7FxDW94XOGHGyB7DK338auQdxf88bCvRZsx7j/zYaWzXZ6p/P/2+\nm/jBNputYcOG137NgQMHbuInAwAAOJhJ3opV8fteHfdolfS1b7WvFhBQ9b8Td+dcPG4LaP/u\nrBfrJs3pVrt6y3HbsoxdCQAAYCCzhJ2cynX+3/ZtX0R1u7926ZyTp9Ls/3/C5t901PcrPn62\nafFDCad5txQAABRdJnkr9qLi1R4a8b+HRlx13OZTu8f4mO7RiYcSEg5lV3B1/DIAAADjmSrs\nrs3m4V+phn8lo2cAAAAYxDRvxQIAAODaCDsAAACLIOwAAAAswiSfsctY1LPS419f6ynC/8+j\n/czfPmnnXuCLAAAAChuThJ3r3U+O6JUyaerXu1NynXzKVa/gl9+lRnf/YrZ8TgEAgKLKLrsk\nmyxeCSYJO6fAhr3GNHwk8vn6TcclNH1zw6LHfIyeBAAACr0c5UzV1FmatVu7JYUopJu69VEf\nZzkbPa1AmOozdt71nu5WxyQpCgAADJapzLZqG6WoVmr1hb74Ql+0UqsoRbVV20xlGr2uQJgr\nk5wrhtYpXSzRmo0NAABuqXf0zs/6ebM2V1GVi0daq3UP9YhQxDt6Z7iGGzuvIJjqip3k3nLa\nkaQvuvI+LAAAuCa77NM0bbiG/1V1F1VRlZf18nRNv/ipO4sxWdgBAABcjzM6c0RHmqrp1aea\nqMlhHT6jMw4fVeAIOwAAYEHZypbkktenzlzlKilHOY7eVPAIOwAAYEElVbKkSm7RlqtPbdbm\nkirpL3/HrypohB0AALAgJzl1U7fRGp2s5EuPJyt5tEZ3UzcnK1aQBX8lAAAASa/pNXe5Ryhi\nnub9ql9/1a9zNTdCER7yiFKU0esKhLkedwIAAHC9/OS3Rmte1su91fuszkrykU83dYtWtI+s\n+YgNwg4AAFiWr3wna/JkTT6kQ5IqqqLRiwoWYQcAAKzP8kl3EZ+xAwAAsAjCDgAAwCIIOwAA\nAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDs\nAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAs\ngrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMA\nALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjC\nDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADA\nIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACzCtGFnz05LOnb0eHJGrtFL\nAAAACgdThZ097bfVM6O6Nw2pEODt4e7tX7ZcYIli7l4lywc16vryxz8dOm83eiEAAIBxXIwe\ncN3Ob5/8cOSQxUey5OwdWLVqnWD/kn6e9vPJSUmJfxzY9Hn02s/fHhk5IWZun5rFjJ4KAABg\nBLOEXeaW6G7PLT5WqtXIyaOfaVWnjMdlZ7MS966aPWrAS3MHd3ur3uYRYa4GrQQAFHXnzmnR\nIu3apexsBQerbVuVKmX0JhQhJnkrNnvb/PnxCo/67qvX2l9ZdZJc/YNaDJy1bFIbjz3z5m/L\nNmIhAADff68qVTRkiHbu1IEDGjFClStr5kyjZ6EIMckVu5wTx07Y/ZtHVHPP/zVOgRERt2vN\n0eM5pvm1AADWsWOHOnTQs89q5Ei5uUlSbq6mTNHTT6tUKbVpY/Q+FAkmuWLnXLpsGVvittiE\nzPxfk3tq65bfVKZsaWfH7QIA4E8jR6plS7355p9VJ8nJSf37q39/DR9u6DIUISYJO5ewTp2D\nFBvVumN0TNypq+ouOzlh5ZSezfssSq3euVMYl+sAAI63dKl69MjjeI8e2rFDJ086fBCKIrNE\nkGv4sFnjd7QdHPNy25go3/JB1SqU8vf387Sln01KOn10/55fEzPlWq7NuE+HhXPnBADA0dLT\nlZqqsmXzOPWf/0jSqVPcRQEHMEvYSZ6h/RbtfWD1/BkffLLgp91xG+PSs+122WwuHn6lyod3\nfqZnr6e6NK3iZTN6JwCgCCpWTF5eOno0j1O//y6JqoNjmCfsJNmKV2rSc1STnqMk5WaknD6V\nYvcOCPB1N8n7yQAAK2vZUjNn6sEHrzw+c6bq1FFAgBGbUOSYtolszs4uzjY73zUBACgcoqK0\nbJmef14ZGX8eycnRe+9pyhRFRxu6DEWIqa7Y2dN+W/PFJzNmfrlq9+ETSWmZuXbZbC6eJcqU\nD2r4YM/ez3RpXNGTt2IBAIaoWVOLFumxx/TJJwoLk4uLtm/XuXP65BO1amX0OBQV5gk7vlIM\nAFDItWihAwcUE6Ndu5SVpc6d9d//qmRJo2ehCDFL2PGVYgAAM/DyUpcu6tLF6B0ookwSdn9/\npdgrNfP48ok/v1Ks+NkafebN3/ZK2N3X/WtlZ2fHxMRkZWVd4zVbt269ickAAAAOZpKwK7Cv\nFDt69Gi/fv3S09Ov8ZqMjAxJdm7UAAAAhZtJwu7vrxRrHuyWz2v+/Eqx+27oK8UqVqx4NM/H\nDl1i+vTpvXv3ttm4LwMAABRqJnncCV8pBgA3bdMmPfqogoJUurSaNtWYMbrm2xQAzMssEcRX\nigHATZkyRYMGKTJSgwapRAnFxen99zVnjpYv55G5gPWYJez4SjEAuHHbtmngQH3yibp1+/NI\n584aMkTNm6tXLy1caOg4ALeeecJOfKUYANygyZN1//1/V91Ffn6aMkURETpyROXLG7QMQIEw\nbRM5ufuUKleuNFUHAPnbulUtWuRxvF49+fiIZzkBlkMWAYB1ZWaqWD7fxuPh8fdXmgKwCsIO\nAKzrjju0Y0cex//4QydPqmpVhw8CULAIOwCwrkce0axZ2r//yuNRUapeXXXqGLEJQAEyyc0T\nGYt6Vnr86wvX8UqP9jN/+6TdNb6gAgCKjocf1pw5atxYY8eqRYs/H3cyfrwWLNCyZeK564Dl\nmCTsXO9+ckSvlElTv96dkuvkU656Bb/8LjW6+xfj71QAcJHNpi+/1Ouvq18/nTv358F69bRm\njcLDDV0GoECYJOycAhv2GtPwkcjn6zcdl9D0zQ2LHvMxehIAmIKbm0aN0ogROnBAp08rOFi3\n3Wb0JgAFxVSfsfOu93S3OiZJUQAoTFxcVK2aGjSg6gBrM1XYybliaJ3Sxdycjd4BAABQCJns\n+pd7y2lHkoweAQAAUCiZ64odAAAA8kXYAQAAWARhBwAAYBGEHQAAgEUQdgAAABZB2AEAAFiE\nyR53AgBAIZepzHjFZykrSEGe8jR6DooWrtgBAHBrnNGZJ/Wkt7xDFRqucG95d1CHIzpi9C4U\nIYQdAAC3QIpSmqhJrGK/1JeJSjyrs8u0LFGJ9+ge2g4OQ9gBAHALjNGYNKWt0Zq2altCJXzk\n00zNlmlZJVV6Xs8bvQ5FBWEHAMAtMFuzh2qon/wuPegmt1f16tf6+rzOGzUMRQphBwDAv5Wh\njCM6EqrQq0/VVu0MZRzWYcevQhFE2AEA8G+5yMVJThnKuPrUBV2Q5C53h49CUUTYAQDwbznL\nubZqL9XSq08t1dIABVRQBcevQhFE2AEAcAsM0ICJmrhJmy49eEAHXtNrfdTHWc5GDUORwgOK\nAQC4Bbqr+wZtaKImj+vx+qrvJrdYxX6sjxuq4ct62eh1KCoIOwAArkua0vZqr7OcgxV89Wfm\nbLJN07SWavmRPnpNr2UqM0Qh4zSup3o68f4YHIWwAwDgHxzRkYEa+I2+yVWuJBe5PKJHxmu8\nv/yveGUHdeigDkZsBCQ+YwcAwLX9rt8jFJGoxOVanqKUMzrzjb7ZoR2N1ChZyUavAy5D2AEA\ncC0v6sXyKr9My5qqqbe8/eTXWq1Xa3WOckZrtNHrgMsQdgAA5Ctd6Qu18BW94ia3S4/7yGeo\nhs7RHKOGAXki7AAAyNdRHb2gC3l+pUSoQv/QH3xXGAoVwg4AgHxdvPv14rdHXOGCLjjJyVWu\nDh8F5Iu7YgEA1vLDD/ryS8XFydNTNWroySdVq9ZN/7D/6D9lVGaplt6hO6449aN+rKVahB0K\nFa7YAQCsIjdXTz6ptm2VnKx27dSwoXbvVt26mjDhpn+kk5z6qd8IjdinfZcej1Xse3pvgAb8\n69HArcQVOwCAVbzzjr76Shs2qG7dvw/Onatu3VSjhpo3v7mf+qJe3Kqt4Qp/Qk/cpbuylLVB\nG/6n//VQj57qeWuWA7cIYQcAsIScHI0bp1GjLqs6SV27aulSvf32TYedq1wXauGn+nSu5i7Q\nAhe51FCNuZrLg4hRCBF2AABLSEjQyZOKjMzjVGSkunf/Nz/bJlsP9eihHv/mhwAOwGfsAACW\nkJoqSb6+eZzy9dX587LbHbwIcDzCDgBgCRUqyGbTL7/kcSo+XuXLy2Zz+CbA0Qg7AIAlBASo\ncWONHXvl8fR0TZ6shx4yYhPgaIQdAMAq3n1XS5aoZ08dOfLnkR071KqV0tM1bJihywAHIewA\nAFZRp46WL9eWLapQQaVKyddXtWurWDH99JP8/Y0eBzgCd8UCACykXj3t3Km9e7V7tzw8VKuW\nKlY0ehPgOIQdAMBabDYFBys42OgdgAF4KxYAAMAiCDsAAACLIOwAAAAsgrADAFjQKZ1KUYrR\nKwBHI+wAANZxRmf6qm+AAkqplK98K6tytKKzlGX0LsBBuCsWAGARJ3WygRp4yGO8xtdV3Qu6\nsFZrR2v0aq3+Vt+6ytXogUCBI+wAAFZwQRee0BNOcvpO35VTuYsHwxQWqchwhU/V1IEaaOxC\nwAF4KxYAYG7ndK6P+vjKd7EWJyihvMo3UZPd2n3xbCVVGqRBH+tjY0cCjkHYAQBM7IIutFCL\nFVrxtt6W9If+2KzNJVSigRrs0q6Lr7lH9+zVXrvshi4FHIGwAwCY2GRNPqRDa7SmmZpJcpFL\nuMIXamEzNeuv/hdfk6tcJ/7/DkUDf6EDAExsrub2Vd9SKnW7bveU51qtlWSTLUpRa7TmqI5K\nWqu1NVTDJpvRY4ECR9gBAEzsgA7UUi1JnvJ8VI8O1/BkJUu6WHIHdGCv9k7UxKf1tNFLAUcg\n7AAAJuYhj3SlX/zXb+ktF7mEKWySJq3W6lzlztbsCEXcr/uf1JPG7gQcg7ADAJjYXbrrB/1w\n8V/fptvWa31XdZ2oiS3UQtJGbYxW9Of63FnOhs4EHISwAwCY2EAN/EyfxSjm4r8truKjNXql\nVlZQhb7qu0M7+qovd06g6OABxQAAE2uhFlGKaq/2ndSpiZp4ynO7ts/UzFCFXnwAClCk8A8x\nAABze0WvrNAKu+wTNfEVvbJHe97Um0u11FOeRk8DHI0rdgAA02usxo3V2OgVgPG4YgcAAGAR\nhB0AAIBFEHYAAAAWQdgBAABYBGEHAABgEYQdAACARRB2AAAAFkHYAQAAWARhBwAAYBGEHQAA\ngEUQdgAAABZB2AEAAFiEacPOnp2WdOzo8eSMXKOXAAAAFA6mCjt72m+rZ0Z1bxpSIcDbw93b\nv2y5wBLF3L1Klg9q1PXlj386dN5u9EIAAADjuBg94Lqd3z754cghi49kydk7sGrVOsH+Jf08\n7eeTk5IS/ziw6fPotZ+/PTJyQszcPjWLGT0VAADACGYJu8wt0d2eW3ysVKuRk0c/06pOGY/L\nzmYl7l01e9SAl+YO7vZWvc0jwlwNWgkAAGAgk7wVm71t/vx4hUd999Vr7a+sOkmu/kEtBs5a\nNqmNx55587dlG7EQAADAaCYJu5wTx07Y/etGVHPP/zVOgRERt+vY0eM5jtsFAABQeJgk7JxL\nly1jS9wWm5CZ/2tyT23d8pvKlC3t7LhdAAq7nBzt26fNm5WaavQUAChwJgk7l7BOnYMUG9W6\nY3RM3Kmr6i47OWHllJ7N+yxKrd65U5hZPjcIoEClp2voUPn6qlo13X23fHzUsqV++cXoWQBQ\ngMwSQa7hw2aN39F2cMzLbWOifMsHVatQyt/fz9OWfjYp6fTR/Xt+TcyUa7k24z4dFs6dEwCU\nlaXWrfXbb5oxQ02ayNtbO3YoOlr33KO1axUSYvQ+ACgQZgk7yTO036K9D6yeP+ODTxb8tDtu\nY1x6tt0um83Fw69U+fDOz/Ts9VSXplW8bEbvBFAYfPCB4uK0fbvKlfvzSIMG+vZbdeigvn31\n00+GjgOAgmKz2835VN/cjJTTp1Ls3gEBvu7/4v3kU6dODRo0KDv7WnfS/vrrr1u3bj137pyX\nl9fN/0kAHCkiQs2ba9SoK4/v2KHatXX4sMqXN2IWACvIzMx0d3dft25d/fr1jd5yJfNcsbuC\nk7tPqXI+krLPHt59INnn9uCKvjfxy7i7u1euXDkn51p30qakpNzsSgAGSUjQc8/lcbxmTbm4\nKCGBsANgSWYKO3vqnq+mfByzKf60x52t+7z4TMPSWXs/fvqhwXPiU3Ll7FW1wxuffjCo3m03\n9Gasj4/P6NGjr/2a6dOn//DDD/9mOQBHc3NTRkYex7OylJMjNzeHDwIARzBN2OUe/7b/fY9M\n251ql6TFMV8tP7jgzZwhveccq9ysa+dKWfHLFy8Yct/RjPWrXqzJ37KBIq9uXS1bpkcfvfL4\n8uVydVWNGkZsAoACZ5LHnej8yjcGfLC3eLPXvt5++I+Dmz7rG3RwfLsOk45GvLlm27LZH86Y\nv3rbjy+EZm0aN/Y73jcFoH799Nln+v77yw6ePKkhQ9Stm3x9DZoFAAXLJGGXtW3xd787Nxj2\nUVS70PKBle5+dNz7ve+wZ3lHDu5X01OSZPOr//yQlp5Ja1buyDJ4LADjtWyp4cPVtq169tQn\nn2jBAr36qmrVkp+fxo83ehwAFBSThF3uqROn7SWCgsv+/1736jWqujiXqVTh7+8Ys/lUqlxS\nSacSc43ZCKBwGTlSS5YoOVlvvKFevbR6tV54QatXy8fH6GUAUFBM8hk7p4DSJW1J8XuP5bao\n6CRJmfv2JGTnnjx0JEN/fdNE2u9HElWiZAmTxCqAAteihVq0MHoEADiOSSLItU6bVv/JWfvm\n06OX7D2eeHT7/Bf6T90np5RvJkzfky5Jsp+LHTd+SdptDZvW5qsnAABAkWSSK3byvPfViU/9\n9OgHrz0Q/Jok2YrVeHbBG9lDOz3f8K5lkY0rZv+ybNHKA853vTH0Ad5lAQAARZNZwk5Oge2m\nbtzQbNKMmI3xpz2qPTBgeJ9GZTIqTzr+4NAv/7fbbrMVqxT51qwZz9XmWScAAKCIMk3YSbJ5\n13x4+ISHLzniWbP3/N2dEuISUnzvCKni784XxQIAgCLMTGGXNzf/O8L8jR4BAABgPJPcPAEA\nAIB/QtgBAABYBGEHAABgEYQdAACARRB2AAAAFkHYAQAAWARhBwAAYBGEHQAAgEUQdgAAABZB\n2AEAAFgEYQcAAGAR5v+uWADAP4lT3DZtS1RikIIiFOEjH6MXASgQhB0AWNkxHeuhHku1tIIq\n3Kbb9mmfu9zHadwTesLoaQBuPcIOACwrXen36T4f+cQrvpqqScpU5jRN663eLnLpru5GDwRw\nixF2AGBZ0zQtWcnrtd5XvhePuMltoAZmKGOohnZRFze5GbsQwK3FzRMAYFnf6ttu6vZX1f2l\nl3qd0ZmN2mjIKgAFh7ADAMs6qqNVVOXq4z7yCVDAUR11/CQABYqwAwDL8pNfosjLfvQAACAA\nSURBVBKvPp6t7GQl+8nP8ZMAFCjCDgAsq7Eaf6kv7bJfcXyxFuco5x7dY8gqAAWHsAMAyxqk\nQQlKeFbPZiv7r4M7tbOP+vRX/9t0m4HbABQE7ooFAMsqp3KLtKiTOn2jb5qpWQmViFPcci3v\npE5jNMbodQBuPa7YAYCVNVXTeMUP0qAc5cQrvpZq/aAf5miOq1yNngbg1uOKHQBYnL/8n9Wz\nRq8A4AhcsQMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADbty5c9q0Sfv2KSfH6CkAAPyNsANu\nRFyc7r1XPj665x5VqyY/P730ki5cMHoWAAASYQfcgG3bVL++fH21YYPS0nT0qKZP1+zZiozk\n0h0AoDAg7IDr1ru32rTRwoW65x55eqpsWT3yiNasUWysPvnE6HEAABB2wHXat0+xsXr9ddls\nlx2vVElPPaU5cwyaBQDA3wg74PokJMjTU3femcep2rW1f7/DBwEAcCXCDrg+7u7Kysr7s3QX\nLsjd3eGDAAC4EmEHXJ/QUOXm6qef8ji1bJnCwhw+CACAKxF2wPUpWVJdumjQICUmXnZ80SJ9\n8YX69zdoFgAAf3MxegBgHpMm6b77VLOmnnpKoaE6c0arVmnePI0ercaNjR4HAABhB1y/227T\nunWaNEkxMZo6Vb6+ql1by5erSROjlwEAIBF2wI3x8NDzz+v5543eAQBAHviMHQAAgEUQdgAA\nABZB2AEAAFgEYQcAAGARhB0AAIBFEHYAAAAWQdgBAABYBGEHAABgEYQdAACARRB2AAAAFpFn\n2F1Iv+DoHQAAAPiX8gq7jJjeYQ+8+vX+NIevAQAAwE3L+63Y9INLRj0YWuuB177ef97BgwAA\nAHBz8go79wfeWhjdOdj1t+/eeDC05n9fW0TdAQAAFH55XrErVrn1sM+3xa+a8ER48cNL3ugQ\nWvO/ry1KoO4AAAAKs/zvinUNbDTgow3xGz8e1LjE0Yt1F0XdAQAAFFou1z7t7F/38fdWPtRr\nwVvPv/Tuktc7rJgZFnHnbZfWoFuTV79+pbFrgY4EgELHLvtBHUxVajVVc5e70XMAQPrHsJMk\n2byrt+jyyOpl69/fdObw1hWHLzvpcVuv3IKZBgCF0gVdGKER0zTtrM5KcpFLa7WeoAmVVdno\naQCKun8Mu9wz22aNfHb41DV/ZHlUbvPqO6N61i1x6RU7W/EA/kkVQJGRpaw2apOghMma3EiN\nfOTzs35+U2/erbvXad2dutPogQCKtGuFXc7p2I9fe/bVGRtPZLuVu/eF8ZNf7Vjdy+awaQBQ\n+EzTtJ3auV3by6ncxSPN1bypmj6gB/qr/4/60dh5AIq4fG6eyDq+7v2n7glq0GvqxiT/hoM+\n3bx76VudqDoARd5n+qyv+v5VdRc5y/kNvbFMy47ruFHDAEB5h13WppERwY0HfLQ1+bZ6vT/c\nsHvVe4/V9KHpAEDap31hCrv6eB3Vscm2X/sdPwkA/pLXW7G5R3bvSfGr88So98c8c0+As8M3\nAUCh5Sa3TGVefTxTmbnKdRXPCABgpLyu2DlVbDduRdzGj/pQdQBwuTqqs1zLrz6+Qivc5R6s\nYMdPAoC/5BV2rnc92qdxIP/YCQBX6au+MzVzlVZdevCUTg3V0G7q5iMfg3YBgHR9z7EDAPwp\nUpEDNfB+3f+EnmisxsVVfId2TNO0cio3TuOMXgegqCPsAODGvK23G6vxVE19Xs+nKjVYwc/q\n2UEaxPdPADAcYQcAN6yt2rZVW6NXAMCV8nmOHQAAAMyGsAMAALAI04adPTst6djR48kZuUYv\nAQAAKBxMFXb2tN9Wz4zq3jSkQoC3h7u3f9lygSWKuXuVLB/UqOvLH/906Lzd6IUAAADGMc/N\nE+e3T344csjiI1ly9g6sWrVOsH9JP0/7+eSkpMQ/Dmz6PHrt52+PjJwQM7dPzWJGTwUAADCC\nWcIuc0t0t+cWHyvVauTk0c+0qlPG47KzWYl7V80eNeCluYO7vVVv84gwHq4MAACKIJO8FZu9\nbf78eIVHfffVa+2vrDpJrv5BLQbOWjapjceeefO3ZRuxEAAAwGgmCbucE8dO2P3rRlS7xuM/\nnQIjIm7XsaPHcxy3CwAAoPAwSdg5ly5bxpa4LTYhM//X5J7auuU3lSlb2tlxuwAAAAoPk4Sd\nS1inzkGKjWrdMTom7tRVdZednLBySs/mfRalVu/cKcwsnxsEAAC4pcwSQa7hw2aN39F2cMzL\nbWOifMsHVatQyt/fz9OWfjYp6fTR/Xt+TcyUa7k24z4dFs6dEwAAoGgyS9hJnqH9Fu19YPX8\nGR98suCn3XEb49Kz7XbZbC4efqXKh3d+pmevp7o0reJlM3ongMLkd/2+S7vO6myIQkIU4mSW\ntykA4KbY7HZzPtU3NyPl9KkUu3dAgK/7v/gb9YEDB6pXr56d/c930qakpHh7e9/8nwTAsU7o\nRG/1XqRFnvL0ktcJnaiqqh/og6ZqavQ0AOaWmZnp7u6+bt26+vXrG73lSua5YncFJ3efUuV8\nlHv+ZML2o9ml76waWPxmbpq4/fbbt2zZcu2wW7hwYXR0tM3GxUDANFKVeq/u9ZTnJm0KV7hN\ntmM6Fq3olmq5TMsaqZHRAwGgQJgp7DIPfTf+9Unba78ze0Cws3JObZgypE/UvJ1nsu2yORev\n3PK5iZOHPVD5qofc/ZPQ0NBrv2DLli03OxmAMd7Te2lK26ANPvK5eCRQgZM0KUMZ/dV/h3YY\nOw8ACohpwi573/T2jft9f9LtnrFOks5veLVF8zE7s/xD7u9yT+XiZ+NX//D9G5H3/PzBhq+f\nrMLzToCiboEW9FKvv6ruLy/ohaqqul/7q6qqIcMAoECZ5HPE9lNfvvLKD8nVnv5yz8qh1Z3t\nJz4fPWlnbo1B3+3Z/v3cD6fOmL8ybte87pWSlrw6+odUo8cCMNxhHa6malcfv123u8r1sA47\nfhIAOIBJwi5r67KVSZ6th4/pUMldUta2dbHnvf47LOq+gP+/OudWoeM7Ua09Ty7/4ecsI5cC\nKAy85JWilKuPpyktS1le8nL8JABwAJOEnXJz7bbiAQHF/7yBIScnx+ZTurTnpS+x+VasVEJn\nkpJzjRgIoDCpr/qLtOjq49/q2+IqXku1HD8JABzAJGHnUr1WkEviioUrk+yS5FKzbqjrqS2b\nD176tbBZ+9ZuOG4L/E8gH7EDirzn9Ny3+na6pl96cI/2DNGQARpQTMWMGgYABcokYedUucuA\nDgEHP+z6wKD/bfg93al891f7VN01ttcbP5242HbpB74e+viYrfaqHR4MNc0NIQAKSrjCP9SH\nAzWwsRq/olfGauwjeqSu6jZUw9f1utHrAKCgmCWCbGU6vf/FgTMPj5j8eIOpg8pXv7NiqeKl\n0le/fm+VqXdUr+B2ev++IynZ/o1e/3jYXW5GbwVQGPRUzwhFzNCMTdqUopQQhczTvHZqZ/Qu\nAChAZgk7yVaiwbDvE7qvn//B+x99uS5++77TqZl2u9JP/RqfXaZKvZ5v9e77RNta/ub5hQAU\ntOqq/o7eMXoFADiOyTrI4z/1u4+s332kJHvWudMnz+Z6/cvvFAMAALAKk4XdJWyu3gH/4btb\nAQAA/h/XugAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAswrx3xQKwJrvsC7UwRjF7tMdX\nvrVV+2k9XVVVjd4FACbAFTsAhUiGMtqrfTd1y1JWR3UMV/g6raulWrM12+hpAGACXLEDUIgM\n07Cf9fN2bb9Td/51cKImPq7Ha6pmLdUycBsAFH5csQNQWJzTuamaOkETLq06SQM1sIVajNd4\no4bBBOLiNGiQ7r1X4eHq0UNff230IMAYhB2AwuJn/ZylrAf0wNWnIhW5XusdPwnmMH26wsK0\na5caN1aXLsrJUdeu6tRJWVlGLwMcjbdiARQWqUr1kIe73K8+5SvfVKU6fhJMYMMG9eunDz9U\nz55/Hxw+XPfeqxEjNHq0ccsAA3DFDkBhUVEV05T2u36/+lS84iuqouMnwQTeeUcPPnhZ1UkK\nDtY772jSJF24YNAswBiEHfBPsrMVH69Nm3TunNFTLC5EIdVUbazGXnH8jM7M0IyH9JAhq1DY\nbdigyMg8jrdtq3PnFBfn8EGAkQg7IH/p6XruOfn5KShI99wjHx81b649e4yeZVk22d7X+9M0\nbYiGnNRJSbnK3aiN9+reAAX0Uz+jB6JQSkuTj08ex7295eSktDSHDwKMRNgB+cjKUps2WrBA\nH32k48eVlqb16+XlpYgI7dxp9DjLaq7mi7V4kRaVVulABXrLu77q36E7lmlZMRUzeh0KpYoV\nFR+fx/FfflFuriryDj6KFm6eAPIxY4Z27tSOHSpX7s8jERH6+mt17Kg+fbRunaHjrKyFWuzT\nvjjF7dVeP/nVUq2yKmv0KBRiHTtq6lT17n3ldbu33lLduqpUyZhVgEG4YgfkY/Zs9er1d9Vd\nZLPp9de1fr1++82YVUWDs5xDFdpFXVqpFVWHfzB4sIoVU/Pmio1Vbq4k/f67evXS559r0iSj\nxwGORtgB+UhIUO3aeRwPDpabm/bvd/ggAHnx9tbKlSpfXvfcI29vBQSofHmtWaOlSxURYfQ4\nwNF4KxbIh5tb3g9KyM5Wdrbc83jWGgBjlC6thQv1xx/atUvnzikkRNWqyYkrFyiKCDsgH3Xr\naulSde9+5fHly+XsrFp8aSlQyJQtq7K8cY+ijn+gAfIxYIDmzlVMzGUHT57U4MF67DH5+Rk0\nCwCAfHHFDsjHvfcqKkrt2+uRR9S4sfz8tH27ZsxQxYp67z2jxwEAkAeu2AH5e/VVLV2q8+c1\nZoz69tW6dXrxRa1Zk/fTUAEAMBpX7IBratZMzZoZPQIAgOvCFTsAAACLIOwAAAAsgrADAACw\nCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAIsg7AAAACyCsAMAALAIwg4A\nAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAAiyDsAAAALIKwAwAAsAjCDgAAwCII\nOwAAAIsg7AAAACyCsAMAALAIwg4AAMAiCDsAAACLIOwAAAAsgrADAACwCMIOAADAIgg7AAAA\niyDsAAAALIKwAwAAsAjCDgAAwCIIOwAAAItwMXoAAAAmcfiw9uyRs7Nq1FBgoNFrgDxwxQ4A\ngH+yd68aNlTFiurYUZGRKltWDzygI0eMngVcibADAOCaEhLUqJH8/bVrl1JSlJqqzZuVmqpG\njXTypNHjgMsQdgAAXNPzz6tOHS1cqBo15OQkZ2eFh+uHH+Tnp5EjjR4HXIawAwAgf2lpWrJE\nL74oZ+fLjnt4aPBgffGFQbOAvBF2AADk748/lJmp4OA8TgUH69QppaU5fBOQL8IOAID8eXpK\n0rlzeZw6d07OzvLwcPAi4BoIOwAA8le2rCpU0OLFeZxavFjh4Ve+RQsYirADACB/NpsGD9br\nr2vLlsuO//ij3n9fzz1n0CwgbzygGACAaxo4UHFxatBAHTsqPFzZ2dqwQd98o5deUqdORo8D\nLkPYAQBwTU5OmjFDDz6ouXM1a5acnVWzplauVKNGRi8DrmTasLNnp505lZxZrGQpP3feTwYA\nFLQ2bdSmjdEjgH9gqiayp/22emZU96YhFQK8Pdy9/cuWCyxRzN2rZPmgRl1f/vinQ+ftRi/E\njcrO1ocfqkMHBQUpIkL9+mn3bqM3AQBgVua5Ynd+++SHI4csPpIlZ+/AqlXrBPuX9PO0n09O\nSkr848Cmz6PXfv72yMgJMXP71Cxm9FRcp3Pn1Lq19u5V165q1UpnzmjZMoWF6YMP1KOH0eMA\nADAfs4Rd5pbobs8tPlaq1cjJo59pVafM5U8Nykrcu2r2qAEvzR3c7a16m0eEuRq0Ejemf3+d\nPq24OAUG/nnkpZc0ZYqeekphYapZ09BxAACYj0neis3eNn9+vMKjvvvqtfZXVp0kV/+gFgNn\nLZvUxmPPvPnbso1YiBt14oRmz9aUKX9X3UV9+6pZM02caNAsAABMzCRhl3Pi2Am7f92Iau75\nv8YpMCLidh07ejzHcbtw87Zskbu7mjbN41SbNoqNdfQeAADMzyRh51y6bBlb4rbYhMz8X5N7\nauuW31SmbGmeAW4K6eny9JRTXn8Fennp/HmHDwIAwPRMEnYuYZ06Byk2qnXH6Ji4U1fVXXZy\nwsopPZv3WZRavXOnMLN8brCIq1xZiYk6fjyPU7t3q3Jlhw8CAMD0zBJBruHDZo3f0XZwzMtt\nY6J8ywdVq1DK39/P05Z+Ninp9NH9e35NzJRruTbjPh0Wzp0T5hAWpjvuUHT0lR+nO3pUM2fq\n7bcNmgUAgImZJewkz9B+i/Y+sHr+jA8+WfDT7riNcenZdrtsNhcPv1Llwzs/07PXU12aVvGy\nGb0T18lm09Spat1aOTl67jlVqaL0dK1cqYEDFRqqxx83eh8AAOZjs9vN+VTf3IyU06dS7N4B\nAb7/5psnsrKyvv3225yca91wsXTp0g8//PDcuXNeXl43/ychT6tWqV8/7dkjT09duCAXFz3x\nhN5+W/xXDQAorDIzM93d3detW1e/fn2jt1zJPFfsruDk7lOqnI+knN9W/G/FiTtaPdyg7E0E\n3rFjx4YNG5adfa1HpKSkpNzsSvyTpk0VF6eDB7V3r/z9FRwsHx+jNwEAYFamDbu/ZG+Z1vvp\ntY//+FCDsm43/p+uUKHCL7/8cu3XTJ8+vXfv3je3Dv/MZlOVKqpSxegdAACYnknCLvf4zz9u\nPZbn+6VZPx/LVcahjd8tvuAiyTmw7v1hZUxysy8AAMAtZJKwy1o7pkOnLy7k/4LvXmn/nSTJ\no9MXyfM7XuM5xgAAABZlkrBzbxU17YlfBs7cleob9li/TsGXfLI+e9eckXMOhj35Uoc7nCW5\nVK9pkt8JRYRd9oM6GK/4AAUEKchL3BcCACgoZokgr5AeH62/u9mQR/t/9NWimk0+m9izlvfF\nJ5tkfLl11NxTdboMfbH5TXzGDihQy7W8v/rHK95TnulKd5NbH/WJVnQxFTN6GgDAgsz0YbTi\nQY9NXxf7WTe3r5+pf/dj035ONueDWlBkfKfvWqlVC7U4oANpSktRyjzNW6iFHdQhV7lGrwMA\nWJCZwk6Sit3Z5f21m+c96fN9/4Z3dZ64KSmXukOhlKOcPuozWIMnamIVVZHkJa/2ar9Kq9Zp\n3TzNM3ogAMCCzBZ2kuRxR8cJqzd/0Sdg1ZCm4Q++s+40bYfCJEEJIzSiuZof1uFsZe/UzkvP\nVlblR/Xo5/rcqHkAAAszY9hJknuVDuNWbflqwH/Wj5iwMsPoNcD/G6/xwQqeozm5yvWSV6xi\n66jOG3rj0teEKOSgDhq1EABgYWa5eSIvbhXbjl2+tfVnX25L9qxV1cy/CawgWcmd1XmpljrJ\n6Xf9vl/7neX8rt49ruOd1KmKqjyqRy++Mk1p3DwBACgIZr1i9//cyjd7YvCQIb3uq2D23wSm\ndkEX7tN967SujdqkKS1NaSu0Ikc5jdW4tEq/oBdGadRfL16iJfVUz8C1AACrIoeAW2CyJv+h\nPzKV+aye9ZCHTbZmatZRHYupWC/16qiO8Yo/qZOS3tW7m7Spn/oZPRkAYEG8gQncvAxlzNGc\nDdrwuT6voirHdMxHPn+d/UAfNFbj7doerWhJYzX2Z/28XutnamY1VTNuNQDAsrhiB9ykQzpU\nV3Wf1/NpSstWtpe8bLI9rIeP6ujFF9ym27Zoi7Oc4xQnabVWBylom7Z1VVdDhwMALIsrdsDN\nyFFOO7UrozJrtdZPfhVU4Sk9FajAH/Xjg3pwgzY4yUlSrnJzlVtKpQIVuFRLjV4NALA4rtgB\nNyxb2VM1db/2z9VcP/lJqqd6S7RkjMbYZNuqrX89pu4LfeEkp03aNFZjDZ0MACgSCDvgBhzU\nwQ7q4CWvARpwXudDFTpGY7KV/ayeXaAFG7VxpVZ6yONRPXqH7qiiKj3Uw0te3+v7Oqpj9HYA\ngPXxVixwvfZqbyM1ClXo1/r6f/rfeZ1vqZZRiopV7Jf6crzG91CPlmpZRVVc5eojn43aeLfu\nXqZl3vI2ejsAoEjgih1wvXqpV4QilmppK7Wqqqqndbqv+q7Rmh/14xzNGaiBG7UxUIEHdOCI\njpRQiWmatkEbqDoAgMMQdsB1+VW/rtGaN/Xmxbsi2qndRm2MVWx1VX9ST36qTyXVVd3u6n5B\nF1ZoxQIt6KEeTvxPDADgQPy/DnAtsYp9XI+HKayBGjjLeaM25ihH/99w7dTuO30XrvB4xdtl\n/0bfPKSHeqlXDdUwejgAoCgi7IB8vaf36qv+WZ3tru4Xv+n1Bb1wn+47r/OSpmv6w3q4ndr1\nUq8TOuEnv07q9Lgen6iJRg8HABRRhB2Qt7VaO1RDZ2v2V/rqWT07VEPtsn+kjw7r8FANleQm\nt/f03iEdqqd61VRtlmYd0qGxGuvCPUkAAIMQdkDeJmjCQ3roYT188d+WUZkO6hCt6GhFf6yP\nU5Ry8fge7Vmrte/onUhFllEZ4/YCAEDYAfnYrM2t1frSI+/r/VSlDtfwTGV+rI+/1Jf91b+1\nWj+n5+7X/UbtBADgL4QdkLd0pXvK89IjpVU6VrFd1EXS83q+t3rHK/5LfTlGYwzaCADAZQg7\nIG9VVCVOcVcc9JZ3d3W3y75He07r9DIti1SkIfMAALgaYQfkrYu6TNf0EzpxxfE39Ea4wquq\nqiGrAAC4BsIOyFtv9a6iKo3VeImWpCo1Rzm7tftRPfqVvpqqqUavAwAgD4QdkDd3uf+gH5qq\naXu195FPcRWvoRr7tX+1Vocr3Oh1AADkgQduAfnykc90TR+v8Xu056zOBiu4rMoaPQoAgHwR\ndsA/KK7id+kuo1cAAPDPeCsWRUuc4lqqZXmVL6ESNVVzpEbmKvfmf1xqqiZOVJcuiohQ166a\nMkXnz9+6sQAA3BjCDkXIm3qzlmqt0ipf+dZUzSQljdCI8ir/19dI3Jh9+xQaqrfekp+f2rWT\nj4/eeEN16ujgwVs9HACA68JbsSgq1mv9cA2vp3prtfavr3NdoiWRimykRju048Z+XFaW2rVT\ncLDmzVPx4n8efOcddeyoDh20daucnW/pfAAA/hlX7FBUDNCA4iq+XutdLvnnmTZq84pe2amd\n+7X/xn7c11/rjz/06ad/V50kb2999pn279eSJbdoNQAAN4CwQ1Hxi35pqIZOV/01P1zDJc3W\n7Bv7cevWqUkT3XbblccDAlS/vtavv/mhAADcLMIOFpeudLvskrKUFaCAq1/gJjcnOSUq8cZ+\nbmqq/PzyPuXnp3PnbngoAAD/GmEHazqlU/3U73bdXlzFfeXbSI3c5LZP+65+5REdyVVukIJu\n7A8oX1778vhpkrRvnypUuPHJAAD8W4QdLOigDoYpbK3WvqSX1mndXM0NV/h5nd+szQd04IoX\nP6EnnOT0uB6/sT+jQwdt3qw1a648vnSpdu9WZOTNrwcA4GYRdnCgnBydOeOAP+cpPXWn7oxV\n7NN6OkIRD+iBd/XuQi20yx6ikC/0xcWXJSmpjdos07IBGuApzxv7M2rVUu/e6tBB8+crO1uS\nsrI0e7Y6d9azz6p69Vv9OxUlqal67TWFhalYMZUurZYtFRNj9CYAMIf/a+8+A6K41jAAv7PL\nUpQmTYooKhZEiojEgoIFFVs0MVbsXUzsGns3GmOPxm5sMfYYjbFr7CV2xa5YEAUFpUjdnfsD\nEwHRm0TY2Z19n1/3njPMfPPdBd87O3OGy52QVuzYgWnTcP48UlNhZ4eQEEyejFKlCuJQt3H7\nIA5ewRUTmGQf/xSfNkTDwzjcCq0UUBjBKB3pCijCET4Hc/7LkebNg40NOndGhw4oVgyPHkGl\nwtdfY9So/DkTwxQbi+BgvH6Nvn3h7Y1Xr3DoED77DEOGYOpUqYsjItJ1DHZU8L75BmPHom9f\njBsHR0dcv45Fi+Dnh4MH4eeX70e7gis2sKmIiu9ONUbjx3g8D/N+w29P8TQAAWEIs4HNfzyS\nUolJkzBoEC5cwIMHcHODnx+srD6qegoPh6kpTp6EpeWbkVat8PnnCA1FcDDq15e0OCIiXcdg\nRwXswgWMHo0tW9C8+ZsRb2+0aoWwMHTogMuX830hXw00765pkkUBhQaa2qhdG7Xz7XhFiqBO\nnXzbm4F79gxbtuDQobepLku9emjbFj/8wGBHRPRhvMeOCtiKFahd+22qyyIImD0bt24VxHpv\nHvB4jufvPiQB4DRO/+unXz9eSgqOHMHSpdi6FZGR2j66frlyBUolAgPzmKpdGxcvar0gIiI9\nw2BHBezaNdSokce4gwPc3XHtWr4f0BOeAQgYgiEaaLKPn8XZ9VjfBV3y/YgfsnYtihdH3br4\n7jt0745SpfDFF3jxL9fMMxyZmVAqocjr75JK9eYhFSIiej8GOyp4gvDvxj/aCqw4giN1UXc7\ntt/F3bM4OxVT66BOJ3RqjMYFdNA8rFuHLl0wdChevcLNm4iLw5kzuHULoaHIyNBeGXqkXDmk\npuYd98+dQ7lyWi+IiEjPMNhRAatQIe/vW2Njcfs2KlQoiGN6wvMcztnDvgM6uMP9E3yyGqu/\nw3dLsKQgDpe3tDQMHIjJkzFsGAr9tZaKvz/278e9e1ixQnuV6JGSJVGzJkaMgCbH1Vbcvo1l\ny9Cxo0RlERHpDQY7KmBdu+LAAezYkWNQFDF4MNzd8/6WNj+4wW0jNr7Cq0d4lICEG7jRC70E\nFNQ1wjwcP45Xr9C3b+5xe3u0b49fftFeJfpl0SIcP44GDbB3L54+xa1bWLQIgYGoUwdhYVIX\nR0Sk6/hULBUwPz+MH4/PP8dXX6FRIzg5ISICP/yAs2dx4EC+PxKbiwChGIoV6CHe6/FjODrC\nwiKPqTJlcPCg1gvSExUq4MwZDBqEpk2Rng4ADg748ksMH573vXdERJQNVWSG/gAAIABJREFU\ngx0VvDFjULEipk3D/PlIT0eRIqhXD+fOwd1d6soKkqUlXr2CRpNHHImPz72cB2VXujS2b0dG\nBu7cgZUVnJ2lLoiISG8w2JFWtGiBFi2QmYm4ODg4SF2NVlSrhqQkHDiAkJAc46KIX35BvXoS\nlaU/VCp4aH1tGiIiPcdgR1pkZCTzVJecjG3bcOkSXr2CpydatkTPnti37+21SbUaw4fj1i1s\n3y5poUREJE8MdkT55NgxtG6NjAwEBMDSEkuW4NYtlCoFLy80aYKKFRETg4MH8ewZtm1DMYnu\n/CMiIlljsCPKDw8eoHFjtGuH2bNhavpmcNs2tG+Pzp0B4PBh2NmhY0d07YqiRaUrlIiI5IzB\njig/TJ8OT08sXJhj1eUWLTB1Kr75BlFRMOLvGhERFTguH0CUH/bvR/v2ebxLIywMMTG4fFmK\nmoiIyOAw2BHlhxcv4OSUx7idHYyN8fy51gsiIiJDxGBHlB8cHPD4cR7jz54hPZ031RERkXYw\n2BHlh9BQ/Pgj1Orc48uXw8UFXl5S1ERERAaHwY6kl4KUS7j0AA+kLuQjDB2KR48QFoaXL3Hy\nJHr1QvXqKFkSY8agQwe+C4uIiLSD/96QlK7hWj3UM4e5L3zd4GYHu8mYnIlMqev695ycsG8f\nzp2DgwNq1MC2bbh2DVFR8PHBzJno2xeiKHWJREQkfwx2JJlzOFcVVc1hfhiH4xF/F3e/wTfz\nMK81WovQwxjk64tJkwCgUyd07Yrvv8ejRzh/Hn/8gbVrsXCh1PUREZH8cW0tkkx3dG+GZmux\nVoAAwBrWpVCqJmpWRuWN2NgaraUu8N+bPRv9+mHWrByD1aph9Gh89x3CwyUqi4iIDAWv2JE0\nruDKJVyagilZqe5v5VG+IzquwRqpCvvvMjNx9iyaNs1jqkkTREYiOlrrNRERkWFhsCNp3MZt\nG9i4we3dKT/43cZtrVf00VJSoNHA0jKPqazB5GQtV0RERIaGwY6koYIqHel5TqUhTQWVluvJ\nBxYWsLXFjRt5TN24AWNjODtrvSYiIjIsvMeOtCEWsUux9DzOP8GT8ihfF3Wro3oSks7gTAAC\ncm18EAcroZIkdX6sli0xaxZatoSJydtBjQbffovQUBQqJF1lRERkEHjFjgrcURytgAqrsdoZ\nzo3ROBOZfdCnIzqGIrQf+iUiMfvG27BtO7b3RV+pqv0o48YhJgaNG+PSpTfrm9y8iS++wLlz\n+PZbqYsjIiL54xU7KlixiP0Un7ZF27mYa/TX5y0KUQ3QwBrWiUj0hndP9PSCVxziDuDAOqyb\niqnVUE3asv8jJyccPYoePeDrC3NzKJV49QqffIIjR1C2rNTFERGR/DHYUcFajMUOcMie6gC4\nwGUVVvnD/wIu/IJftmHbFEyxg50PfPZhX23UlrDgj+Xmhn378OgRrl5FRgYqVkSpUlLXRERE\nhoLBjgrWcRxvgiZG73zSKqOyK1wv4uJ4jB+P8VKUVpBcXeHqKnURRERkcHiPHRWsJCQVQZE8\np4qgSAIStFwPERGRjDHY0b8UH4/Mf/EuV1e43sKtd8fTkX4f94ujeP5VRkREZOgY7OifefQI\nnTqhaFHY2MDcHFWrYtOmXJs8xuO5mNsTPXuj9wIsiEEMgBZosQVbIhGZa+OlWKqAoi7qaqd8\nIiIiQ8BgR/9ARAT8/HDnDubMwaVL2LkTtWohLAwjR/69yRIsKYMyC7EwCUkv8XImZpZG6Z/x\nc0u0rIZq9VDvEA6JEAG8xuvZmD0Ig77FtxawkO6siIiI5IYPT9A/0KkTatTAli1QKt+M1KuH\nkBA0bIhGjRAY+Bt+C0f4Qizsju5Z737VQDMLszqggytct2Fbf/QPQYgpTB3g8BAPrWG9EAu7\noZuUJ0VERCQ7gpi1jCq93+LFi3v37p2YmGhubi51LVK4eBF+frh3D25uuadatIC1NVaurIzK\ntVBrNmbnmu+IjtGI3od9AKIRfREXn+JpWZSthEqFwNcwEBGRXkpPTzcxMTl+/Hj16tWlriU3\nXrGj/+fqVbi45JHqANSogQ0b4hF/HucXYdG78+3RvimaqqFWQukEJyc4FXSxREREhozBTl4e\nPsSePYiIgLU1fHzQuDFUqoI+ZhziADjC8d0pRzhmICMBCe9b8YSIiIjykd4GOzEzOT72ZbqZ\nnYO1CZ8AyTJtGsaOhYsLvL2RkIDvvoOjIzZvho/PR+3W0xNRUXj4EMXfWZrk5El4ejrAQQHF\nQzx0Re4leR/ggRnMrGD1UQXkr6QkbNyICxcQF4fy5REaCn9/qWsiIiLKH3qVicTkyCM/jusY\n7Fnc3sLUxMLWuZiTjZmJuZ2rR822o1b88eC1Id8u+MMPmDABa9fi/n1s345Dh/D4Mfz9ERKC\nmJiP2rOvLypVwsCB0GhyjB88iO3b0bWrBSwCEbgUS9/90WVY1gANFLrzMTt9GuXLY8QIREfD\n3By7dyMgAL17Q62WujIiIqJ8oD9X7F5f/L51s0G/PcqA0sKpTJlKFWztrAuJr1/Gxb14cvf0\nhqnHNsyY0GzuzvV9vMykLlX7MjIwZgymT0erVm8HraywZg38/DBzJqZP/+87FwSsWoWgIAQH\no1+/eJ/iFtFJRnsPYtYsDB6MWrUAfINvghHsCtdRGGUKUwDJSB6BEQdw4DROf+zZ5Zdnz9Co\nEZo3x4IFMDV9M3jiBJo1g50dJk+WtDgiIqL8IOqHtLOjKhoLRi6hE7adj07JPZv+PGLv3Hbl\nzAQTn3Hn0vP72IsWLQKQmJiY3zvOPydOiIIgvnyZx9T06aKv78cf4eHjkx2PlHSIVUCESSo+\nuVJ447H+2TfYIe6wF+2tRKuaYs1qYjVz0dxFdDkkHvr4Q+ebESPEihXFzMzc4xs2iKam4qtX\nUtRERET6Jy0tDcDx48elLiQPenLFLvPCxo034D/u922jvUzenVbZeoR8tWZ/4VcV+/y88cJo\nvwA9Oa18ExsLCwtY5XUrW7FiH/tVLBCBiCCXpmVdys7BFM945xiLlL0VD4Zh7gUUmoqpWds0\nQZP7uL8XeyMQoYBiBEZkLVz3kYfOTwcPonXrt0vx/a15cwA4eRINGmi/KCIionykJwlI/Sz6\nmWhbt1q5PFLdXxRO1aqVxtGop2q9Oa18Y2+PpCQkJMDSMvdUVBTs7T9y953QqQZqbMEWJZRZ\nj7fWQ8MQhDREw0ZoFIjArM0Ko3ALtGiBFh95uIISHw8HhzzGjY1hY4O4OK0XRERElM905q72\nD1MWdXYUXlw4cyf9/dtoYs/9GQlH56LvXJGRv8qVYW2NNWtyj6vVWLcOISEfs++LuHgO5+Zg\njhI5OhuCkGZothzLP2bnBS4uDseP48YNZGbC0REPHuSxTXIyYmPhmMdyLURERPpFT4Kdkd8X\nrTxwZlxoy6k7r8a+k+4yX945tLBL3T7bk8q3+sLP0C7XATA2xoQJGDoUW7e+HUxMRKdOiIrC\nkCEfs++ruOoCFze4vTtVAzWu4urH7LwAnT2LTz6BrS0CA+HhgSJFYGSENWuQnJx7yxUrYG4O\n3Vs9nIiI6N/SlxCk8h+xZtalpgN3jmq6c5yVq0e54g62ttaFhJRXcXHPo25H3HuRDlWxRjNX\nj/Av8AV5dVO/foiPR+vWKFkS3t54+RJnz8LWFnv2oGhRqYvTumPHEBKCli2xaBE8PREfj337\nMHQoEhLQrBl+/BGurgCg0WD1agwdijlzYPKBr/mJiIj0g169K1ZMjjyycdmSlVv+uPbwWXxK\npihCEIxMrR1cPWp+3qVX9zbBpcyF/D+sPr0r9v79N2+esLREpUpo0uTj88oFXKiMypGILI7c\nCxR/js8tYPEjfvzIQ+QzUYSnJwIDsWRJjvHbt+HrC2dnREaiXDnY2uLaNbx+jalTMWCARLUS\nEZH+0eV3xepVsMtOk5bwPDZBtLC3t/qYN0/ExMR07949JSXlA9tERUVdv349ISHBwsLivx9J\nb4kQ/eHvBrdN2JR9qeGDOFgf9Q/iYC3UkrC8PJw7hypV8PgxnJ1zT/Xpg8ePMWIELl5EbCwq\nVEBQUN5PVBAREb2HLgc7ffkq9m+alJh7918Vdi3pZOlQLOcjoJrXL57Gp5naOtv88zU2zMzM\nfHx8MjIyPrCNUqm8fv26IBTAxUB9IEBYhVVBCApGcD/0q4iKz/BsH/bNwqzBGKxzqQ7A3buw\nt88j1QHw8cHhw6henXfUERGRLOlRsNPEn130VffRG67EZ4iCsVO1LpMXftfV5+2Xo2Lsunal\neh5ptunlxpb/+OtHCwuLSZMmfXibxYsX79mz5z/XLQMVUfE8zo/EyL7o+wIvVFB5wWslVrZF\nW6lLy4uJCVJT855KTeW9dEREJGN6E+zSLk5rVGf06VQbj7otK9kmXNx3cEmPercTDu4akNeK\nxZTfSqDEOqwDEItYa1iroMPPqFSujMREnDqFqlVzT+3di8qVpaiJiIhIG/RkuRMx/tdpM8+k\nle2x9fKlfZvW/rzn8o29w/zTDo/uOfvKB5a2o/xnD/u8U50oYv16NGmCkiXh7o7mzbF9u9ar\nAwAUK4YWLdC3L+Ljc4yvXo29e9GvnzRVERERFTw9CXYZFw4fe1UodOSUJs5Z1xgV9rUnr5kc\npDjz7aifojQSV0dQq9GmDXr0QIkSGDsWX38Ne3u0bo1evSDJ0zmLF0OthpcXxozBhg344Qd8\n9hm6dcO8eahUSYJ6iIiItEJPvooVE14mwLZkSatsDzAYle05rd/SwG8nTz/Scl6wzi9EIm8z\nZ+LAAZw+DU/PNyPdu6NHD9Sti4AAdOum7Xrs7HDqFObNw+7dWLoUNjaoVAnHjyMgQNuVEBER\naZGeXLFTODja49nFC49zXJwzrTJkWgeXyKUDp5/90HIlVMBEEfPnY8yYt6kuS0AABg3C3LnS\nVGVmhuHDcegQnj5FRATWrWOqIyIi2dOTYKfyaVDPKePohE4jf72T7Y1Qgk2jaXPa2V35tm3P\ndXeY7aTy9CkeP0aDBnlM1a+Pq1eRlqb1moiIiAyRngQ7FK43Zmb7UinHpjcvZ29fpsm8a+qs\nccG++ew1wyvH/dTBt3yDmRc+tBwdFZSs3GZmlseUmRlEkcGOiIhIO/Ql2EFRrNWqixc2jetQ\n37eoOiY2+e878gXb4Mm7D64YEFz4wZ3nMnqMQq3Wmzzk5IRChXD5ch5Tly/D3h6WlnlMERER\nUX7Tm2AHAIXLfT5+1e8nrt47Mykg+1MfgqVvp1k7I6Kf3btyavfIGjq8wtr/J4pYtgxVqsDC\nAubmKF8eY8YgOfn//6CETEzw2WeYMgXpOZeeSU7GjBloq5OLGBMREcmRXgW7DxNMbd0qfhLk\n66S/56TRoEMHDBqEhg3xyy84cAB9+mDtWlSvnntJNl0zbRoeP0ZICI4exevXSEzE/v0IDkZG\nBsaNk7o4IiIiQ6G/IUiOVq/G9u04dgyTJqF+fdSqhf79ceECMjMxdKjUxX2QiwtOnoS1NYKC\nYGEBKys0aAB3dxw7BhsbqYsjIiIyFHqyjp2BWLwYffvC2zvHoLU1pk1Dq1aYOxeFC0tU2T/g\n6ort25GYiIgIKJXw8NDpaomIiOSIV+x0yZUrqFUrj/GaNZGaitu3tV7Qv2dhgU8+gb8/Ux0R\nEZH28YqdLtFooFTmMZ41qFb/3x1EIGIRFl3CJStYBSGoP/ob8X9iIiIig8ErdrrEwwOnT+cx\nfvo0VCq4u3/4pwMR6AnP+Zh/Aid+x+9DMMQCFruwq0BKJSIiIt3DYKdLOnfG/PmIjMwxmJKC\nUaPw2Wewsnrfz2UiszRKH8fxkij5Db4ZgRGBCFRBZQzjZmh2F3cLunAiIiLSBQx2uqR3b/j7\no2pVLFyIS5dw5w5+/hnVqyMmBrNnv++HnuBJKELv4V5RFO2N3q3QaiImHsKhxVicjGQFFJ3Q\nSZsnQURERFJhsNMlKhV27EC/fpg2Db6+KFMGffrA3x9nz8LJKc+fWIu1ZVDmAA4AqIVaa7Cm\nPMrPwzwAXdClJVoWQZELuKDVsyAiIiKJMNjpGJUKo0fj4UPExSEqCvHxWLoUdnZ5bnsUR7ug\nS1/0FSEKEDZi4xVcWYEVQzBkG7YBaIZmSUhKg568moyIiIg+DoOdripSBM7OH95kEia1Q7tP\n8akAQYQYgxgAYQgbiIHjMA6AFazSkKaCXr9kjYiIiP4pBjv98xzP92Hfciw/hEPN0bw4imdd\nsRuKN2+naIM2V3DlOZ5HIEKEWB7lpS2YiIiItIOLnOmTdKR/ja8XYIECCic4ZSIzDGETMKEK\nqkQjeg3WVECF4RheFEUBRCBiNEZroFmCJVIXTkRERNrAK3b6pCu6/oyfN2NzEpLu4q45zLug\nywRM8IXvczy3h/3X+LoQCgUgAEAQgtKRPgdzqqCK1IUTERGRNjDY6Y2jOLoBG3ZhV1M0VUIp\nQGiERndwZyVWrsKqdVhnC1sAaqijEAWgOIpfwqX+6C914URERKQlDHZ6Yyu21kEdX/j+PTIB\nE07gxB7scYRjDGKu4uqf+DMUoUYw2o7tD/DAG94SFkxERERaxnvs9MYjPCqLstlHyqP8buwO\nQ9hjPJ6ACSuwIgIRFrDYgR0N0VCqOomIiEgqDHZ6wwIWL/Ey12B1VL+Jm17wcod7LdSqiIq1\nUdsMZpJUSERERNJisNMbgQgciZHJSC6MwtnHn+DJXdxdiqU1UVOq2oiIiEgX8B47nZOO9N/w\n2zRMG4MxG7AhHvFZ423R1hSm3dAtFal/bxyHuPZoXxVVAxEoUb1ERESkK3jFTrecwZk2aBOD\nGC94FUbhRViUhrQFWNABHQqh0A7saIIm5VG+ERq5wvUO7mzHdhe47MZuAYLUtRMREZHEeMVO\nh0QisgEaBCHoCZ6cxMn92B+N6ImY2BVdd2AHAG94X8GVr/DVC7zYgR3pSJ+GaWdwxglOiItD\nbKzUZ6A/kpPx+LHURRAREeUzBjudkIrUNVjTGI2VUDrA4RROiRABGMFoAAYMxMBhGJa1pRWs\nBmHQBmw4gRNrsKZ7apjJmMlwcYGtLRwc4OCAAQOQkCDp2egwUcSSJfDwgIUFXF1haYlWrRAZ\nKXVZRERE+YPBTno3cMMHPgMw4D7uV0Kli7jYFE2bomkykrM26I7uN3AjEpG5fzIlBfXqYdUq\njB2LS5dw7RpmzMDvv6N6dcTHa/ks9EPv3hg8GGFhOHUKt29j9WrExKByZVy7JnVlRERE+YDB\nTmKv8boRGpVH+UhEZiBjOIbvwZ4ruHITN3ugR9Y2xVEcwFM8zf3DM2YgMhJnzqBXL3h7o0IF\ndOqEM2eg0WD0aC2fiB7YtQsrV+LAAYwahYAAuLujeXMcPIhatdC1q9TFERER5QMGOy3KzMTR\no1i0CIsW4ehRZGYCWIVVKUj5CT9ZwMIWtlnprSzK/oSffsbPN3ETQDSiAdjBLvcOV67EsGFw\ndMwxaGWFceOwdi0yMrRyVvpj5Uq0aYOAgByDCgW++w5nzvCiHRERyQCDnbacPAkPD9Spg3nz\nMG8e6tSBhwdOnjyMw03RNGtpunqotxZrszavgiolUfIwDgNYh3VucCuN0jl2mJKCyEh88kke\nx6paFQkJfDggt+vXUbVqHuOlS8PeHteva70gIiKifMZgpxXXrqF+fQQH49kzREQgIgLPniEo\nCPXrxyc/tod91lYjMfIIjgzH8AxkAHCAQzzif8bPkzBpEiblXtBEEABAo8njcGo1ACj4P25O\nCsWbzrxLrWa7iIhIBviPmVaMHIngYCxZAhubNyM2Nli6FEFBTmce/f1URAVU2IZty7G8BEp8\nhs8u4dJ8zO+IjlMwJQxhufdpaoqyZXH0aB6HO3YMNjZwcSmw89FPXl55t+vaNcTFwctL6wUR\nERHlMwa7gpeejj17EB6eJCRvxubxGD8e4zdjc7LwGuHhTZY9/RW/RiEqa9sGaHAXdydhUjrS\n05DWC71u4uYQDMl7zz17YsYM3Lv35r+KIn75BWFh6NkTVlYYOxb372vlDPVEjx7YuhX79+cY\nTE9H//4IDkaZMhKVRURElG8Y7Are8+dIS9vl9agUSvVAjyM4cgRHeqBHSZTc5f34858zfNMr\nNETDK7iStbklLIugyDEcG4mRYzG2JEq+d89ffYWAAAQEYOpU7N2LoCB88QW2bkXRomjeHPv3\nw9MTGzdq6TR1X3Awhg1D48YYOBC7duHUKSxbhoAA3LiBFSukLo6IiCgf8JViBc/K6swnQgvn\n8CEYOhZjTWACIA1pEzChhXP4MX/hV/W2rgj3gY8b3FzgchM3X+HVMAybiIn/Z88qFX79FfPn\nY+VKjB0LtRqlS6NzZwwdChMTAPjuO4SFwdMTnp4Ff576YOpU+Ptj1iwsW4bkZLi5oWFDTJgA\ne3upKyMiIsoHgiiKUteg6xYvXty7d+/ExERzc/P/tof6p63sVE4/+d3INd7ufPnnGdF7P3kF\n4BquncGZJ3hSFmVroIYznP/FARITYW+P1avRqtU7x64PV1csX/7fKpctUURqKszMpK6DiIj0\nT3p6uomJyfHjx6tXry51Lbnxil2BS0XqoYDk3Q3voNNPaNfu7cS6dV3X3An9HWlIM4GJJzw9\n8V+vq/35J9RqNG+OtDTcuAFzc5Qs+eYxz08/xfff58NpyIwgMNUREZH88B67AheHuExB7dr+\na3TujCpVEB6O8HBUqYIuXYq3+zpTUL/Ai489RlISTEzQsiXMzeHrC3d3WFtj6FCkpMDaGomJ\n+XEeREREpOsY7ApcERRRQPG0Y31cvYomTRAbi9hYNGmCq1ejO4YooCiCIvlwmORkxMVhzx7E\nxeHhQyxahE2b0LAhIiJQvHg+7J+IiIh0Hr+KLXBmMKuBGmuwplbZpRg3LvvUWswIRKAZPvo7\nwQULUKgQAgJQpw4AFCmCdu0QFARfX1y6xPfGEhERGQgGO22YiIkhCKmACv3RXwEFAA00czH3\nR/y4D/s+du/PnmHfPsyahSFDoFJh2DDY2gJAVBRUKiQnIzz8o8+AiIiI9ACDnTYEI3gVVvVE\nz7mYWwVVAJzF2Rd4sQqrghH8sXu/exeiiD59ULYs+vTBt9/CxQUJCUhKQtWquHqVTwkQEREZ\nCAY7LWmHdvVQbxu2ZS1EPAIjWqCFAxzyYdcqFUQRGRkIDcXt27hyBRERsLaGtzfOnEHv3vlw\nCCIiItIHDHba4wCHXuiV//v18ICpKfbvx6efQqWCnx/8/N5MffMNKlXK/yMSERGRTuJTsfrP\n3BydOmHYMMTE5Bj/4w+sWMEb7IiIiAwHr9jJwowZqF8fPj7o1Qu+vnj9GkeOYMUKfPUVmjeX\nujgiIiLSEgY7WbCwwB9/YP58bN2KOXNgbg5vb2zZgqZNpa6MiIiItIfBTi6MjTF4MAYPlroO\nIiIikgzvsSMiIiKSCQY7IiIiIplgsNMNN2+iZ0/4+cHRETVrYuxYvHwpdU1ERESkZxjsdMCv\nv6JSJdy5gw4dMHcu6tfH+vXw9cW9e1JXRkRERPqED09I7ckTtG+P4cMxbtzbwSFD0KIF2rTB\nqVNQMHwTERHRP8LQILXly1G8OMaMyTFoZoZly3D+PE6elKgsIiIi0j8MdlI7dw4hIXlclitW\nDBUq4Nw5KWoiIiIivcRgJ7XUVJiZ5T1lZobUVO1WQ0RERHqMwU5q7u64fDmP8bQ03LgBd3et\nF0RERET6isFOam3bYs8enDiRe3zmTKhUqF9fipqIiIhILzHYSa1GDfTogUaNsGgRoqKgVuPm\nTQwahLFjsXAhzM2lro+IiIj0Bpc70QELFqB0aYwejT59oFBAo0H58tixA6GhUldGRERE+oTB\nTgcoFBgyBAMH4v59REWhTBk4O0tdExEREekfBjudoVTC3Z1PSxAREdF/xnvsiIiIiGSCwY6I\niIhIJhjsiIiIiGSCwY6IiIhIJvjwhB6Kjsa5c3j4EO7u8PeHjY3UBREREZFOYLDTK6mpGDwY\nS5bAzAyurrh3DwoFRo7EyJEQBKmLIyIiIokx2OmVjh1x8iR+++3Nq8bUaqxfj/BwpKZi0iSp\niyMiIiKJMdjpj8OH8csvOHcOXl5vRpRKhIXB0hItW6J7d5QoIWl9REREJDE+PKE/fvkFdeu+\nTXV/a9YMxYph1y4paiIiIiIdwmCnPx4/fu97KcqUwaNH2q2GiIiIdA6Dnf6wtER8fN5TcXGw\nstJuNURERKRzGOz0R82a2LMHSUm5x+/fx4ULCAyUoiYiIiLSIQx2+qNNG1hYoGNHvH79djA2\nFm3bomZN1KghXWVERESkE/hUrP4wM8POnWjcGGXKIDQUrq64fRs7d8LdHRs2SF0cERERSY9X\n7PRKhQq4cgUjRyI9HYcOwcwM8+fj5Ek4OEhdGREREUmPV+z0jbk5wsMRHi51HURERKRz9PaK\nnZiZHBcd9fRlmkbqSoiIiIh0g14FOzE58siP4zoGexa3tzA1sbB1LuZkY2ZibufqUbPtqBV/\nPHgtSl0hERERkXT056vY1xe/b91s0G+PMqC0cCpTplIFWzvrQuLrl3FxL57cPb1h6rENMyY0\nm7tzfR8vM6lLJSIiIpKCvgS79D+ndhj8W7RDwwnfT+nZsJKjaY7ZjBfXD6+b/OXX6wd2mP7J\n2fF+KomqJCIiIpKQngS7zAsbN96A/7jft432Mnl3WmXrEfLVmv2FX1Xs8/PGC6P9Av7NaSUn\nJ6enp39gg9fZ140jIiIi0lV6EuzUz6KfibZ1q5XLI9X9ReFUrVppHI16qv4Xp3X37t2yZctq\nNP//EQxBEP7pTomIiIikoCfBTlnU2VF4ceHMnfS6FYzfs40m9tyfkXCsV1T5L3ZcunTpCxcu\nZGRkfGCby5cvd+3aVaXiF7xERESk0/Qk2Bn5fdHKY86UcaEtxQVTe9SvaJ8z3WW+vHP0p0n9\nhm1PKj/qC79/eU7e3t4f3iAtLe1flktEREQkAT0JdlD5j1gz61KpyDPbAAARFklEQVTTgTtH\nNd05zsrVo1xxB1tb60JCyqu4uOdRtyPuvUiHqlijmatH+PPCGhERERkmfQl2QCGf8O3XGx/Z\nuGzJyi1/XLt66mpKpihCEIxMrR1c/Vv17NKre5vgUua8EY6IiIgMlf4EOwBCYbegLpODukwG\noElLeB6bIFrY21uZ6NUqy0REREQFRK+CXXYKE0uHYpZSV0FERESkO3ixi4iIiEgmGOyIiIiI\nZILBjoiIiEgmGOyIiIiIZEJvH57QImNjYwAmJh94nRkREREZlqx4oGsEURSlrkEPXLp0KTMz\nU+oq/o8RI0ZkZGR069ZN6kJ0Tu/evbt37+7v7y91Ibrl9u3bEydO/PHHH5XKf/MaPgOwadOm\nyMjIoUOHSl2Izhk5cmTt2rVDQkKkLkS3vHjxYsCAAbNmzbK3t5e6Ft2yf//+Q4cObd26VepC\nCoSRkZGPj4/UVeSBwU4+OnToYGpqunTpUqkL0Tk2NjbLly9v0aKF1IXoluPHjwcGBqanp/M9\nyLmMHDny/Pnzu3fvlroQnePt7d2jR48vv/xS6kJ0y8OHD0uUKHH37t1SpUpJXYtuWbhw4YIF\nC65duyZ1IYaF99gRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFM\nMNgRERERyQSDHREREZFMMNgRERERyQSDnXwYGxvr5nvrJMfO5MnY2NjIyEih4B+B3PiBeR92\nJk9ZPWFn3sUPjCT4SjH5iIuLUygU1tbWUheicx48eFCsWDG+EfVd9+7d40uQ3pWUlPT69WsH\nBwepC9E5UVFRdnZ2JiYmUheic/irlKf09PSYmJhixYpJXYhhYbAjIiIikgl+C0NEREQkEwx2\nRERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdE\nREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2+kvz\n+Id65o4996a/M5N6d/v41tVK25ubmduXrtZ63LY7KRLUp33iq8trhn4W6F3S3sLSsWyVkM5T\nd93PeeYG2ZmUezsntq/t517U0sLOzavG58PXXozX5NjCINuSk/rBqhaORjadd6blGDbMzmSc\nGOxuJOSmqjjmQubf2xhmZwDg9a2t49vXLOdoVdjK2aNG2wm/3nmdY97AOqN59H1tk3c+LG+Y\nhi5/LmZtZ2BtkZhIeirxxHBvY0XRHnvSco5n3FnezFEhqGw96rTq0Kp2eRuVoCjaZOntDGnK\n1BpN9NYupY0FwdS1aovOvXu0rV/BRikI5pWGHHihydrCIDvz+tyU6paCoHL0a9qhV+8uzas4\nmwiCcbm+u//qimG2JZe0a7OCrRUQinTakfp21GA7E7+mmZlg4uwTFJxd3S7Lb2VmbWCwnRGT\n/vymprVCMHOu1KBd105NKzmoBIXjpyvuvWmMAXZGE7N1QEjwO4L83cwFhWPnHQmiaIhtkRaD\nnb7RvIjYt3Hp9P7NKlgrBbwT7DTPN7V1UCicmi29lS6Koiim3VjUuKhC4dBm03NNnjuUibST\nQ8saCSaVhh+L/yvHPfm1Z1mVoKow8my6aKCd0cSsaW4lKEt22xGjfjMSt69fOZVgUm1G1j/S\nBtmWXJJOjalcSBAEIUewM9zOZFwY42WkqjTpSmbe84bbmfTLk/xNFNbVRh958/+L1E+2diyh\nVNi13RyvEQ25M7lonv/Wo5Sxa/vN0WqRbdE+Bjt9k/prB2vh7wuuuYOd5vnqTy0E40+m33z7\nNzkzYkpllWDRfO0LGf8OZVwc520kFGm9MT7boOb5j00KCarKUyIyDbQzqb91sVUYeY29mO3/\nGWecG+VpJJi33ZoiGvAH5m+auL3h5UysanRv76PKHuwMuDPJW9taKixabUrKe9pwO5N66MsS\nSiOvMefT346pI3fNnjhx7u6HakPuTE6ap1vCXE3K9t0TpxFFtkUCvMdO35g0XvEsJTU1NTX5\nz7HeRrlnMy6eOJOsLBlcu5Ty7zGle+3aJRTJZ09czNBqpVqleZEkuLoH1fa3zDYoFLKyVEFM\nTUkRDbQzonOdL0eOHvJpmRyfFAEQzC3NFTDgD8wbmugt/botiQuZuXKgl0mOGcPtjPrx7bsp\nCrdyLtFH1i+cPmHSzKWb9l2NfXt3ncF2JvPqvgNRQqnQxl6qt4OKEqEDxoz5qoGrwoA7k4P4\ndNOAfhtNey2eXr+IALAtEngnGZCuUxgZmxgBUJsYKYRcc+LLe/fiNEpv9xLKbKNGJUqXMNIc\nu3c/QYRd7h+RCeM63128913OMfHZr+sPJCoca9QsZ2SgnTH1DRvn++Y/a9KTX8VFXdu/aMTC\n6yrP4Z0CjQ35AwMAyLy7rHufjeIX65d0KZU4I8eUAXdGff/2fbUmffGnXpNjU7PufBeUdlV6\nzl01s115MwPuzOub1yPVyqDS5sfn95uz8dCpK9EqV8+AxuGTRrf2MBcM+jOTTfKRb0ZtSW+8\nYkSQedYA26J9vGInK2JiYpIoKCwsLbL/pggWlhYCxKSERFGyyrQu9d62Qc16b461azBuWN3C\n7Ez6gXA3SxuncjU7zLlUcsC2PZOqmcHAPzCpl2Z2GrLfpufy+Z87vfOH0HA7o4m5c+eVRky1\nCJ76++VHLxNibh1d0a8K/lzUpcWYo68NuDOal89fZIrqiFnNQ4dsvm/u3ejzhp6KW79+286/\n2oC9L0QD7kw26puLRi2L8h00sY3jX01gW7SPV+xk5gO/JGqN5v2TMiK+urpx+tej5+26qynZ\nbOb61d1KKwGNoXdGWbrZ8EnFU40R9+fG5Qu6dnHctnFIgKUBf2ASj43rOP586cEHpte3yfOC\ngaF2RrCoM2bz9hHFq9X1sVcCgFVgl7nbzJ76tt+8bP7OMTVbGmpnxLS0dBGZD6PsRx86Pb66\njQKAOnb/oNqN5y8cOLPDxal+htqZv4lx2yfNOG3xxZbwitmjhaG3Rft4xU5WBHNLc0HUJCcl\nZ/9VEpMSk0QIFpbmsr/inXLv19ENvPzbTjuMoOHrz17cNiDAUgDYGSjdQvuPHDF8yIjp6/ev\naKPYP6r33Ktqw21Lwv4RXWZH+o9fPa6aeZ4bGGxnIFh51GnarP6bVPdmzPHTDg2sheTL529m\nGmxnBLNCZgKUbt2mjcpKdQCU9nXHfh1aWH17z57baoPtzF80jzYs3Pa82Bdd6ltlGzX4tkiA\nwU5WhCJuJYso1A/vPVRnG1U/uv9IrShS0q2IrH+FxJfHxtTxazH1mEmTGQdvXv3tm9aeb6/9\nG2Zn0i+sGPzVwDmHX2T/iyrY1KztY5QRcercK9Ew2wJoYi9deJiRcGy4t+mblVSNyn19JkOM\nX9XUNGtVVRhoZ97HyMraQhA1Go3BfmagsHN1KSQoi5Usnu3ZCQiWJdxsFZr4F3Eag+3MG+qI\n1UuPpJdt26mGafZhQ2+LFBjs5EXlW71KIfWdY8ej3l7g1jw8fuy+ulCVar6qD/ykvtPcW96+\nxdTzti3mHTm7aXCwi0mueYPsjJB6/deF82b8fD7H6xQ00Y+jNYKqUCGVYJhtAQQrn+Y9e2fX\nq01VB4VgUi60Z+/ePRp7mAoG2hlkXp4W5Ori1X9vcvZRzdMrV59pTMqUL2VkqJ8ZwNgnwMc4\n887liOytEeNu336uURYrXkxpuJ0BAKSdXLHyMrzata+U60wNuy3SkHq9FfqvMq9M9FW9u0Bx\n7IZW9gqjUl1+ic5akVYTt69vGSOFQ+uNsXJeMCjt6MBSSmXJ8AOJ79vCIDuTciC8uFKwqjPj\n8t9rkqXcWfGZo0JR5NNV0RrRQNuSh8yb0wJUORcoNtDOpJ8d6akSzKuMPv7yr9NMubO6dTGl\nwq7l+mcG/ZlRP1gUYqEw8f5q79M3K7KlP9zcqZSRUDho7h21aMCdEUUx49TQMkqj8l+fefdt\nEobcFmkw2OmtvIOdKGbcWtK4qEIwd6/XecjIQR3ruJsLCsdmy++8ZxV5eci8OqmSCgr7irXr\nvaPhgC3RGlE0zM6oH6z53EUpCIXcanzepU/fri2DSlsoBJVLsyU3/1pk1RDb8q53g53BdkaT\ncGK0v7kgmDhXad61b58un9UsbaEQCpfvtunRm9eXGGpnRDHl4re1iigEI3ufRmE9e7Zv4Gmj\nFIwcQ+ddTnmzgcF25s2/R913p+Y1a7BtkQiDnd56X7ATRTH55ubRLQPcbAoXti9btUnXidtu\nv5agQG1K29XV7n23FRgHzr7/1z9IhtcZUcx4duKHvo2qlHctUtjcvpRPrc+HrDofp86xiSG2\nJZe8gp1osJ1Rvzi/dsTnVcsVsylc2L60X502ozbeyPUeCgPtjKh5deWn0R0bVStf1NLK2aNq\n4z7fH4/JGU8MsjPqe7MDjQWTeouevO8SnEG2RSqCKHIVGSIiIiI54MMTRERERDLBYEdEREQk\nEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLB\nYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2\nRERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdE\nREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERE\nRDLBYEdEREQkEwx2RETvJb7a36+sSqF0brvxqZhrLv3q9OrmCqVt6JJ7akmKIyJ6B4MdEdF7\nCVZ1Jy/oWVrxdNOQETueZ492mTcWhE85lWLTYNrCbqWUkhVIRJQDgx0R0QcI1vWmLOhRSni8\nduCYfS//inbqu0u+nHAs2TZ0+sJuJRnriEhnMNgREX2QYB0yZUH3Uri//KsJRxIBQPPgx/5j\nDybaNJ6+oIsb/4oSkQ4RRDH3fSNERJSTGL+7j3/TJY89hv9xaorrlpZ+nbeLTZb/ua1zceY6\nItIlDHZERP+AGL+7d+UmS6O9ew9w++Xb7eqmK//c2tGVsY6IdAuDHRHRPyLG7erp32z5fbWo\nKNpi1Z+bw4ox1hGRruHfJSKif0SwqdWqQTEFoCgS1CbUhX89iUgH8U8TEdE/knxi8uCVD4VC\nhYzito4YuSeO33YQke5hsCMi+geST0zqPeeq6Dlox+4Rfkb3l/cd/jujHRHpHAY7IqL/K+n4\nhF6zr2rK9pk/uk7NYd8PqKiMXBk+fNcLRjsi0i0MdkRE/0fSsYm95l7TlOo+b3yQBVCo6qjv\nw8srHqzsx2hHRDqGwY6I6IOSjo7vNTdCXbzjnEkh1gIAwLzmuHk9SgsPfgwftvM5ox0R6Q4G\nOyKiD0g8Or73/Otq57azvmlsK/w1KljVnTSnU3E8WPUlox0R6RAGOyKi90o8Mr7X/OuZRT/7\n7tvmDkL2GcGm0dRZbV3wYHW/ITtiGe2ISDdwgWIiIiIimeAVOyIiIiKZYLAjIiIikgkGOyIi\nIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIi\nkgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZ\nYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkG\nOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikon/AQMP0WB5pVJl\nAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(X,Y, col=\"red\")\n",
    "points(X, g, col=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787b241-8005-438b-a0c8-9e6305313f4c",
   "metadata": {},
   "source": [
    "Как видно из графика, имеется довольно сильный разброс значений относительно модельных. Качество модели можно оценить уже рассмотренным ранее коэффициентом детерминации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e1a35e1-24d3-4b4b-95d8-c33f5da8703a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ X)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-48.204 -10.253  -3.740   7.262  63.615 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  -0.5429     6.7379  -0.081    0.936    \n",
       "X             1.5573     0.2061   7.556 3.13e-08 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 20.97 on 28 degrees of freedom\n",
       "Multiple R-squared:  0.671,\tAdjusted R-squared:  0.6592 \n",
       "F-statistic:  57.1 on 1 and 28 DF,  p-value: 3.13e-08\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ae067-1d90-4f6d-9ee2-d65cc437b6fa",
   "metadata": {},
   "source": [
    "Функция lm также легко позволяет строить многофакторные линейные модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cbd0d5b-96df-4177-a16a-a10fc1d9e4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=D$`Работающие активы`\n",
    "X1<-D$`Капитал`\n",
    "X2<-D$`Уставной капитал`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8875d5f2-ea78-4b03-994a-d07050249b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ X1 + X2, data = data.frame(Y, X1, X2))\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)           X1           X2  \n",
       "     -4.583        1.397        1.001  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res<-lm(Y~X1+X2,data.frame(Y,X1,X2))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dbfdad7-8f90-456b-8b5c-4a2160282d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 30 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Y</th><th scope=col>g</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td> 11.7</td><td> 26.747448</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 19.8</td><td> 40.746745</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>  2.6</td><td> 11.116406</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 43.6</td><td> 80.390216</td></tr>\n",
       "\t<tr><th scope=row>5</th><td> 29.0</td><td> 53.061108</td></tr>\n",
       "\t<tr><th scope=row>6</th><td> 98.5</td><td> 80.798339</td></tr>\n",
       "\t<tr><th scope=row>7</th><td> 25.6</td><td> 34.541983</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>  6.2</td><td>  8.519598</td></tr>\n",
       "\t<tr><th scope=row>9</th><td> 79.8</td><td> 55.748309</td></tr>\n",
       "\t<tr><th scope=row>10</th><td> 10.1</td><td> 13.314812</td></tr>\n",
       "\t<tr><th scope=row>11</th><td> 30.0</td><td> 58.922621</td></tr>\n",
       "\t<tr><th scope=row>12</th><td> 21.2</td><td> 33.254890</td></tr>\n",
       "\t<tr><th scope=row>13</th><td> 16.7</td><td>  9.078594</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>  9.1</td><td> 18.681360</td></tr>\n",
       "\t<tr><th scope=row>15</th><td> 31.7</td><td> 31.861793</td></tr>\n",
       "\t<tr><th scope=row>16</th><td> 54.4</td><td> 80.905184</td></tr>\n",
       "\t<tr><th scope=row>17</th><td> 21.4</td><td> 14.115728</td></tr>\n",
       "\t<tr><th scope=row>18</th><td> 41.1</td><td> 23.860579</td></tr>\n",
       "\t<tr><th scope=row>19</th><td> 29.8</td><td> 27.408334</td></tr>\n",
       "\t<tr><th scope=row>20</th><td> 10.9</td><td>  7.822910</td></tr>\n",
       "\t<tr><th scope=row>21</th><td> 53.4</td><td> 40.135775</td></tr>\n",
       "\t<tr><th scope=row>22</th><td> 22.6</td><td> 19.228017</td></tr>\n",
       "\t<tr><th scope=row>23</th><td> 11.7</td><td> 14.257533</td></tr>\n",
       "\t<tr><th scope=row>24</th><td> 27.3</td><td> 35.063401</td></tr>\n",
       "\t<tr><th scope=row>25</th><td> 70.2</td><td> 33.465682</td></tr>\n",
       "\t<tr><th scope=row>26</th><td>124.2</td><td>117.587847</td></tr>\n",
       "\t<tr><th scope=row>27</th><td> 90.4</td><td> 94.479065</td></tr>\n",
       "\t<tr><th scope=row>28</th><td>101.7</td><td> 90.956549</td></tr>\n",
       "\t<tr><th scope=row>29</th><td> 18.2</td><td> 19.002829</td></tr>\n",
       "\t<tr><th scope=row>30</th><td>127.7</td><td> 65.526345</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & Y & g\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 &  11.7 &  26.747448\\\\\n",
       "\t2 &  19.8 &  40.746745\\\\\n",
       "\t3 &   2.6 &  11.116406\\\\\n",
       "\t4 &  43.6 &  80.390216\\\\\n",
       "\t5 &  29.0 &  53.061108\\\\\n",
       "\t6 &  98.5 &  80.798339\\\\\n",
       "\t7 &  25.6 &  34.541983\\\\\n",
       "\t8 &   6.2 &   8.519598\\\\\n",
       "\t9 &  79.8 &  55.748309\\\\\n",
       "\t10 &  10.1 &  13.314812\\\\\n",
       "\t11 &  30.0 &  58.922621\\\\\n",
       "\t12 &  21.2 &  33.254890\\\\\n",
       "\t13 &  16.7 &   9.078594\\\\\n",
       "\t14 &   9.1 &  18.681360\\\\\n",
       "\t15 &  31.7 &  31.861793\\\\\n",
       "\t16 &  54.4 &  80.905184\\\\\n",
       "\t17 &  21.4 &  14.115728\\\\\n",
       "\t18 &  41.1 &  23.860579\\\\\n",
       "\t19 &  29.8 &  27.408334\\\\\n",
       "\t20 &  10.9 &   7.822910\\\\\n",
       "\t21 &  53.4 &  40.135775\\\\\n",
       "\t22 &  22.6 &  19.228017\\\\\n",
       "\t23 &  11.7 &  14.257533\\\\\n",
       "\t24 &  27.3 &  35.063401\\\\\n",
       "\t25 &  70.2 &  33.465682\\\\\n",
       "\t26 & 124.2 & 117.587847\\\\\n",
       "\t27 &  90.4 &  94.479065\\\\\n",
       "\t28 & 101.7 &  90.956549\\\\\n",
       "\t29 &  18.2 &  19.002829\\\\\n",
       "\t30 & 127.7 &  65.526345\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 2\n",
       "\n",
       "| <!--/--> | Y &lt;dbl&gt; | g &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 |  11.7 |  26.747448 |\n",
       "| 2 |  19.8 |  40.746745 |\n",
       "| 3 |   2.6 |  11.116406 |\n",
       "| 4 |  43.6 |  80.390216 |\n",
       "| 5 |  29.0 |  53.061108 |\n",
       "| 6 |  98.5 |  80.798339 |\n",
       "| 7 |  25.6 |  34.541983 |\n",
       "| 8 |   6.2 |   8.519598 |\n",
       "| 9 |  79.8 |  55.748309 |\n",
       "| 10 |  10.1 |  13.314812 |\n",
       "| 11 |  30.0 |  58.922621 |\n",
       "| 12 |  21.2 |  33.254890 |\n",
       "| 13 |  16.7 |   9.078594 |\n",
       "| 14 |   9.1 |  18.681360 |\n",
       "| 15 |  31.7 |  31.861793 |\n",
       "| 16 |  54.4 |  80.905184 |\n",
       "| 17 |  21.4 |  14.115728 |\n",
       "| 18 |  41.1 |  23.860579 |\n",
       "| 19 |  29.8 |  27.408334 |\n",
       "| 20 |  10.9 |   7.822910 |\n",
       "| 21 |  53.4 |  40.135775 |\n",
       "| 22 |  22.6 |  19.228017 |\n",
       "| 23 |  11.7 |  14.257533 |\n",
       "| 24 |  27.3 |  35.063401 |\n",
       "| 25 |  70.2 |  33.465682 |\n",
       "| 26 | 124.2 | 117.587847 |\n",
       "| 27 |  90.4 |  94.479065 |\n",
       "| 28 | 101.7 |  90.956549 |\n",
       "| 29 |  18.2 |  19.002829 |\n",
       "| 30 | 127.7 |  65.526345 |\n",
       "\n"
      ],
      "text/plain": [
       "   Y     g         \n",
       "1   11.7  26.747448\n",
       "2   19.8  40.746745\n",
       "3    2.6  11.116406\n",
       "4   43.6  80.390216\n",
       "5   29.0  53.061108\n",
       "6   98.5  80.798339\n",
       "7   25.6  34.541983\n",
       "8    6.2   8.519598\n",
       "9   79.8  55.748309\n",
       "10  10.1  13.314812\n",
       "11  30.0  58.922621\n",
       "12  21.2  33.254890\n",
       "13  16.7   9.078594\n",
       "14   9.1  18.681360\n",
       "15  31.7  31.861793\n",
       "16  54.4  80.905184\n",
       "17  21.4  14.115728\n",
       "18  41.1  23.860579\n",
       "19  29.8  27.408334\n",
       "20  10.9   7.822910\n",
       "21  53.4  40.135775\n",
       "22  22.6  19.228017\n",
       "23  11.7  14.257533\n",
       "24  27.3  35.063401\n",
       "25  70.2  33.465682\n",
       "26 124.2 117.587847\n",
       "27  90.4  94.479065\n",
       "28 101.7  90.956549\n",
       "29  18.2  19.002829\n",
       "30 127.7  65.526345"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g<-predict(res,data.frame(X1, X2,Y))\n",
    "View(data.frame(Y,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd3ae22-612e-433e-a114-598a0ba607c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Y ~ X1 + X2, data = data.frame(Y, X1, X2))\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-36.790  -9.422  -1.561   7.537  62.174 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  -4.5833     7.2505  -0.632    0.533    \n",
       "X1            1.3975     0.2336   5.982 2.22e-06 ***\n",
       "X2            1.0011     0.7261   1.379    0.179    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 20.64 on 27 degrees of freedom\n",
       "Multiple R-squared:  0.6926,\tAdjusted R-squared:  0.6698 \n",
       "F-statistic: 30.42 on 2 and 27 DF,  p-value: 1.213e-07\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c617d-556f-4517-924e-d27a977527c4",
   "metadata": {},
   "source": [
    "## Дискриминантный анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec03e1-def9-4a27-a5cf-5e232e7af551",
   "metadata": {},
   "source": [
    "Для повышения точности можно загрубить цель предсказания, например, разбить значения переменной Y на квартили и пытыться предсказать, в какой квартиль мы попадем при том или ином значении показателей X1, X2. В этом случае мы приходим к идеям **дискриминантного анализа** - один из методов многомерного анализа для классификации объектов.\n",
    "\n",
    "В среде R для реализации дискриминантного анализа используется функция lda, входящую в библиотеку MASS.\n",
    "\n",
    "Рассмотрим использование дискриминантного анализа на примере датасета iris.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7dc2d78-d9f2-4705-badf-13b4e163ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/home/juna/R/x86_64-pc-linux-gnu-library/3.5’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"MASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf1f775-4cf6-4c09-a367-f89c8a9b65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c5f44c5-b509-49e3-8c03-96154ac24dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 150 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>№</th><th scope=col>sl</th><th scope=col>sw</th><th scope=col>pl</th><th scope=col>pw</th><th scope=col>class</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 2</td><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 3</td><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 4</td><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 5</td><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 6</td><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr>\n",
       "\t<tr><td> 7</td><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>setosa</td></tr>\n",
       "\t<tr><td> 8</td><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td> 9</td><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>10</td><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>setosa</td></tr>\n",
       "\t<tr><td>11</td><td>5.4</td><td>3.7</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>12</td><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>13</td><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td><td>setosa</td></tr>\n",
       "\t<tr><td>14</td><td>4.3</td><td>3.0</td><td>1.1</td><td>0.1</td><td>setosa</td></tr>\n",
       "\t<tr><td>15</td><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>16</td><td>5.7</td><td>4.4</td><td>1.5</td><td>0.4</td><td>setosa</td></tr>\n",
       "\t<tr><td>17</td><td>5.4</td><td>3.9</td><td>1.3</td><td>0.4</td><td>setosa</td></tr>\n",
       "\t<tr><td>18</td><td>5.1</td><td>3.5</td><td>1.4</td><td>0.3</td><td>setosa</td></tr>\n",
       "\t<tr><td>19</td><td>5.7</td><td>3.8</td><td>1.7</td><td>0.3</td><td>setosa</td></tr>\n",
       "\t<tr><td>20</td><td>5.1</td><td>3.8</td><td>1.5</td><td>0.3</td><td>setosa</td></tr>\n",
       "\t<tr><td>21</td><td>5.4</td><td>3.4</td><td>1.7</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>22</td><td>5.1</td><td>3.7</td><td>1.5</td><td>0.4</td><td>setosa</td></tr>\n",
       "\t<tr><td>23</td><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>24</td><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>setosa</td></tr>\n",
       "\t<tr><td>25</td><td>4.8</td><td>3.4</td><td>1.9</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>26</td><td>5.0</td><td>3.0</td><td>1.6</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>27</td><td>5.0</td><td>3.4</td><td>1.6</td><td>0.4</td><td>setosa</td></tr>\n",
       "\t<tr><td>28</td><td>5.2</td><td>3.5</td><td>1.5</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>29</td><td>5.2</td><td>3.4</td><td>1.4</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>30</td><td>4.7</td><td>3.2</td><td>1.6</td><td>0.2</td><td>setosa</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>121</td><td>6.9</td><td>3.2</td><td>5.7</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>122</td><td>5.6</td><td>2.8</td><td>4.9</td><td>2.0</td><td>virginica</td></tr>\n",
       "\t<tr><td>123</td><td>7.7</td><td>2.8</td><td>6.7</td><td>2.0</td><td>virginica</td></tr>\n",
       "\t<tr><td>124</td><td>6.3</td><td>2.7</td><td>4.9</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>125</td><td>6.7</td><td>3.3</td><td>5.7</td><td>2.1</td><td>virginica</td></tr>\n",
       "\t<tr><td>126</td><td>7.2</td><td>3.2</td><td>6.0</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>127</td><td>6.2</td><td>2.8</td><td>4.8</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>128</td><td>6.1</td><td>3.0</td><td>4.9</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>129</td><td>6.4</td><td>2.8</td><td>5.6</td><td>2.1</td><td>virginica</td></tr>\n",
       "\t<tr><td>130</td><td>7.2</td><td>3.0</td><td>5.8</td><td>1.6</td><td>virginica</td></tr>\n",
       "\t<tr><td>131</td><td>7.4</td><td>2.8</td><td>6.1</td><td>1.9</td><td>virginica</td></tr>\n",
       "\t<tr><td>132</td><td>7.9</td><td>3.8</td><td>6.4</td><td>2.0</td><td>virginica</td></tr>\n",
       "\t<tr><td>133</td><td>6.4</td><td>2.8</td><td>5.6</td><td>2.2</td><td>virginica</td></tr>\n",
       "\t<tr><td>134</td><td>6.3</td><td>2.8</td><td>5.1</td><td>1.5</td><td>virginica</td></tr>\n",
       "\t<tr><td>135</td><td>6.1</td><td>2.6</td><td>5.6</td><td>1.4</td><td>virginica</td></tr>\n",
       "\t<tr><td>136</td><td>7.7</td><td>3.0</td><td>6.1</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>137</td><td>6.3</td><td>3.4</td><td>5.6</td><td>2.4</td><td>virginica</td></tr>\n",
       "\t<tr><td>138</td><td>6.4</td><td>3.1</td><td>5.5</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>139</td><td>6.0</td><td>3.0</td><td>4.8</td><td>1.8</td><td>virginica</td></tr>\n",
       "\t<tr><td>140</td><td>6.9</td><td>3.1</td><td>5.4</td><td>2.1</td><td>virginica</td></tr>\n",
       "\t<tr><td>141</td><td>6.7</td><td>3.1</td><td>5.6</td><td>2.4</td><td>virginica</td></tr>\n",
       "\t<tr><td>142</td><td>6.9</td><td>3.1</td><td>5.1</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>143</td><td>5.8</td><td>2.7</td><td>5.1</td><td>1.9</td><td>virginica</td></tr>\n",
       "\t<tr><td>144</td><td>6.8</td><td>3.2</td><td>5.9</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>145</td><td>6.7</td><td>3.3</td><td>5.7</td><td>2.5</td><td>virginica</td></tr>\n",
       "\t<tr><td>146</td><td>6.7</td><td>3.0</td><td>5.2</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>147</td><td>6.3</td><td>2.5</td><td>5.0</td><td>1.9</td><td>virginica</td></tr>\n",
       "\t<tr><td>148</td><td>6.5</td><td>3.0</td><td>5.2</td><td>2.0</td><td>virginica</td></tr>\n",
       "\t<tr><td>149</td><td>6.2</td><td>3.4</td><td>5.4</td><td>2.3</td><td>virginica</td></tr>\n",
       "\t<tr><td>150</td><td>5.9</td><td>3.0</td><td>5.1</td><td>1.8</td><td>virginica</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 150 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " № & sl & sw & pl & pw & class\\\\\n",
       " <int> & <dbl> & <dbl> & <dbl> & <dbl> & <chr>\\\\\n",
       "\\hline\n",
       "\t  1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa\\\\\n",
       "\t  2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa\\\\\n",
       "\t  3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa\\\\\n",
       "\t  4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa\\\\\n",
       "\t  5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa\\\\\n",
       "\t  6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa\\\\\n",
       "\t  7 & 4.6 & 3.4 & 1.4 & 0.3 & setosa\\\\\n",
       "\t  8 & 5.0 & 3.4 & 1.5 & 0.2 & setosa\\\\\n",
       "\t  9 & 4.4 & 2.9 & 1.4 & 0.2 & setosa\\\\\n",
       "\t 10 & 4.9 & 3.1 & 1.5 & 0.1 & setosa\\\\\n",
       "\t 11 & 5.4 & 3.7 & 1.5 & 0.2 & setosa\\\\\n",
       "\t 12 & 4.8 & 3.4 & 1.6 & 0.2 & setosa\\\\\n",
       "\t 13 & 4.8 & 3.0 & 1.4 & 0.1 & setosa\\\\\n",
       "\t 14 & 4.3 & 3.0 & 1.1 & 0.1 & setosa\\\\\n",
       "\t 15 & 5.8 & 4.0 & 1.2 & 0.2 & setosa\\\\\n",
       "\t 16 & 5.7 & 4.4 & 1.5 & 0.4 & setosa\\\\\n",
       "\t 17 & 5.4 & 3.9 & 1.3 & 0.4 & setosa\\\\\n",
       "\t 18 & 5.1 & 3.5 & 1.4 & 0.3 & setosa\\\\\n",
       "\t 19 & 5.7 & 3.8 & 1.7 & 0.3 & setosa\\\\\n",
       "\t 20 & 5.1 & 3.8 & 1.5 & 0.3 & setosa\\\\\n",
       "\t 21 & 5.4 & 3.4 & 1.7 & 0.2 & setosa\\\\\n",
       "\t 22 & 5.1 & 3.7 & 1.5 & 0.4 & setosa\\\\\n",
       "\t 23 & 4.6 & 3.6 & 1.0 & 0.2 & setosa\\\\\n",
       "\t 24 & 5.1 & 3.3 & 1.7 & 0.5 & setosa\\\\\n",
       "\t 25 & 4.8 & 3.4 & 1.9 & 0.2 & setosa\\\\\n",
       "\t 26 & 5.0 & 3.0 & 1.6 & 0.2 & setosa\\\\\n",
       "\t 27 & 5.0 & 3.4 & 1.6 & 0.4 & setosa\\\\\n",
       "\t 28 & 5.2 & 3.5 & 1.5 & 0.2 & setosa\\\\\n",
       "\t 29 & 5.2 & 3.4 & 1.4 & 0.2 & setosa\\\\\n",
       "\t 30 & 4.7 & 3.2 & 1.6 & 0.2 & setosa\\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 121 & 6.9 & 3.2 & 5.7 & 2.3 & virginica\\\\\n",
       "\t 122 & 5.6 & 2.8 & 4.9 & 2.0 & virginica\\\\\n",
       "\t 123 & 7.7 & 2.8 & 6.7 & 2.0 & virginica\\\\\n",
       "\t 124 & 6.3 & 2.7 & 4.9 & 1.8 & virginica\\\\\n",
       "\t 125 & 6.7 & 3.3 & 5.7 & 2.1 & virginica\\\\\n",
       "\t 126 & 7.2 & 3.2 & 6.0 & 1.8 & virginica\\\\\n",
       "\t 127 & 6.2 & 2.8 & 4.8 & 1.8 & virginica\\\\\n",
       "\t 128 & 6.1 & 3.0 & 4.9 & 1.8 & virginica\\\\\n",
       "\t 129 & 6.4 & 2.8 & 5.6 & 2.1 & virginica\\\\\n",
       "\t 130 & 7.2 & 3.0 & 5.8 & 1.6 & virginica\\\\\n",
       "\t 131 & 7.4 & 2.8 & 6.1 & 1.9 & virginica\\\\\n",
       "\t 132 & 7.9 & 3.8 & 6.4 & 2.0 & virginica\\\\\n",
       "\t 133 & 6.4 & 2.8 & 5.6 & 2.2 & virginica\\\\\n",
       "\t 134 & 6.3 & 2.8 & 5.1 & 1.5 & virginica\\\\\n",
       "\t 135 & 6.1 & 2.6 & 5.6 & 1.4 & virginica\\\\\n",
       "\t 136 & 7.7 & 3.0 & 6.1 & 2.3 & virginica\\\\\n",
       "\t 137 & 6.3 & 3.4 & 5.6 & 2.4 & virginica\\\\\n",
       "\t 138 & 6.4 & 3.1 & 5.5 & 1.8 & virginica\\\\\n",
       "\t 139 & 6.0 & 3.0 & 4.8 & 1.8 & virginica\\\\\n",
       "\t 140 & 6.9 & 3.1 & 5.4 & 2.1 & virginica\\\\\n",
       "\t 141 & 6.7 & 3.1 & 5.6 & 2.4 & virginica\\\\\n",
       "\t 142 & 6.9 & 3.1 & 5.1 & 2.3 & virginica\\\\\n",
       "\t 143 & 5.8 & 2.7 & 5.1 & 1.9 & virginica\\\\\n",
       "\t 144 & 6.8 & 3.2 & 5.9 & 2.3 & virginica\\\\\n",
       "\t 145 & 6.7 & 3.3 & 5.7 & 2.5 & virginica\\\\\n",
       "\t 146 & 6.7 & 3.0 & 5.2 & 2.3 & virginica\\\\\n",
       "\t 147 & 6.3 & 2.5 & 5.0 & 1.9 & virginica\\\\\n",
       "\t 148 & 6.5 & 3.0 & 5.2 & 2.0 & virginica\\\\\n",
       "\t 149 & 6.2 & 3.4 & 5.4 & 2.3 & virginica\\\\\n",
       "\t 150 & 5.9 & 3.0 & 5.1 & 1.8 & virginica\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 150 × 6\n",
       "\n",
       "| № &lt;int&gt; | sl &lt;dbl&gt; | sw &lt;dbl&gt; | pl &lt;dbl&gt; | pw &lt;dbl&gt; | class &lt;chr&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "|  1 | 5.1 | 3.5 | 1.4 | 0.2 | setosa |\n",
       "|  2 | 4.9 | 3.0 | 1.4 | 0.2 | setosa |\n",
       "|  3 | 4.7 | 3.2 | 1.3 | 0.2 | setosa |\n",
       "|  4 | 4.6 | 3.1 | 1.5 | 0.2 | setosa |\n",
       "|  5 | 5.0 | 3.6 | 1.4 | 0.2 | setosa |\n",
       "|  6 | 5.4 | 3.9 | 1.7 | 0.4 | setosa |\n",
       "|  7 | 4.6 | 3.4 | 1.4 | 0.3 | setosa |\n",
       "|  8 | 5.0 | 3.4 | 1.5 | 0.2 | setosa |\n",
       "|  9 | 4.4 | 2.9 | 1.4 | 0.2 | setosa |\n",
       "| 10 | 4.9 | 3.1 | 1.5 | 0.1 | setosa |\n",
       "| 11 | 5.4 | 3.7 | 1.5 | 0.2 | setosa |\n",
       "| 12 | 4.8 | 3.4 | 1.6 | 0.2 | setosa |\n",
       "| 13 | 4.8 | 3.0 | 1.4 | 0.1 | setosa |\n",
       "| 14 | 4.3 | 3.0 | 1.1 | 0.1 | setosa |\n",
       "| 15 | 5.8 | 4.0 | 1.2 | 0.2 | setosa |\n",
       "| 16 | 5.7 | 4.4 | 1.5 | 0.4 | setosa |\n",
       "| 17 | 5.4 | 3.9 | 1.3 | 0.4 | setosa |\n",
       "| 18 | 5.1 | 3.5 | 1.4 | 0.3 | setosa |\n",
       "| 19 | 5.7 | 3.8 | 1.7 | 0.3 | setosa |\n",
       "| 20 | 5.1 | 3.8 | 1.5 | 0.3 | setosa |\n",
       "| 21 | 5.4 | 3.4 | 1.7 | 0.2 | setosa |\n",
       "| 22 | 5.1 | 3.7 | 1.5 | 0.4 | setosa |\n",
       "| 23 | 4.6 | 3.6 | 1.0 | 0.2 | setosa |\n",
       "| 24 | 5.1 | 3.3 | 1.7 | 0.5 | setosa |\n",
       "| 25 | 4.8 | 3.4 | 1.9 | 0.2 | setosa |\n",
       "| 26 | 5.0 | 3.0 | 1.6 | 0.2 | setosa |\n",
       "| 27 | 5.0 | 3.4 | 1.6 | 0.4 | setosa |\n",
       "| 28 | 5.2 | 3.5 | 1.5 | 0.2 | setosa |\n",
       "| 29 | 5.2 | 3.4 | 1.4 | 0.2 | setosa |\n",
       "| 30 | 4.7 | 3.2 | 1.6 | 0.2 | setosa |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 121 | 6.9 | 3.2 | 5.7 | 2.3 | virginica |\n",
       "| 122 | 5.6 | 2.8 | 4.9 | 2.0 | virginica |\n",
       "| 123 | 7.7 | 2.8 | 6.7 | 2.0 | virginica |\n",
       "| 124 | 6.3 | 2.7 | 4.9 | 1.8 | virginica |\n",
       "| 125 | 6.7 | 3.3 | 5.7 | 2.1 | virginica |\n",
       "| 126 | 7.2 | 3.2 | 6.0 | 1.8 | virginica |\n",
       "| 127 | 6.2 | 2.8 | 4.8 | 1.8 | virginica |\n",
       "| 128 | 6.1 | 3.0 | 4.9 | 1.8 | virginica |\n",
       "| 129 | 6.4 | 2.8 | 5.6 | 2.1 | virginica |\n",
       "| 130 | 7.2 | 3.0 | 5.8 | 1.6 | virginica |\n",
       "| 131 | 7.4 | 2.8 | 6.1 | 1.9 | virginica |\n",
       "| 132 | 7.9 | 3.8 | 6.4 | 2.0 | virginica |\n",
       "| 133 | 6.4 | 2.8 | 5.6 | 2.2 | virginica |\n",
       "| 134 | 6.3 | 2.8 | 5.1 | 1.5 | virginica |\n",
       "| 135 | 6.1 | 2.6 | 5.6 | 1.4 | virginica |\n",
       "| 136 | 7.7 | 3.0 | 6.1 | 2.3 | virginica |\n",
       "| 137 | 6.3 | 3.4 | 5.6 | 2.4 | virginica |\n",
       "| 138 | 6.4 | 3.1 | 5.5 | 1.8 | virginica |\n",
       "| 139 | 6.0 | 3.0 | 4.8 | 1.8 | virginica |\n",
       "| 140 | 6.9 | 3.1 | 5.4 | 2.1 | virginica |\n",
       "| 141 | 6.7 | 3.1 | 5.6 | 2.4 | virginica |\n",
       "| 142 | 6.9 | 3.1 | 5.1 | 2.3 | virginica |\n",
       "| 143 | 5.8 | 2.7 | 5.1 | 1.9 | virginica |\n",
       "| 144 | 6.8 | 3.2 | 5.9 | 2.3 | virginica |\n",
       "| 145 | 6.7 | 3.3 | 5.7 | 2.5 | virginica |\n",
       "| 146 | 6.7 | 3.0 | 5.2 | 2.3 | virginica |\n",
       "| 147 | 6.3 | 2.5 | 5.0 | 1.9 | virginica |\n",
       "| 148 | 6.5 | 3.0 | 5.2 | 2.0 | virginica |\n",
       "| 149 | 6.2 | 3.4 | 5.4 | 2.3 | virginica |\n",
       "| 150 | 5.9 | 3.0 | 5.1 | 1.8 | virginica |\n",
       "\n"
      ],
      "text/plain": [
       "    №   sl  sw  pl  pw  class    \n",
       "1    1  5.1 3.5 1.4 0.2 setosa   \n",
       "2    2  4.9 3.0 1.4 0.2 setosa   \n",
       "3    3  4.7 3.2 1.3 0.2 setosa   \n",
       "4    4  4.6 3.1 1.5 0.2 setosa   \n",
       "5    5  5.0 3.6 1.4 0.2 setosa   \n",
       "6    6  5.4 3.9 1.7 0.4 setosa   \n",
       "7    7  4.6 3.4 1.4 0.3 setosa   \n",
       "8    8  5.0 3.4 1.5 0.2 setosa   \n",
       "9    9  4.4 2.9 1.4 0.2 setosa   \n",
       "10  10  4.9 3.1 1.5 0.1 setosa   \n",
       "11  11  5.4 3.7 1.5 0.2 setosa   \n",
       "12  12  4.8 3.4 1.6 0.2 setosa   \n",
       "13  13  4.8 3.0 1.4 0.1 setosa   \n",
       "14  14  4.3 3.0 1.1 0.1 setosa   \n",
       "15  15  5.8 4.0 1.2 0.2 setosa   \n",
       "16  16  5.7 4.4 1.5 0.4 setosa   \n",
       "17  17  5.4 3.9 1.3 0.4 setosa   \n",
       "18  18  5.1 3.5 1.4 0.3 setosa   \n",
       "19  19  5.7 3.8 1.7 0.3 setosa   \n",
       "20  20  5.1 3.8 1.5 0.3 setosa   \n",
       "21  21  5.4 3.4 1.7 0.2 setosa   \n",
       "22  22  5.1 3.7 1.5 0.4 setosa   \n",
       "23  23  4.6 3.6 1.0 0.2 setosa   \n",
       "24  24  5.1 3.3 1.7 0.5 setosa   \n",
       "25  25  4.8 3.4 1.9 0.2 setosa   \n",
       "26  26  5.0 3.0 1.6 0.2 setosa   \n",
       "27  27  5.0 3.4 1.6 0.4 setosa   \n",
       "28  28  5.2 3.5 1.5 0.2 setosa   \n",
       "29  29  5.2 3.4 1.4 0.2 setosa   \n",
       "30  30  4.7 3.2 1.6 0.2 setosa   \n",
       "⋮   ⋮   ⋮   ⋮   ⋮   ⋮   ⋮        \n",
       "121 121 6.9 3.2 5.7 2.3 virginica\n",
       "122 122 5.6 2.8 4.9 2.0 virginica\n",
       "123 123 7.7 2.8 6.7 2.0 virginica\n",
       "124 124 6.3 2.7 4.9 1.8 virginica\n",
       "125 125 6.7 3.3 5.7 2.1 virginica\n",
       "126 126 7.2 3.2 6.0 1.8 virginica\n",
       "127 127 6.2 2.8 4.8 1.8 virginica\n",
       "128 128 6.1 3.0 4.9 1.8 virginica\n",
       "129 129 6.4 2.8 5.6 2.1 virginica\n",
       "130 130 7.2 3.0 5.8 1.6 virginica\n",
       "131 131 7.4 2.8 6.1 1.9 virginica\n",
       "132 132 7.9 3.8 6.4 2.0 virginica\n",
       "133 133 6.4 2.8 5.6 2.2 virginica\n",
       "134 134 6.3 2.8 5.1 1.5 virginica\n",
       "135 135 6.1 2.6 5.6 1.4 virginica\n",
       "136 136 7.7 3.0 6.1 2.3 virginica\n",
       "137 137 6.3 3.4 5.6 2.4 virginica\n",
       "138 138 6.4 3.1 5.5 1.8 virginica\n",
       "139 139 6.0 3.0 4.8 1.8 virginica\n",
       "140 140 6.9 3.1 5.4 2.1 virginica\n",
       "141 141 6.7 3.1 5.6 2.4 virginica\n",
       "142 142 6.9 3.1 5.1 2.3 virginica\n",
       "143 143 5.8 2.7 5.1 1.9 virginica\n",
       "144 144 6.8 3.2 5.9 2.3 virginica\n",
       "145 145 6.7 3.3 5.7 2.5 virginica\n",
       "146 146 6.7 3.0 5.2 2.3 virginica\n",
       "147 147 6.3 2.5 5.0 1.9 virginica\n",
       "148 148 6.5 3.0 5.2 2.0 virginica\n",
       "149 149 6.2 3.4 5.4 2.3 virginica\n",
       "150 150 5.9 3.0 5.1 1.8 virginica"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris<-read.csv(\"iris.csv\")\n",
    "colnames(iris)<-c(\"№\",\"sl\",\"sw\",\"pl\",\"pw\", \"class\")\n",
    "View(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13142cd4-d4d6-4d85-bfd6-e58641db7194",
   "metadata": {},
   "source": [
    "Обычно для построения модели проводят разбиение всех данных на обучающую и тестовую выборку. Как правило, обучающая выборка составляет 75-80% от объема исходных данных, хотя каких-то строгих правил в этом отношении не существует. На обучающей выборке строят модель, на тестовой выборке проверяют ее адекватность. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39f1562-a12e-490b-8ef2-49f93dbcff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формирование обучающей и тестовой выборок\n",
    "x<-c(1:150)\n",
    "y<-sample(x,floor(150*0.8))\n",
    "z<-setdiff(x,y)\n",
    "D_teach<-iris[y,]\n",
    "D_test<-iris[z,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff27c418-74a8-4cba-8761-19da15c05e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(class ~ sl + sw + pl + pw, data = D_teach)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    setosa versicolor  virginica \n",
       " 0.3333333  0.3166667  0.3500000 \n",
       "\n",
       "Group means:\n",
       "                 sl       sw       pl       pw\n",
       "setosa     4.965000 3.402500 1.465000 0.242500\n",
       "versicolor 5.839474 2.768421 4.200000 1.310526\n",
       "virginica  6.597619 2.969048 5.590476 2.035714\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "          LD1        LD2\n",
       "sl  0.8272437  0.6085842\n",
       "sw  1.6043183  1.8732868\n",
       "pl -2.0268562 -1.1915768\n",
       "pw -3.1599949  2.7742864\n",
       "\n",
       "Proportion of trace:\n",
       "   LD1    LD2 \n",
       "0.9919 0.0081 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res<-lda(class~sl+sw+pl+pw,D_teach)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "944ee247-e3aa-40ee-a565-e5f281f73153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 30 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>D_test.class</th><th scope=col>g.class</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 2\n",
       "\\begin{tabular}{ll}\n",
       " D\\_test.class & g.class\\\\\n",
       " <fct> & <fct>\\\\\n",
       "\\hline\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 2\n",
       "\n",
       "| D_test.class &lt;fct&gt; | g.class &lt;fct&gt; |\n",
       "|---|---|\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "\n"
      ],
      "text/plain": [
       "   D_test.class g.class   \n",
       "1  setosa       setosa    \n",
       "2  setosa       setosa    \n",
       "3  setosa       setosa    \n",
       "4  setosa       setosa    \n",
       "5  setosa       setosa    \n",
       "6  setosa       setosa    \n",
       "7  setosa       setosa    \n",
       "8  versicolor   versicolor\n",
       "9  versicolor   versicolor\n",
       "10 versicolor   versicolor\n",
       "11 versicolor   versicolor\n",
       "12 versicolor   versicolor\n",
       "13 versicolor   versicolor\n",
       "14 versicolor   virginica \n",
       "15 versicolor   versicolor\n",
       "16 versicolor   versicolor\n",
       "17 versicolor   versicolor\n",
       "18 versicolor   versicolor\n",
       "19 virginica    virginica \n",
       "20 virginica    virginica \n",
       "21 virginica    virginica \n",
       "22 virginica    virginica \n",
       "23 virginica    virginica \n",
       "24 virginica    virginica \n",
       "25 virginica    virginica \n",
       "26 virginica    virginica \n",
       "27 virginica    virginica \n",
       "28 virginica    virginica \n",
       "29 virginica    virginica \n",
       "30 virginica    virginica "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g<-predict(res,D_test)\n",
    "View(data.frame(D_test$class,g$class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ed111-04d5-40d2-9aeb-8a7419c3bc16",
   "metadata": {},
   "source": [
    "## Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8cb8f-9487-4022-a678-57c87bf8536c",
   "metadata": {},
   "source": [
    "Кроме рассмотренный методов в последнее время разработано множество более продвинутых методов многомерного анализа.\n",
    "\n",
    "Один из таких методов является **метод опорных векторов**.\n",
    "\n",
    "В этом методе задача классификации состоит в поиске гиперплоскости в  $n$-мерном пространстве, разделяющей точки одного класса от точек другого класса. Метод опорных вектров базируется на постулате, что наилучшая разделяющая гиперплоскость отстоит максимально далеко от ближайших до нее точек обоих классов, тогда для оценки качества разделения будем искать две гиперплоскости максимально близкие к точках двух классов и параллельные искомой разделяющей гиперплоскости. Ясно, что для улучшения разделения классов расстояние между указанными гиперплоскостями следует увеличивать. Это приводит к следующей формализации:\n",
    "\n",
    "найти вектор  $Y=(y_{1,}y_{2,}\\ldots y_{n})$, а также некоторое значение $b$, такие что:\n",
    "\n",
    "$\\langle W_{i},Y\\rangle -b\\geqslant 1$, если вектору  $W_{i}=(w_{1}^{i},w_{2}^{i},\\ldots w_{n}^{i})$  сопоставлена  $r_{i}=1$;\n",
    "$\\langle W_{i},Y\\rangle -b\\leqslant -1$ , если вектору  $W_{i}=(w_{1}^{i},w_{2}^{i},\\ldots w_{n}^{i})$  сопоставлена $r_{i}=-1$\n",
    "\n",
    "Эти условия обычно переписывают в виде:\n",
    "$$r_{i}\\cdot (\\langle W_{i},Y\\rangle -b)\\geq 1$$\n",
    "\n",
    "Показано, что ширина полосы между гиперплоскостями может быть найдена как  $\\frac{2}{\\langle Y,Y\\rangle }$ , где  $\\langle Y,Y\\rangle $ - скалярное произведение вектора  $Y$  на самого себя. Поэтому условие оптимального выбора разделяющей плоскости сводится к нахождению наиболее широкой разделяющей полосы:  $\\langle Y,Y\\rangle\\to \\mathit{min}$ , или   $\\frac{1}{2}\\cdot \\langle Y,Y\\rangle \\to \\mathit{min}$ .\n",
    "\n",
    "Во многих случаях строгая линейная разделимость точек невозможна (ряд точек из учебной коллекции попадает внутрь разделяющей полосы). Поэтому ограничения смягчают, позволяя допускать ошибки:\n",
    " $$r_{i}\\cdot (\\langle W_{i},Y\\rangle -b)\\geqslant 1-\\xi_{i},$$\n",
    "где при  $\\xi_{i}\\geqslant 0$ на  $d_{i}$-м объекте допускается ошибка (точка попадает внутрь разделяющей полосы).\n",
    "Для минимизации таких ошибок в целевую  функцию вводят штраф. \n",
    "Окончательно задача формулируется в виде - найти такие  $Y=(y_{1,}y_{2,}\\ldots y_{n})$ ,  $b$  и  $\\xi_{i}$ что:\n",
    " $$ r_{i}\\cdot (\\langle W_{i},Y\\rangle -b)\\geqslant 1-\\xi_{i},$$\n",
    "$$\\xi_{i}\\geqslant 0,$$\n",
    "$$ \\frac{1}{2}\\cdot \\langle Y,Y\\rangle +S\\cdot \\sum _{i}\\xi_{i}\\to \\mathit{min},$$\n",
    "$$i=1,\\ldots,m$$\n",
    "где  $S$ - параметр настройки метода, который позволяет регулировать соотношение между максимизацией ширины разделяющей полосы и минимизацией суммарной ошибки;  $m$ - количество обучающих примеров.\n",
    "\n",
    "Существует также другой подход к линейной разделимости выборки, основанный на преобразовании исходного признакового пространства с помощью ядер. При этом исходный алгоритм сохраняется с заменой скалярных произведений векторов нелинейной функцией ядра (скалярным произведением в пространстве с большей размерностью).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172ac8d-2601-42fa-b6fb-41235eea5707",
   "metadata": {},
   "source": [
    "В среде R методы данного класса реализованы в пакетах «knitr», «kernlab». После их подключения становится доступна функция:\n",
    "\n",
    "ksvm(x, data = NULL, y, scaled, type, kernel, …, С=1),\n",
    "\n",
    "где x - символическое описание модели;\n",
    "\n",
    "data – набор данных для обучения;\n",
    "\n",
    " y - вектор ответа с одной меткой для каждой строки  компонента x. Может быть либо фактором (для задач классификации), либо числовым вектором (для регрессии);\n",
    " \n",
    "scaled - логический вектор, указывающий масштабируемые переменные;\n",
    "\n",
    "type – задает режим работы: классификация, регрессия, обнаружение новизны, допустимые варианты: C-svc; Nu-SVC; C-bsvc; spoc-svc; kbb-svc; one-svc;  eps-svr;  nu-svr; eps-bsvr;\n",
    "\n",
    "kernel - функция ядра, используемая при обучении и прогнозировании. kernlab предоставляет наиболее популярные функции ядра, которые можно использовать, задав для параметра ядра следующие строки: rbfdot  - ядро с радиальным базисом \"Гауссово\"; polydot  - полиномиальное ядро; vanilladot - линейное ядро; tanhdot  - ядро гиперболического тангенса; laplacedot - ядро Лапласа; besseldot  - ядро Бесселя; anovadot  - ANOVA RBF ядро; splinedot -  сплайновое ядро; stringdot  - строковое ядро;\n",
    "\n",
    "kpar - список гиперпараметров (параметров ядра). Это список, который содержит параметры, которые будут использоваться с функцией ядра. Допустимые параметры для существующих ядер: sigma обратная ширина ядра для радиальной базисной ядерной функции \"rbfdot\" и ядра лапласа \"laplacedot\"; degree, scale, offset для полиномиального ядра \"polydot\"; scale, offset для функции ядра гиперболического тангенса \"tanhdot\"; sigma, order, degree для ядра Бесселя \"besseldot\"; sigma, degree для ядра ANOVA \"anovadot\"; length, lambda, normalized для ядра \"stringdot\";\n",
    "\n",
    "С – штраф за ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0636cc56-be50-41bf-992f-c59db2ceae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"kernlab\")\n",
    "library(kernlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3199443-3152-497e-9215-66c68d55a882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table width=\"100%\" summary=\"page for ksvm {kernlab}\"><tr><td>ksvm {kernlab}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2>Support Vector Machines</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Support Vector Machines are an excellent tool for classification, \n",
       "novelty detection, and regression. <code>ksvm</code> supports the \n",
       "well known C-svc, nu-svc, (classification) one-class-svc (novelty)\n",
       "eps-svr, nu-svr (regression) formulations along with \n",
       "native multi-class classification formulations and \n",
       "the bound-constraint SVM formulations.<br />\n",
       "<code>ksvm</code> also supports class-probabilities output and \n",
       "confidence intervals for regression.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre>\n",
       "## S4 method for signature 'formula'\n",
       "ksvm(x, data = NULL, ..., subset, na.action = na.omit, scaled = TRUE)\n",
       "\n",
       "## S4 method for signature 'vector'\n",
       "ksvm(x, ...)\n",
       "\n",
       "## S4 method for signature 'matrix'\n",
       "ksvm(x, y = NULL, scaled = TRUE, type = NULL,\n",
       "     kernel =\"rbfdot\", kpar = \"automatic\",\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ..., \n",
       "     subset, na.action = na.omit)\n",
       "\n",
       "## S4 method for signature 'kernelMatrix'\n",
       "ksvm(x, y = NULL, type = NULL,\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ...)\n",
       "\n",
       "## S4 method for signature 'list'\n",
       "ksvm(x, y = NULL, type = NULL,\n",
       "     kernel = \"stringdot\", kpar = list(length = 4, lambda = 0.5),\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ...,\n",
       "     na.action = na.omit)\n",
       "\n",
       "</pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table summary=\"R argblock\">\n",
       "<tr valign=\"top\"><td><code>x</code></td>\n",
       "<td>\n",
       "<p>a symbolic description of the model to be fit.  When not\n",
       "using a formula x can be a matrix or vector containing the training\n",
       "data \n",
       "or a kernel matrix of class <code>kernelMatrix</code> of the training data\n",
       "or a list of character vectors (for use with the string\n",
       "kernel). Note, that the intercept is always excluded, whether\n",
       "given in the formula or not.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>data</code></td>\n",
       "<td>\n",
       "<p>an optional data frame containing the training data, when using a formula.\n",
       "By default the data is taken from the environment which\n",
       "&lsquo;ksvm&rsquo; is called from.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>y</code></td>\n",
       "<td>\n",
       "<p>a response vector with one label for each row/component of <code>x</code>. Can be either\n",
       "a factor (for classification tasks) or a numeric vector (for\n",
       "regression).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>scaled</code></td>\n",
       "<td>\n",
       "<p>A logical vector indicating the variables to be\n",
       "scaled. If <code>scaled</code> is of length 1, the value is recycled as\n",
       "many times as needed and all non-binary variables are scaled.\n",
       "Per default, data are scaled internally (both <code>x</code> and <code>y</code>\n",
       "variables) to zero mean and unit variance. The center and scale\n",
       "values are returned and used for later predictions.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>type</code></td>\n",
       "<td>\n",
       "<p><code>ksvm</code> can be used for classification\n",
       ", for regression, or for novelty detection.\n",
       "Depending on whether <code>y</code> is\n",
       "a factor or not, the default setting for <code>type</code> is <code>C-svc</code>\n",
       "or <code>eps-svr</code>,\n",
       "respectively, but can be overwritten by setting an explicit value.<br />\n",
       "Valid options are:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li> <p><code>C-svc</code>   C classification\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>nu-svc</code>  nu classification\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>C-bsvc</code>  bound-constraint svm classification\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>spoc-svc</code>  Crammer, Singer native multi-class\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>kbb-svc</code>  Weston, Watkins native multi-class\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>one-svc</code>  novelty detection\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>eps-svr</code>  epsilon regression\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>nu-svr</code>   nu regression\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>eps-bsvr</code>  bound-constraint svm regression\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>kernel</code></td>\n",
       "<td>\n",
       "<p>the kernel function used in training and predicting.\n",
       "This parameter can be set to any function, of class kernel, which\n",
       "computes the inner product in feature space between two\n",
       "vector arguments (see <code>kernels</code>). <br />\n",
       "kernlab provides the most popular kernel functions\n",
       "which can be used by setting the kernel parameter to the following\n",
       "strings:\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li> <p><code>rbfdot</code> Radial Basis kernel &quot;Gaussian&quot;\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>polydot</code> Polynomial kernel\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>vanilladot</code> Linear kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>tanhdot</code> Hyperbolic tangent kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>laplacedot</code> Laplacian kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>besseldot</code> Bessel kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>anovadot</code> ANOVA RBF kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>splinedot</code> Spline kernel \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>stringdot</code> String kernel \n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "<p>Setting the kernel parameter to &quot;matrix&quot; treats <code>x</code> as a kernel\n",
       "matrix calling the <code>kernelMatrix</code> interface.<br />\n",
       "</p>\n",
       "<p>The kernel parameter can also be set to a user defined function of\n",
       "class kernel by passing the function name as an argument.\n",
       "</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>kpar</code></td>\n",
       "<td>\n",
       "<p>the list of hyper-parameters (kernel parameters).\n",
       "This is a list which contains the parameters to be used with the\n",
       "kernel function. For valid parameters for existing kernels are :\n",
       "</p>\n",
       "\n",
       "<ul>\n",
       "<li> <p><code>sigma</code> inverse kernel width for the Radial Basis\n",
       "kernel function &quot;rbfdot&quot; and the Laplacian kernel &quot;laplacedot&quot;.\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>degree, scale, offset</code> for the Polynomial kernel &quot;polydot&quot;\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>scale, offset</code> for the Hyperbolic tangent kernel\n",
       "function &quot;tanhdot&quot;\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>sigma, order, degree</code> for the Bessel kernel &quot;besseldot&quot;. \n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>sigma, degree</code> for the ANOVA kernel &quot;anovadot&quot;.\n",
       "</p>\n",
       "</li>\n",
       "<li> <p><code>length, lambda, normalized</code> for the &quot;stringdot&quot; kernel\n",
       "where length is the length of the strings considered, lambda the\n",
       "decay factor and normalized a logical parameter determining if the\n",
       "kernel evaluations should be normalized.\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "<p>Hyper-parameters for user defined kernels can be passed through the\n",
       "kpar parameter as well. In the case of a Radial Basis kernel function (Gaussian)\n",
       "kpar can also be set to the string &quot;automatic&quot; which uses the heuristics in \n",
       "<code>sigest</code> to calculate a good <code>sigma</code> value for the\n",
       "Gaussian RBF or Laplace kernel, from the data.\n",
       "(default = &quot;automatic&quot;).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>C</code></td>\n",
       "<td>\n",
       "<p>cost of constraints violation (default: 1) this is the\n",
       "&lsquo;C&rsquo;-constant of the regularization term in the Lagrange\n",
       "formulation.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nu</code></td>\n",
       "<td>\n",
       "<p>parameter needed for <code>nu-svc</code>,\n",
       "<code>one-svc</code>, and <code>nu-svr</code>. The <code>nu</code>\n",
       "parameter sets the upper bound on the training error and the lower\n",
       "bound on the fraction of data points to become Support Vectors (default: 0.2).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>epsilon</code></td>\n",
       "<td>\n",
       "<p>epsilon in the insensitive-loss function used for\n",
       "<code>eps-svr</code>, <code>nu-svr</code> and <code>eps-bsvm</code> (default: 0.1)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>prob.model</code></td>\n",
       "<td>\n",
       "<p>if set to <code>TRUE</code> builds a model for calculating class\n",
       "probabilities or in case of regression, calculates the scaling\n",
       "parameter of the Laplacian distribution fitted on the residuals.\n",
       "Fitting is done  on output data created by performing a\n",
       "3-fold cross-validation on the training data. For details see\n",
       "references. (default: <code>FALSE</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>class.weights</code></td>\n",
       "<td>\n",
       "<p>a named vector of weights for the different\n",
       "classes, used for asymmetric class sizes. Not all factor levels have\n",
       "to be supplied (default weight: 1). All components have to be named.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cache</code></td>\n",
       "<td>\n",
       "<p>cache memory in MB (default 40)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>tol</code></td>\n",
       "<td>\n",
       "<p>tolerance of termination criterion (default: 0.001)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>shrinking</code></td>\n",
       "<td>\n",
       "<p>option whether to use the shrinking-heuristics\n",
       "(default: <code>TRUE</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cross</code></td>\n",
       "<td>\n",
       "<p>if a integer value k&gt;0 is specified, a k-fold cross\n",
       "validation on the training data is performed to assess the quality\n",
       "of the model: the accuracy rate for classification and the Mean\n",
       "Squared Error for regression</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>fit</code></td>\n",
       "<td>\n",
       "<p>indicates whether the fitted values should be computed\n",
       "and included in the model or not (default: <code>TRUE</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>...</code></td>\n",
       "<td>\n",
       "<p>additional parameters for the low level fitting function</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>subset</code></td>\n",
       "<td>\n",
       "<p>An index vector specifying the cases to be used in the\n",
       "training sample.  (NOTE: If given, this argument must be\n",
       "named.)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>na.action</code></td>\n",
       "<td>\n",
       "<p>A function to specify the action to be taken if <code>NA</code>s are\n",
       "found. The default action is <code>na.omit</code>, which leads to rejection of cases\n",
       "with missing values on any required variable. An alternative\n",
       "is <code>na.fail</code>, which causes an error if <code>NA</code> cases\n",
       "are found. (NOTE: If given, this argument must be named.)</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p><code>ksvm</code> uses John Platt's SMO algorithm for solving the SVM QP problem an\n",
       "most SVM formulations. On the <code>spoc-svc</code>, <code>kbb-svc</code>, <code>C-bsvc</code> and\n",
       "<code>eps-bsvr</code> formulations a chunking algorithm based on the TRON QP\n",
       "solver is used. <br />\n",
       "For multiclass-classification with <i>k</i> classes, <i>k &gt; 2</i>, <code>ksvm</code> uses the\n",
       "&lsquo;one-against-one&rsquo;-approach, in which <i>k(k-1)/2</i> binary classifiers are\n",
       "trained; the appropriate class is found by a voting scheme,\n",
       "The <code>spoc-svc</code> and the <code>kbb-svc</code> formulations deal with the\n",
       "multiclass-classification problems by solving a single quadratic problem involving all the classes.<br />\n",
       "If the predictor variables include factors, the formula interface must be used to get a\n",
       "correct model matrix. <br />\n",
       "In classification when <code>prob.model</code> is <code>TRUE</code> a 3-fold cross validation is\n",
       "performed on the data and a sigmoid function is fitted on the\n",
       "resulting decision values <i>f</i>.\n",
       "The data can be passed to the <code>ksvm</code> function in a <code>matrix</code> or a\n",
       "<code>data.frame</code>, in addition <code>ksvm</code> also supports input in the form of a\n",
       "kernel matrix of class <code>kernelMatrix</code> or as a list of character\n",
       "vectors where a string kernel has to be used.<br />\n",
       "The <code>plot</code> function for binary classification <code>ksvm</code> objects\n",
       "displays a contour plot of the decision values with the corresponding\n",
       "support vectors highlighted.<br />\n",
       "The predict function can return class probabilities for \n",
       "classification problems by setting the <code>type</code> parameter to\n",
       "&quot;probabilities&quot;. <br />\n",
       "The problem of model selection is partially addressed by an empirical\n",
       "observation for the RBF kernels (Gaussian , Laplace) where the optimal values of the\n",
       "<i>sigma</i> width parameter are shown to lie in between the 0.1 and 0.9\n",
       "quantile of the <i>\\|x- x'\\|</i> statistics. When using an RBF kernel\n",
       "and setting <code>kpar</code> to &quot;automatic&quot;, <code>ksvm</code> uses the <code>sigest</code> function\n",
       "to estimate the quantiles and uses the median of the values.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>An S4 object of class <code>\"ksvm\"</code> containing the fitted model,\n",
       "Accessor functions can be used to access the slots of the object (see\n",
       "examples) which include:\n",
       "</p>\n",
       "<table summary=\"R valueblock\">\n",
       "<tr valign=\"top\"><td><code>alpha</code></td>\n",
       "<td>\n",
       "<p>The resulting support vectors, (alpha vector) (possibly scaled).</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>alphaindex</code></td>\n",
       "<td>\n",
       "<p>The index of the resulting support vectors in the data\n",
       "matrix. Note that this index refers to the pre-processed data (after\n",
       "the possible effect of <code>na.omit</code> and <code>subset</code>)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>coef</code></td>\n",
       "<td>\n",
       "<p>The corresponding coefficients times the training labels.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>b</code></td>\n",
       "<td>\n",
       "<p>The negative intercept.</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>nSV</code></td>\n",
       "<td>\n",
       "<p>The number of Support Vectors</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>obj</code></td>\n",
       "<td>\n",
       "<p>The value of the objective function. In case of one-against-one classification this is a vector of values</p>\n",
       "</td></tr> \n",
       "<tr valign=\"top\"><td><code>error</code></td>\n",
       "<td>\n",
       "<p>Training error</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>cross</code></td>\n",
       "<td>\n",
       "<p>Cross validation error, (when cross &gt; 0)</p>\n",
       "</td></tr>\n",
       "<tr valign=\"top\"><td><code>prob.model</code></td>\n",
       "<td>\n",
       "<p>Contains the width of the Laplacian fitted on the\n",
       "residuals in case of regression, or the parameters of the sigmoid\n",
       "fitted on the decision values in case of classification.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Note</h3>\n",
       "\n",
       "<p>Data is scaled internally by default, usually yielding better results.</p>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Alexandros Karatzoglou (SMO optimizers in C++ by Chih-Chung Chang &amp; Chih-Jen Lin)<br />\n",
       "<a href=\"mailto:alexandros.karatzoglou@ci.tuwien.ac.at\">alexandros.karatzoglou@ci.tuwien.ac.at</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "\n",
       "<ul>\n",
       "<li>\n",
       "<p>Chang Chih-Chung, Lin Chih-Jen<br />\n",
       "<em>LIBSVM: a library for Support Vector Machines</em><br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/libsvm\">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>Chih-Wei Hsu, Chih-Jen Lin<br />\n",
       "<em>BSVM</em>\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/bsvm/\">http://www.csie.ntu.edu.tw/~cjlin/bsvm/</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>J. Platt<br />\n",
       "<em>Probabilistic outputs for support vector machines and comparison to regularized likelihood methods</em> <br />\n",
       "Advances in Large Margin Classifiers, A. Smola, P. Bartlett, B. Schoelkopf and D. Schuurmans, Eds. Cambridge, MA: MIT Press, 2000.<br />\n",
       "<a href=\"http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639\">http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>H.-T. Lin, C.-J. Lin and R. C. Weng<br />\n",
       "<em>A note on Platt's probabilistic outputs for support vector machines</em><br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf\">http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>C.-W. Hsu and C.-J. Lin <br />\n",
       "<em>A comparison on methods for multi-class support vector machines</em><br />\n",
       "IEEE Transactions on Neural Networks, 13(2002) 415-425.<br />\n",
       "<a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.ps.gz\">http://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.ps.gz</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>K. Crammer, Y. Singer<br />\n",
       "<em>On the learnability and design of output codes for multiclass prolems</em><br />\n",
       "Computational Learning Theory, 35-46, 2000.<br />\n",
       "<a href=\"http://webee.technion.ac.il/people/koby/publications/ecoc-mlj02.pdf\">http://webee.technion.ac.il/people/koby/publications/ecoc-mlj02.pdf</a>\n",
       "</p>\n",
       "</li>\n",
       "<li>\n",
       "<p>J. Weston, C. Watkins<br />\n",
       "<em>Multi-class support vector machines</em> <br />\n",
       "In M. Verleysen, Proceedings of ESANN99 Brussels, 1999<br />\n",
       "<a href=\"http://citeseer.ist.psu.edu/8884.html\">http://citeseer.ist.psu.edu/8884.html</a>\n",
       "</p>\n",
       "</li></ul>\n",
       "\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>predict.ksvm</code>, <code>ksvm-class</code>, <code>couple</code> </p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre>\n",
       "\n",
       "## simple example using the spam data set\n",
       "data(spam)\n",
       "\n",
       "## create test and training set\n",
       "index &lt;- sample(1:dim(spam)[1])\n",
       "spamtrain &lt;- spam[index[1:floor(dim(spam)[1]/2)], ]\n",
       "spamtest &lt;- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]\n",
       "\n",
       "## train a support vector machine\n",
       "filter &lt;- ksvm(type~.,data=spamtrain,kernel=\"rbfdot\",\n",
       "               kpar=list(sigma=0.05),C=5,cross=3)\n",
       "filter\n",
       "\n",
       "## predict mail type on the test set\n",
       "mailtype &lt;- predict(filter,spamtest[,-58])\n",
       "\n",
       "## Check results\n",
       "table(mailtype,spamtest[,58])\n",
       "\n",
       "\n",
       "## Another example with the famous iris data\n",
       "data(iris)\n",
       "\n",
       "## Create a kernel function using the build in rbfdot function\n",
       "rbf &lt;- rbfdot(sigma=0.1)\n",
       "rbf\n",
       "\n",
       "## train a bound constraint support vector machine\n",
       "irismodel &lt;- ksvm(Species~.,data=iris,type=\"C-bsvc\",\n",
       "                  kernel=rbf,C=10,prob.model=TRUE)\n",
       "\n",
       "irismodel\n",
       "\n",
       "## get fitted values\n",
       "fitted(irismodel)\n",
       "\n",
       "## Test on the training set with probabilities as output\n",
       "predict(irismodel, iris[,-5], type=\"probabilities\")\n",
       "\n",
       "\n",
       "## Demo of the plot function\n",
       "x &lt;- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))\n",
       "y &lt;- matrix(c(rep(1,60),rep(-1,60)))\n",
       "\n",
       "svp &lt;- ksvm(x,y,type=\"C-svc\")\n",
       "plot(svp,data=x)\n",
       "\n",
       "\n",
       "### Use kernelMatrix\n",
       "K &lt;- as.kernelMatrix(crossprod(t(x)))\n",
       "\n",
       "svp2 &lt;- ksvm(K, y, type=\"C-svc\")\n",
       "\n",
       "svp2\n",
       "\n",
       "# test data\n",
       "xtest &lt;- rbind(matrix(rnorm(20),,2),matrix(rnorm(20,mean=3),,2))\n",
       "# test kernel matrix i.e. inner/kernel product of test data with\n",
       "# Support Vectors\n",
       "\n",
       "Ktest &lt;- as.kernelMatrix(crossprod(t(xtest),t(x[SVindex(svp2), ])))\n",
       "\n",
       "predict(svp2, Ktest)\n",
       "\n",
       "\n",
       "#### Use custom kernel \n",
       "\n",
       "k &lt;- function(x,y) {(sum(x*y) +1)*exp(-0.001*sum((x-y)^2))}\n",
       "class(k) &lt;- \"kernel\"\n",
       "\n",
       "data(promotergene)\n",
       "\n",
       "## train svm using custom kernel\n",
       "gene &lt;- ksvm(Class~.,data=promotergene[c(1:20, 80:100),],kernel=k,\n",
       "             C=5,cross=5)\n",
       "\n",
       "gene\n",
       "\n",
       "\n",
       "#### Use text with string kernels\n",
       "data(reuters)\n",
       "is(reuters)\n",
       "tsv &lt;- ksvm(reuters,rlabels,kernel=\"stringdot\",\n",
       "            kpar=list(length=5),cross=3,C=10)\n",
       "tsv\n",
       "\n",
       "\n",
       "## regression\n",
       "# create data\n",
       "x &lt;- seq(-20,20,0.1)\n",
       "y &lt;- sin(x)/x + rnorm(401,sd=0.03)\n",
       "\n",
       "# train support vector machine\n",
       "regm &lt;- ksvm(x,y,epsilon=0.01,kpar=list(sigma=16),cross=3)\n",
       "plot(x,y,type=\"l\")\n",
       "lines(x,predict(regm,x),col=\"red\")\n",
       "</pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>kernlab</em> version 0.9-29 ]</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{ksvm}{Support Vector Machines}{ksvm}\n",
       "\\aliasA{coef,ksvm-method}{ksvm}{coef,ksvm.Rdash.method}\n",
       "\\aliasA{ksvm,formula-method}{ksvm}{ksvm,formula.Rdash.method}\n",
       "\\aliasA{ksvm,kernelMatrix-method}{ksvm}{ksvm,kernelMatrix.Rdash.method}\n",
       "\\aliasA{ksvm,list-method}{ksvm}{ksvm,list.Rdash.method}\n",
       "\\aliasA{ksvm,matrix-method}{ksvm}{ksvm,matrix.Rdash.method}\n",
       "\\aliasA{ksvm,vector-method}{ksvm}{ksvm,vector.Rdash.method}\n",
       "\\aliasA{show,ksvm-method}{ksvm}{show,ksvm.Rdash.method}\n",
       "\\keyword{methods}{ksvm}\n",
       "\\keyword{regression}{ksvm}\n",
       "\\keyword{nonlinear}{ksvm}\n",
       "\\keyword{classif}{ksvm}\n",
       "\\keyword{neural}{ksvm}\n",
       "%\n",
       "\\begin{Description}\\relax\n",
       "Support Vector Machines are an excellent tool for classification, \n",
       "novelty detection, and regression. \\code{ksvm} supports the \n",
       "well known C-svc, nu-svc, (classification) one-class-svc (novelty)\n",
       "eps-svr, nu-svr (regression) formulations along with \n",
       "native multi-class classification formulations and \n",
       "the bound-constraint SVM formulations.\\\\{}\n",
       "\\code{ksvm} also supports class-probabilities output and \n",
       "confidence intervals for regression.\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "## S4 method for signature 'formula'\n",
       "ksvm(x, data = NULL, ..., subset, na.action = na.omit, scaled = TRUE)\n",
       "\n",
       "## S4 method for signature 'vector'\n",
       "ksvm(x, ...)\n",
       "\n",
       "## S4 method for signature 'matrix'\n",
       "ksvm(x, y = NULL, scaled = TRUE, type = NULL,\n",
       "     kernel =\"rbfdot\", kpar = \"automatic\",\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ..., \n",
       "     subset, na.action = na.omit)\n",
       "\n",
       "## S4 method for signature 'kernelMatrix'\n",
       "ksvm(x, y = NULL, type = NULL,\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ...)\n",
       "\n",
       "## S4 method for signature 'list'\n",
       "ksvm(x, y = NULL, type = NULL,\n",
       "     kernel = \"stringdot\", kpar = list(length = 4, lambda = 0.5),\n",
       "     C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "     class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "     tol = 0.001, shrinking = TRUE, ...,\n",
       "     na.action = na.omit)\n",
       "\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{x}] a symbolic description of the model to be fit.  When not\n",
       "using a formula x can be a matrix or vector containing the training\n",
       "data \n",
       "or a kernel matrix of class \\code{kernelMatrix} of the training data\n",
       "or a list of character vectors (for use with the string\n",
       "kernel). Note, that the intercept is always excluded, whether\n",
       "given in the formula or not.\n",
       "\n",
       "\\item[\\code{data}] an optional data frame containing the training data, when using a formula.\n",
       "By default the data is taken from the environment which\n",
       "`ksvm' is called from.\n",
       "\n",
       "\\item[\\code{y}] a response vector with one label for each row/component of \\code{x}. Can be either\n",
       "a factor (for classification tasks) or a numeric vector (for\n",
       "regression).\n",
       "\n",
       "\\item[\\code{scaled}] A logical vector indicating the variables to be\n",
       "scaled. If \\code{scaled} is of length 1, the value is recycled as\n",
       "many times as needed and all non-binary variables are scaled.\n",
       "Per default, data are scaled internally (both \\code{x} and \\code{y}\n",
       "variables) to zero mean and unit variance. The center and scale\n",
       "values are returned and used for later predictions.\n",
       "\n",
       "\\item[\\code{type}] \\code{ksvm} can be used for classification\n",
       ", for regression, or for novelty detection.\n",
       "Depending on whether \\code{y} is\n",
       "a factor or not, the default setting for \\code{type} is \\code{C-svc}\n",
       "or \\code{eps-svr},\n",
       "respectively, but can be overwritten by setting an explicit value.\\\\{}\n",
       "Valid options are:\n",
       "\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \\code{C-svc}   C classification\n",
       "\n",
       "\\item \\code{nu-svc}  nu classification\n",
       "\n",
       "\\item \\code{C-bsvc}  bound-constraint svm classification\n",
       "\n",
       "\\item \\code{spoc-svc}  Crammer, Singer native multi-class\n",
       "\n",
       "\\item \\code{kbb-svc}  Weston, Watkins native multi-class\n",
       "\n",
       "\\item \\code{one-svc}  novelty detection\n",
       "\n",
       "\\item \\code{eps-svr}  epsilon regression\n",
       "\n",
       "\\item \\code{nu-svr}   nu regression\n",
       "\n",
       "\\item \\code{eps-bsvr}  bound-constraint svm regression\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "\n",
       "\\item[\\code{kernel}] the kernel function used in training and predicting.\n",
       "This parameter can be set to any function, of class kernel, which\n",
       "computes the inner product in feature space between two\n",
       "vector arguments (see \\code{\\LinkA{kernels}{kernels}}). \\\\{}\n",
       "kernlab provides the most popular kernel functions\n",
       "which can be used by setting the kernel parameter to the following\n",
       "strings:\n",
       "\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \\code{rbfdot} Radial Basis kernel \"Gaussian\"\n",
       "\n",
       "\\item \\code{polydot} Polynomial kernel\n",
       "\n",
       "\\item \\code{vanilladot} Linear kernel \n",
       "\n",
       "\\item \\code{tanhdot} Hyperbolic tangent kernel \n",
       "\n",
       "\\item \\code{laplacedot} Laplacian kernel \n",
       "\n",
       "\\item \\code{besseldot} Bessel kernel \n",
       "\n",
       "\\item \\code{anovadot} ANOVA RBF kernel \n",
       "\n",
       "\\item \\code{splinedot} Spline kernel \n",
       "\n",
       "\\item \\code{stringdot} String kernel \n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "Setting the kernel parameter to \"matrix\" treats \\code{x} as a kernel\n",
       "matrix calling the \\code{kernelMatrix} interface.\\\\{}\n",
       "\n",
       "The kernel parameter can also be set to a user defined function of\n",
       "class kernel by passing the function name as an argument.\n",
       "\n",
       "\n",
       "\\item[\\code{kpar}] the list of hyper-parameters (kernel parameters).\n",
       "This is a list which contains the parameters to be used with the\n",
       "kernel function. For valid parameters for existing kernels are :\n",
       "\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \\code{sigma} inverse kernel width for the Radial Basis\n",
       "kernel function \"rbfdot\" and the Laplacian kernel \"laplacedot\".\n",
       "\n",
       "\\item \\code{degree, scale, offset} for the Polynomial kernel \"polydot\"\n",
       "\n",
       "\\item \\code{scale, offset} for the Hyperbolic tangent kernel\n",
       "function \"tanhdot\"\n",
       "\n",
       "\\item \\code{sigma, order, degree} for the Bessel kernel \"besseldot\". \n",
       "\n",
       "\\item \\code{sigma, degree} for the ANOVA kernel \"anovadot\".\n",
       "\n",
       "\\item \\code{length, lambda, normalized} for the \"stringdot\" kernel\n",
       "where length is the length of the strings considered, lambda the\n",
       "decay factor and normalized a logical parameter determining if the\n",
       "kernel evaluations should be normalized.\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\n",
       "Hyper-parameters for user defined kernels can be passed through the\n",
       "kpar parameter as well. In the case of a Radial Basis kernel function (Gaussian)\n",
       "kpar can also be set to the string \"automatic\" which uses the heuristics in \n",
       "\\code{\\LinkA{sigest}{sigest}} to calculate a good \\code{sigma} value for the\n",
       "Gaussian RBF or Laplace kernel, from the data.\n",
       "(default = \"automatic\").\n",
       "\n",
       "\\item[\\code{C}] cost of constraints violation (default: 1) this is the\n",
       "`C'-constant of the regularization term in the Lagrange\n",
       "formulation.\n",
       "\n",
       "\\item[\\code{nu}] parameter needed for \\code{nu-svc},\n",
       "\\code{one-svc}, and \\code{nu-svr}. The \\code{nu}\n",
       "parameter sets the upper bound on the training error and the lower\n",
       "bound on the fraction of data points to become Support Vectors (default: 0.2).\n",
       "\n",
       "\\item[\\code{epsilon}] epsilon in the insensitive-loss function used for\n",
       "\\code{eps-svr}, \\code{nu-svr} and \\code{eps-bsvm} (default: 0.1)\n",
       "\n",
       "\\item[\\code{prob.model}] if set to \\code{TRUE} builds a model for calculating class\n",
       "probabilities or in case of regression, calculates the scaling\n",
       "parameter of the Laplacian distribution fitted on the residuals.\n",
       "Fitting is done  on output data created by performing a\n",
       "3-fold cross-validation on the training data. For details see\n",
       "references. (default: \\code{FALSE})\n",
       "\n",
       "\\item[\\code{class.weights}] a named vector of weights for the different\n",
       "classes, used for asymmetric class sizes. Not all factor levels have\n",
       "to be supplied (default weight: 1). All components have to be named.\n",
       "\n",
       "\\item[\\code{cache}] cache memory in MB (default 40)\n",
       "\n",
       "\\item[\\code{tol}] tolerance of termination criterion (default: 0.001)\n",
       "\n",
       "\\item[\\code{shrinking}] option whether to use the shrinking-heuristics\n",
       "(default: \\code{TRUE})\n",
       "\n",
       "\\item[\\code{cross}] if a integer value k>0 is specified, a k-fold cross\n",
       "validation on the training data is performed to assess the quality\n",
       "of the model: the accuracy rate for classification and the Mean\n",
       "Squared Error for regression\n",
       "\n",
       "\\item[\\code{fit}] indicates whether the fitted values should be computed\n",
       "and included in the model or not (default: \\code{TRUE})\n",
       "\n",
       "\\item[\\code{...}] additional parameters for the low level fitting function\n",
       "\n",
       "\\item[\\code{subset}] An index vector specifying the cases to be used in the\n",
       "training sample.  (NOTE: If given, this argument must be\n",
       "named.)\n",
       "\n",
       "\\item[\\code{na.action}] A function to specify the action to be taken if \\code{NA}s are\n",
       "found. The default action is \\code{na.omit}, which leads to rejection of cases\n",
       "with missing values on any required variable. An alternative\n",
       "is \\code{na.fail}, which causes an error if \\code{NA} cases\n",
       "are found. (NOTE: If given, this argument must be named.)\n",
       "\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\\relax\n",
       "\\code{ksvm} uses John Platt's SMO algorithm for solving the SVM QP problem an\n",
       "most SVM formulations. On the \\code{spoc-svc}, \\code{kbb-svc}, \\code{C-bsvc} and\n",
       "\\code{eps-bsvr} formulations a chunking algorithm based on the TRON QP\n",
       "solver is used. \\\\{}\n",
       "For multiclass-classification with \\eqn{k}{} classes, \\eqn{k > 2}{}, \\code{ksvm} uses the\n",
       "`one-against-one'-approach, in which \\eqn{k(k-1)/2}{} binary classifiers are\n",
       "trained; the appropriate class is found by a voting scheme,\n",
       "The \\code{spoc-svc} and the \\code{kbb-svc} formulations deal with the\n",
       "multiclass-classification problems by solving a single quadratic problem involving all the classes.\\\\{}\n",
       "If the predictor variables include factors, the formula interface must be used to get a\n",
       "correct model matrix. \\\\{}\n",
       "In classification when \\code{prob.model} is \\code{TRUE} a 3-fold cross validation is\n",
       "performed on the data and a sigmoid function is fitted on the\n",
       "resulting decision values \\eqn{f}{}.\n",
       "The data can be passed to the \\code{ksvm} function in a \\code{matrix} or a\n",
       "\\code{data.frame}, in addition \\code{ksvm} also supports input in the form of a\n",
       "kernel matrix of class \\code{kernelMatrix} or as a list of character\n",
       "vectors where a string kernel has to be used.\\\\{}\n",
       "The \\code{plot} function for binary classification \\code{ksvm} objects\n",
       "displays a contour plot of the decision values with the corresponding\n",
       "support vectors highlighted.\\\\{}\n",
       "The predict function can return class probabilities for \n",
       "classification problems by setting the \\code{type} parameter to\n",
       "\"probabilities\". \\\\{}\n",
       "The problem of model selection is partially addressed by an empirical\n",
       "observation for the RBF kernels (Gaussian , Laplace) where the optimal values of the\n",
       "\\eqn{sigma}{} width parameter are shown to lie in between the 0.1 and 0.9\n",
       "quantile of the \\eqn{\\|x- x'\\|}{} statistics. When using an RBF kernel\n",
       "and setting \\code{kpar} to \"automatic\", \\code{ksvm} uses the \\code{sigest} function\n",
       "to estimate the quantiles and uses the median of the values.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "An S4 object of class \\code{\"ksvm\"} containing the fitted model,\n",
       "Accessor functions can be used to access the slots of the object (see\n",
       "examples) which include:\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{alpha}] The resulting support vectors, (alpha vector) (possibly scaled).\n",
       "\\item[\\code{alphaindex}] The index of the resulting support vectors in the data\n",
       "matrix. Note that this index refers to the pre-processed data (after\n",
       "the possible effect of \\code{na.omit} and \\code{subset})\n",
       "\\item[\\code{coef}] The corresponding coefficients times the training labels.\n",
       "\\item[\\code{b}] The negative intercept.\n",
       "\\item[\\code{nSV}] The number of Support Vectors\n",
       "\\item[\\code{obj}] The value of the objective function. In case of one-against-one classification this is a vector of values\n",
       "\\item[\\code{error}] Training error\n",
       "\\item[\\code{cross}] Cross validation error, (when cross > 0)\n",
       "\\item[\\code{prob.model}] Contains the width of the Laplacian fitted on the\n",
       "residuals in case of regression, or the parameters of the sigmoid\n",
       "fitted on the decision values in case of classification.\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Note}\\relax\n",
       "Data is scaled internally by default, usually yielding better results.\n",
       "\\end{Note}\n",
       "%\n",
       "\\begin{Author}\\relax\n",
       "Alexandros Karatzoglou (SMO optimizers in C++ by Chih-Chung Chang \\& Chih-Jen Lin)\\\\{}\n",
       "\\email{alexandros.karatzoglou@ci.tuwien.ac.at}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\\relax\n",
       "\\begin{itemize}\n",
       "\n",
       "\\item \n",
       "Chang Chih-Chung, Lin Chih-Jen\\\\{}\n",
       "\\emph{LIBSVM: a library for Support Vector Machines}\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}\n",
       "\n",
       "\\item \n",
       "Chih-Wei Hsu, Chih-Jen Lin\\\\{}\n",
       "\\emph{BSVM}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/bsvm/}\n",
       "\n",
       "\\item \n",
       "J. Platt\\\\{}\n",
       "\\emph{Probabilistic outputs for support vector machines and comparison to regularized likelihood methods} \\\\{}\n",
       "Advances in Large Margin Classifiers, A. Smola, P. Bartlett, B. Schoelkopf and D. Schuurmans, Eds. Cambridge, MA: MIT Press, 2000.\\\\{}\n",
       "\\url{http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639}\n",
       "\n",
       "\\item \n",
       "H.-T. Lin, C.-J. Lin and R. C. Weng\\\\{}\n",
       "\\emph{A note on Platt's probabilistic outputs for support vector machines}\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf}\n",
       "\n",
       "\\item \n",
       "C.-W. Hsu and C.-J. Lin \\\\{}\n",
       "\\emph{A comparison on methods for multi-class support vector machines}\\\\{}\n",
       "IEEE Transactions on Neural Networks, 13(2002) 415-425.\\\\{}\n",
       "\\url{http://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.ps.gz}\n",
       "\n",
       "\\item \n",
       "K. Crammer, Y. Singer\\\\{}\n",
       "\\emph{On the learnability and design of output codes for multiclass prolems}\\\\{}\n",
       "Computational Learning Theory, 35-46, 2000.\\\\{}\n",
       "\\url{http://webee.technion.ac.il/people/koby/publications/ecoc-mlj02.pdf}\n",
       "\n",
       "\\item \n",
       "J. Weston, C. Watkins\\\\{}\n",
       "\\emph{Multi-class support vector machines} \\\\{}\n",
       "In M. Verleysen, Proceedings of ESANN99 Brussels, 1999\\\\{}\n",
       "\\url{http://citeseer.ist.psu.edu/8884.html}\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\\relax\n",
       "\\code{\\LinkA{predict.ksvm}{predict.ksvm}}, \\code{\\LinkA{ksvm-class}{ksvm.Rdash.class}}, \\code{\\LinkA{couple}{couple}} \n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "\n",
       "## simple example using the spam data set\n",
       "data(spam)\n",
       "\n",
       "## create test and training set\n",
       "index <- sample(1:dim(spam)[1])\n",
       "spamtrain <- spam[index[1:floor(dim(spam)[1]/2)], ]\n",
       "spamtest <- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]\n",
       "\n",
       "## train a support vector machine\n",
       "filter <- ksvm(type~.,data=spamtrain,kernel=\"rbfdot\",\n",
       "               kpar=list(sigma=0.05),C=5,cross=3)\n",
       "filter\n",
       "\n",
       "## predict mail type on the test set\n",
       "mailtype <- predict(filter,spamtest[,-58])\n",
       "\n",
       "## Check results\n",
       "table(mailtype,spamtest[,58])\n",
       "\n",
       "\n",
       "## Another example with the famous iris data\n",
       "data(iris)\n",
       "\n",
       "## Create a kernel function using the build in rbfdot function\n",
       "rbf <- rbfdot(sigma=0.1)\n",
       "rbf\n",
       "\n",
       "## train a bound constraint support vector machine\n",
       "irismodel <- ksvm(Species~.,data=iris,type=\"C-bsvc\",\n",
       "                  kernel=rbf,C=10,prob.model=TRUE)\n",
       "\n",
       "irismodel\n",
       "\n",
       "## get fitted values\n",
       "fitted(irismodel)\n",
       "\n",
       "## Test on the training set with probabilities as output\n",
       "predict(irismodel, iris[,-5], type=\"probabilities\")\n",
       "\n",
       "\n",
       "## Demo of the plot function\n",
       "x <- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))\n",
       "y <- matrix(c(rep(1,60),rep(-1,60)))\n",
       "\n",
       "svp <- ksvm(x,y,type=\"C-svc\")\n",
       "plot(svp,data=x)\n",
       "\n",
       "\n",
       "### Use kernelMatrix\n",
       "K <- as.kernelMatrix(crossprod(t(x)))\n",
       "\n",
       "svp2 <- ksvm(K, y, type=\"C-svc\")\n",
       "\n",
       "svp2\n",
       "\n",
       "# test data\n",
       "xtest <- rbind(matrix(rnorm(20),,2),matrix(rnorm(20,mean=3),,2))\n",
       "# test kernel matrix i.e. inner/kernel product of test data with\n",
       "# Support Vectors\n",
       "\n",
       "Ktest <- as.kernelMatrix(crossprod(t(xtest),t(x[SVindex(svp2), ])))\n",
       "\n",
       "predict(svp2, Ktest)\n",
       "\n",
       "\n",
       "#### Use custom kernel \n",
       "\n",
       "k <- function(x,y) {(sum(x*y) +1)*exp(-0.001*sum((x-y)^2))}\n",
       "class(k) <- \"kernel\"\n",
       "\n",
       "data(promotergene)\n",
       "\n",
       "## train svm using custom kernel\n",
       "gene <- ksvm(Class~.,data=promotergene[c(1:20, 80:100),],kernel=k,\n",
       "             C=5,cross=5)\n",
       "\n",
       "gene\n",
       "\n",
       "\n",
       "#### Use text with string kernels\n",
       "data(reuters)\n",
       "is(reuters)\n",
       "tsv <- ksvm(reuters,rlabels,kernel=\"stringdot\",\n",
       "            kpar=list(length=5),cross=3,C=10)\n",
       "tsv\n",
       "\n",
       "\n",
       "## regression\n",
       "# create data\n",
       "x <- seq(-20,20,0.1)\n",
       "y <- sin(x)/x + rnorm(401,sd=0.03)\n",
       "\n",
       "# train support vector machine\n",
       "regm <- ksvm(x,y,epsilon=0.01,kpar=list(sigma=16),cross=3)\n",
       "plot(x,y,type=\"l\")\n",
       "lines(x,predict(regm,x),col=\"red\")\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "ksvm                  package:kernlab                  R Documentation\n",
       "\n",
       "_\bS_\bu_\bp_\bp_\bo_\br_\bt _\bV_\be_\bc_\bt_\bo_\br _\bM_\ba_\bc_\bh_\bi_\bn_\be_\bs\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Support Vector Machines are an excellent tool for classification,\n",
       "     novelty detection, and regression. ‘ksvm’ supports the well known\n",
       "     C-svc, nu-svc, (classification) one-class-svc (novelty) eps-svr,\n",
       "     nu-svr (regression) formulations along with native multi-class\n",
       "     classification formulations and the bound-constraint SVM\n",
       "     formulations.\n",
       "     ‘ksvm’ also supports class-probabilities output and confidence\n",
       "     intervals for regression.\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     ## S4 method for signature 'formula'\n",
       "     ksvm(x, data = NULL, ..., subset, na.action = na.omit, scaled = TRUE)\n",
       "     \n",
       "     ## S4 method for signature 'vector'\n",
       "     ksvm(x, ...)\n",
       "     \n",
       "     ## S4 method for signature 'matrix'\n",
       "     ksvm(x, y = NULL, scaled = TRUE, type = NULL,\n",
       "          kernel =\"rbfdot\", kpar = \"automatic\",\n",
       "          C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "          class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "          tol = 0.001, shrinking = TRUE, ..., \n",
       "          subset, na.action = na.omit)\n",
       "     \n",
       "     ## S4 method for signature 'kernelMatrix'\n",
       "     ksvm(x, y = NULL, type = NULL,\n",
       "          C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "          class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "          tol = 0.001, shrinking = TRUE, ...)\n",
       "     \n",
       "     ## S4 method for signature 'list'\n",
       "     ksvm(x, y = NULL, type = NULL,\n",
       "          kernel = \"stringdot\", kpar = list(length = 4, lambda = 0.5),\n",
       "          C = 1, nu = 0.2, epsilon = 0.1, prob.model = FALSE,\n",
       "          class.weights = NULL, cross = 0, fit = TRUE, cache = 40,\n",
       "          tol = 0.001, shrinking = TRUE, ...,\n",
       "          na.action = na.omit)\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       x: a symbolic description of the model to be fit.  When not\n",
       "          using a formula x can be a matrix or vector containing the\n",
       "          training data or a kernel matrix of class ‘kernelMatrix’ of\n",
       "          the training data or a list of character vectors (for use\n",
       "          with the string kernel). Note, that the intercept is always\n",
       "          excluded, whether given in the formula or not.\n",
       "\n",
       "    data: an optional data frame containing the training data, when\n",
       "          using a formula.  By default the data is taken from the\n",
       "          environment which `ksvm' is called from.\n",
       "\n",
       "       y: a response vector with one label for each row/component of\n",
       "          ‘x’. Can be either a factor (for classification tasks) or a\n",
       "          numeric vector (for regression).\n",
       "\n",
       "  scaled: A logical vector indicating the variables to be scaled. If\n",
       "          ‘scaled’ is of length 1, the value is recycled as many times\n",
       "          as needed and all non-binary variables are scaled.  Per\n",
       "          default, data are scaled internally (both ‘x’ and ‘y’\n",
       "          variables) to zero mean and unit variance. The center and\n",
       "          scale values are returned and used for later predictions.\n",
       "\n",
       "    type: ‘ksvm’ can be used for classification , for regression, or\n",
       "          for novelty detection.  Depending on whether ‘y’ is a factor\n",
       "          or not, the default setting for ‘type’ is ‘C-svc’ or\n",
       "          ‘eps-svr’, respectively, but can be overwritten by setting an\n",
       "          explicit value.\n",
       "          Valid options are:\n",
       "\n",
       "            • ‘C-svc’ C classification\n",
       "\n",
       "            • ‘nu-svc’ nu classification\n",
       "\n",
       "            • ‘C-bsvc’ bound-constraint svm classification\n",
       "\n",
       "            • ‘spoc-svc’ Crammer, Singer native multi-class\n",
       "\n",
       "            • ‘kbb-svc’ Weston, Watkins native multi-class\n",
       "\n",
       "            • ‘one-svc’ novelty detection\n",
       "\n",
       "            • ‘eps-svr’ epsilon regression\n",
       "\n",
       "            • ‘nu-svr’ nu regression\n",
       "\n",
       "            • ‘eps-bsvr’ bound-constraint svm regression\n",
       "\n",
       "  kernel: the kernel function used in training and predicting.  This\n",
       "          parameter can be set to any function, of class kernel, which\n",
       "          computes the inner product in feature space between two\n",
       "          vector arguments (see ‘kernels’).\n",
       "          kernlab provides the most popular kernel functions which can\n",
       "          be used by setting the kernel parameter to the following\n",
       "          strings:\n",
       "\n",
       "            • ‘rbfdot’ Radial Basis kernel \"Gaussian\"\n",
       "\n",
       "            • ‘polydot’ Polynomial kernel\n",
       "\n",
       "            • ‘vanilladot’ Linear kernel\n",
       "\n",
       "            • ‘tanhdot’ Hyperbolic tangent kernel\n",
       "\n",
       "            • ‘laplacedot’ Laplacian kernel\n",
       "\n",
       "            • ‘besseldot’ Bessel kernel\n",
       "\n",
       "            • ‘anovadot’ ANOVA RBF kernel\n",
       "\n",
       "            • ‘splinedot’ Spline kernel\n",
       "\n",
       "            • ‘stringdot’ String kernel\n",
       "\n",
       "          Setting the kernel parameter to \"matrix\" treats ‘x’ as a\n",
       "          kernel matrix calling the ‘kernelMatrix’ interface.\n",
       "          The kernel parameter can also be set to a user defined\n",
       "          function of class kernel by passing the function name as an\n",
       "          argument.\n",
       "\n",
       "    kpar: the list of hyper-parameters (kernel parameters).  This is a\n",
       "          list which contains the parameters to be used with the kernel\n",
       "          function. For valid parameters for existing kernels are :\n",
       "\n",
       "            • ‘sigma’ inverse kernel width for the Radial Basis kernel\n",
       "              function \"rbfdot\" and the Laplacian kernel \"laplacedot\".\n",
       "\n",
       "            • ‘degree, scale, offset’ for the Polynomial kernel\n",
       "              \"polydot\"\n",
       "\n",
       "            • ‘scale, offset’ for the Hyperbolic tangent kernel\n",
       "              function \"tanhdot\"\n",
       "\n",
       "            • ‘sigma, order, degree’ for the Bessel kernel \"besseldot\".\n",
       "\n",
       "            • ‘sigma, degree’ for the ANOVA kernel \"anovadot\".\n",
       "\n",
       "            • ‘length, lambda, normalized’ for the \"stringdot\" kernel\n",
       "              where length is the length of the strings considered,\n",
       "              lambda the decay factor and normalized a logical\n",
       "              parameter determining if the kernel evaluations should be\n",
       "              normalized.\n",
       "\n",
       "          Hyper-parameters for user defined kernels can be passed\n",
       "          through the kpar parameter as well. In the case of a Radial\n",
       "          Basis kernel function (Gaussian) kpar can also be set to the\n",
       "          string \"automatic\" which uses the heuristics in ‘sigest’ to\n",
       "          calculate a good ‘sigma’ value for the Gaussian RBF or\n",
       "          Laplace kernel, from the data.  (default = \"automatic\").\n",
       "\n",
       "       C: cost of constraints violation (default: 1) this is the\n",
       "          `C'-constant of the regularization term in the Lagrange\n",
       "          formulation.\n",
       "\n",
       "      nu: parameter needed for ‘nu-svc’, ‘one-svc’, and ‘nu-svr’. The\n",
       "          ‘nu’ parameter sets the upper bound on the training error and\n",
       "          the lower bound on the fraction of data points to become\n",
       "          Support Vectors (default: 0.2).\n",
       "\n",
       " epsilon: epsilon in the insensitive-loss function used for ‘eps-svr’,\n",
       "          ‘nu-svr’ and ‘eps-bsvm’ (default: 0.1)\n",
       "\n",
       "prob.model: if set to ‘TRUE’ builds a model for calculating class\n",
       "          probabilities or in case of regression, calculates the\n",
       "          scaling parameter of the Laplacian distribution fitted on the\n",
       "          residuals.  Fitting is done on output data created by\n",
       "          performing a 3-fold cross-validation on the training data.\n",
       "          For details see references. (default: ‘FALSE’)\n",
       "\n",
       "class.weights: a named vector of weights for the different classes,\n",
       "          used for asymmetric class sizes. Not all factor levels have\n",
       "          to be supplied (default weight: 1). All components have to be\n",
       "          named.\n",
       "\n",
       "   cache: cache memory in MB (default 40)\n",
       "\n",
       "     tol: tolerance of termination criterion (default: 0.001)\n",
       "\n",
       "shrinking: option whether to use the shrinking-heuristics (default:\n",
       "          ‘TRUE’)\n",
       "\n",
       "   cross: if a integer value k>0 is specified, a k-fold cross\n",
       "          validation on the training data is performed to assess the\n",
       "          quality of the model: the accuracy rate for classification\n",
       "          and the Mean Squared Error for regression\n",
       "\n",
       "     fit: indicates whether the fitted values should be computed and\n",
       "          included in the model or not (default: ‘TRUE’)\n",
       "\n",
       "     ...: additional parameters for the low level fitting function\n",
       "\n",
       "  subset: An index vector specifying the cases to be used in the\n",
       "          training sample.  (NOTE: If given, this argument must be\n",
       "          named.)\n",
       "\n",
       "na.action: A function to specify the action to be taken if ‘NA’s are\n",
       "          found. The default action is ‘na.omit’, which leads to\n",
       "          rejection of cases with missing values on any required\n",
       "          variable. An alternative is ‘na.fail’, which causes an error\n",
       "          if ‘NA’ cases are found. (NOTE: If given, this argument must\n",
       "          be named.)\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     ‘ksvm’ uses John Platt's SMO algorithm for solving the SVM QP\n",
       "     problem an most SVM formulations. On the ‘spoc-svc’, ‘kbb-svc’,\n",
       "     ‘C-bsvc’ and ‘eps-bsvr’ formulations a chunking algorithm based on\n",
       "     the TRON QP solver is used.\n",
       "     For multiclass-classification with k classes, k > 2, ‘ksvm’ uses\n",
       "     the `one-against-one'-approach, in which k(k-1)/2 binary\n",
       "     classifiers are trained; the appropriate class is found by a\n",
       "     voting scheme, The ‘spoc-svc’ and the ‘kbb-svc’ formulations deal\n",
       "     with the multiclass-classification problems by solving a single\n",
       "     quadratic problem involving all the classes.\n",
       "     If the predictor variables include factors, the formula interface\n",
       "     must be used to get a correct model matrix.\n",
       "     In classification when ‘prob.model’ is ‘TRUE’ a 3-fold cross\n",
       "     validation is performed on the data and a sigmoid function is\n",
       "     fitted on the resulting decision values f.  The data can be passed\n",
       "     to the ‘ksvm’ function in a ‘matrix’ or a ‘data.frame’, in\n",
       "     addition ‘ksvm’ also supports input in the form of a kernel matrix\n",
       "     of class ‘kernelMatrix’ or as a list of character vectors where a\n",
       "     string kernel has to be used.\n",
       "     The ‘plot’ function for binary classification ‘ksvm’ objects\n",
       "     displays a contour plot of the decision values with the\n",
       "     corresponding support vectors highlighted.\n",
       "     The predict function can return class probabilities for\n",
       "     classification problems by setting the ‘type’ parameter to\n",
       "     \"probabilities\".\n",
       "     The problem of model selection is partially addressed by an\n",
       "     empirical observation for the RBF kernels (Gaussian , Laplace)\n",
       "     where the optimal values of the sigma width parameter are shown to\n",
       "     lie in between the 0.1 and 0.9 quantile of the \\|x- x'\\|\n",
       "     statistics. When using an RBF kernel and setting ‘kpar’ to\n",
       "     \"automatic\", ‘ksvm’ uses the ‘sigest’ function to estimate the\n",
       "     quantiles and uses the median of the values.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     An S4 object of class ‘\"ksvm\"’ containing the fitted model,\n",
       "     Accessor functions can be used to access the slots of the object\n",
       "     (see examples) which include:\n",
       "\n",
       "   alpha: The resulting support vectors, (alpha vector) (possibly\n",
       "          scaled).\n",
       "\n",
       "alphaindex: The index of the resulting support vectors in the data\n",
       "          matrix. Note that this index refers to the pre-processed data\n",
       "          (after the possible effect of ‘na.omit’ and ‘subset’)\n",
       "\n",
       "    coef: The corresponding coefficients times the training labels.\n",
       "\n",
       "       b: The negative intercept.\n",
       "\n",
       "     nSV: The number of Support Vectors\n",
       "\n",
       "     obj: The value of the objective function. In case of\n",
       "          one-against-one classification this is a vector of values\n",
       "\n",
       "   error: Training error\n",
       "\n",
       "   cross: Cross validation error, (when cross > 0)\n",
       "\n",
       "prob.model: Contains the width of the Laplacian fitted on the residuals\n",
       "          in case of regression, or the parameters of the sigmoid\n",
       "          fitted on the decision values in case of classification.\n",
       "\n",
       "_\bN_\bo_\bt_\be:\n",
       "\n",
       "     Data is scaled internally by default, usually yielding better\n",
       "     results.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Alexandros Karatzoglou (SMO optimizers in C++ by Chih-Chung Chang\n",
       "     & Chih-Jen Lin)\n",
       "     <email: alexandros.karatzoglou@ci.tuwien.ac.at>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "        • Chang Chih-Chung, Lin Chih-Jen\n",
       "          _LIBSVM: a library for Support Vector Machines_\n",
       "          <URL: http://www.csie.ntu.edu.tw/~cjlin/libsvm>\n",
       "\n",
       "        • Chih-Wei Hsu, Chih-Jen Lin\n",
       "          _BSVM_ <URL: http://www.csie.ntu.edu.tw/~cjlin/bsvm/>\n",
       "\n",
       "        • J. Platt\n",
       "          _Probabilistic outputs for support vector machines and\n",
       "          comparison to regularized likelihood methods_\n",
       "          Advances in Large Margin Classifiers, A. Smola, P. Bartlett,\n",
       "          B. Schoelkopf and D. Schuurmans, Eds. Cambridge, MA: MIT\n",
       "          Press, 2000.\n",
       "          <URL:\n",
       "          http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>\n",
       "\n",
       "        • H.-T. Lin, C.-J. Lin and R. C. Weng\n",
       "          _A note on Platt's probabilistic outputs for support vector\n",
       "          machines_\n",
       "          <URL:\n",
       "          http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf>\n",
       "\n",
       "        • C.-W. Hsu and C.-J. Lin\n",
       "          _A comparison on methods for multi-class support vector\n",
       "          machines_\n",
       "          IEEE Transactions on Neural Networks, 13(2002) 415-425.\n",
       "          <URL:\n",
       "          http://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.ps.gz>\n",
       "\n",
       "        • K. Crammer, Y. Singer\n",
       "          _On the learnability and design of output codes for\n",
       "          multiclass prolems_\n",
       "          Computational Learning Theory, 35-46, 2000.\n",
       "          <URL:\n",
       "          http://webee.technion.ac.il/people/koby/publications/ecoc-mlj02.pdf>\n",
       "\n",
       "        • J. Weston, C. Watkins\n",
       "          _Multi-class support vector machines_\n",
       "          In M. Verleysen, Proceedings of ESANN99 Brussels, 1999\n",
       "          <URL: http://citeseer.ist.psu.edu/8884.html>\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘predict.ksvm’, ‘ksvm-class’, ‘couple’\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     ## simple example using the spam data set\n",
       "     data(spam)\n",
       "     \n",
       "     ## create test and training set\n",
       "     index <- sample(1:dim(spam)[1])\n",
       "     spamtrain <- spam[index[1:floor(dim(spam)[1]/2)], ]\n",
       "     spamtest <- spam[index[((ceiling(dim(spam)[1]/2)) + 1):dim(spam)[1]], ]\n",
       "     \n",
       "     ## train a support vector machine\n",
       "     filter <- ksvm(type~.,data=spamtrain,kernel=\"rbfdot\",\n",
       "                    kpar=list(sigma=0.05),C=5,cross=3)\n",
       "     filter\n",
       "     \n",
       "     ## predict mail type on the test set\n",
       "     mailtype <- predict(filter,spamtest[,-58])\n",
       "     \n",
       "     ## Check results\n",
       "     table(mailtype,spamtest[,58])\n",
       "     \n",
       "     \n",
       "     ## Another example with the famous iris data\n",
       "     data(iris)\n",
       "     \n",
       "     ## Create a kernel function using the build in rbfdot function\n",
       "     rbf <- rbfdot(sigma=0.1)\n",
       "     rbf\n",
       "     \n",
       "     ## train a bound constraint support vector machine\n",
       "     irismodel <- ksvm(Species~.,data=iris,type=\"C-bsvc\",\n",
       "                       kernel=rbf,C=10,prob.model=TRUE)\n",
       "     \n",
       "     irismodel\n",
       "     \n",
       "     ## get fitted values\n",
       "     fitted(irismodel)\n",
       "     \n",
       "     ## Test on the training set with probabilities as output\n",
       "     predict(irismodel, iris[,-5], type=\"probabilities\")\n",
       "     \n",
       "     \n",
       "     ## Demo of the plot function\n",
       "     x <- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))\n",
       "     y <- matrix(c(rep(1,60),rep(-1,60)))\n",
       "     \n",
       "     svp <- ksvm(x,y,type=\"C-svc\")\n",
       "     plot(svp,data=x)\n",
       "     \n",
       "     \n",
       "     ### Use kernelMatrix\n",
       "     K <- as.kernelMatrix(crossprod(t(x)))\n",
       "     \n",
       "     svp2 <- ksvm(K, y, type=\"C-svc\")\n",
       "     \n",
       "     svp2\n",
       "     \n",
       "     # test data\n",
       "     xtest <- rbind(matrix(rnorm(20),,2),matrix(rnorm(20,mean=3),,2))\n",
       "     # test kernel matrix i.e. inner/kernel product of test data with\n",
       "     # Support Vectors\n",
       "     \n",
       "     Ktest <- as.kernelMatrix(crossprod(t(xtest),t(x[SVindex(svp2), ])))\n",
       "     \n",
       "     predict(svp2, Ktest)\n",
       "     \n",
       "     \n",
       "     #### Use custom kernel \n",
       "     \n",
       "     k <- function(x,y) {(sum(x*y) +1)*exp(-0.001*sum((x-y)^2))}\n",
       "     class(k) <- \"kernel\"\n",
       "     \n",
       "     data(promotergene)\n",
       "     \n",
       "     ## train svm using custom kernel\n",
       "     gene <- ksvm(Class~.,data=promotergene[c(1:20, 80:100),],kernel=k,\n",
       "                  C=5,cross=5)\n",
       "     \n",
       "     gene\n",
       "     \n",
       "     \n",
       "     #### Use text with string kernels\n",
       "     data(reuters)\n",
       "     is(reuters)\n",
       "     tsv <- ksvm(reuters,rlabels,kernel=\"stringdot\",\n",
       "                 kpar=list(length=5),cross=3,C=10)\n",
       "     tsv\n",
       "     \n",
       "     \n",
       "     ## regression\n",
       "     # create data\n",
       "     x <- seq(-20,20,0.1)\n",
       "     y <- sin(x)/x + rnorm(401,sd=0.03)\n",
       "     \n",
       "     # train support vector machine\n",
       "     regm <- ksvm(x,y,epsilon=0.01,kpar=list(sigma=16),cross=3)\n",
       "     plot(x,y,type=\"l\")\n",
       "     lines(x,predict(regm,x),col=\"red\")\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "help(ksvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c4a001-e4db-4cf3-924a-b4be982ba267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed90cd7a-11a0-42f5-81b9-728d15e3de61",
   "metadata": {},
   "source": [
    "Используем метод опорных векторов для классификации сортов ириса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fef3449-9a8f-4d30-91b3-4c6a6e957219",
   "metadata": {},
   "outputs": [],
   "source": [
    "res<-ksvm(class~pl+pw+sl+sw, \n",
    "          data=D_teach, kernel = \"rbfdot\", \n",
    "            kpar = list(sigma = 0.05),\n",
    "            C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2eca7c6f-bc47-47bc-b7bb-4c22764b0e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 120 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predict.res..D_teach.</th><th scope=col>D_teach.class</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 120 × 2\n",
       "\\begin{tabular}{ll}\n",
       " predict.res..D\\_teach. & D\\_teach.class\\\\\n",
       " <fct> & <fct>\\\\\n",
       "\\hline\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t ⋮ & ⋮\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 120 × 2\n",
       "\n",
       "| predict.res..D_teach. &lt;fct&gt; | D_teach.class &lt;fct&gt; |\n",
       "|---|---|\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | virginica  |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | virginica  |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | virginica  |\n",
       "| ⋮ | ⋮ |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "\n"
      ],
      "text/plain": [
       "    predict.res..D_teach. D_teach.class\n",
       "1   virginica             virginica    \n",
       "2   virginica             virginica    \n",
       "3   setosa                setosa       \n",
       "4   setosa                setosa       \n",
       "5   setosa                setosa       \n",
       "6   versicolor            versicolor   \n",
       "7   versicolor            versicolor   \n",
       "8   versicolor            versicolor   \n",
       "9   setosa                setosa       \n",
       "10  versicolor            versicolor   \n",
       "11  setosa                setosa       \n",
       "12  virginica             virginica    \n",
       "13  setosa                setosa       \n",
       "14  virginica             virginica    \n",
       "15  setosa                setosa       \n",
       "16  setosa                setosa       \n",
       "17  setosa                setosa       \n",
       "18  virginica             virginica    \n",
       "19  versicolor            versicolor   \n",
       "20  versicolor            versicolor   \n",
       "21  versicolor            versicolor   \n",
       "22  versicolor            versicolor   \n",
       "23  setosa                setosa       \n",
       "24  versicolor            versicolor   \n",
       "25  setosa                setosa       \n",
       "26  setosa                setosa       \n",
       "27  virginica             versicolor   \n",
       "28  setosa                setosa       \n",
       "29  setosa                setosa       \n",
       "30  virginica             virginica    \n",
       "⋮   ⋮                     ⋮            \n",
       "91  versicolor            versicolor   \n",
       "92  setosa                setosa       \n",
       "93  versicolor            versicolor   \n",
       "94  virginica             virginica    \n",
       "95  versicolor            versicolor   \n",
       "96  virginica             virginica    \n",
       "97  virginica             virginica    \n",
       "98  setosa                setosa       \n",
       "99  setosa                setosa       \n",
       "100 virginica             virginica    \n",
       "101 virginica             virginica    \n",
       "102 versicolor            versicolor   \n",
       "103 virginica             virginica    \n",
       "104 versicolor            versicolor   \n",
       "105 virginica             virginica    \n",
       "106 virginica             virginica    \n",
       "107 virginica             virginica    \n",
       "108 setosa                setosa       \n",
       "109 versicolor            versicolor   \n",
       "110 versicolor            versicolor   \n",
       "111 setosa                setosa       \n",
       "112 setosa                setosa       \n",
       "113 versicolor            versicolor   \n",
       "114 virginica             virginica    \n",
       "115 versicolor            versicolor   \n",
       "116 versicolor            versicolor   \n",
       "117 versicolor            versicolor   \n",
       "118 setosa                setosa       \n",
       "119 versicolor            versicolor   \n",
       "120 virginica             virginica    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 30 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predict.res..D_test.</th><th scope=col>D_test.class</th></tr>\n",
       "\t<tr><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>setosa    </td><td>setosa    </td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>versicolor</td><td>versicolor</td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "\t<tr><td>virginica </td><td>virginica </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 30 × 2\n",
       "\\begin{tabular}{ll}\n",
       " predict.res..D\\_test. & D\\_test.class\\\\\n",
       " <fct> & <fct>\\\\\n",
       "\\hline\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t setosa     & setosa    \\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t versicolor & versicolor\\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\t virginica  & virginica \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 30 × 2\n",
       "\n",
       "| predict.res..D_test. &lt;fct&gt; | D_test.class &lt;fct&gt; |\n",
       "|---|---|\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| setosa     | setosa     |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| versicolor | versicolor |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "| virginica  | virginica  |\n",
       "\n"
      ],
      "text/plain": [
       "   predict.res..D_test. D_test.class\n",
       "1  setosa               setosa      \n",
       "2  setosa               setosa      \n",
       "3  setosa               setosa      \n",
       "4  setosa               setosa      \n",
       "5  setosa               setosa      \n",
       "6  setosa               setosa      \n",
       "7  setosa               setosa      \n",
       "8  versicolor           versicolor  \n",
       "9  versicolor           versicolor  \n",
       "10 versicolor           versicolor  \n",
       "11 versicolor           versicolor  \n",
       "12 versicolor           versicolor  \n",
       "13 versicolor           versicolor  \n",
       "14 virginica            versicolor  \n",
       "15 versicolor           versicolor  \n",
       "16 versicolor           versicolor  \n",
       "17 versicolor           versicolor  \n",
       "18 versicolor           versicolor  \n",
       "19 virginica            virginica   \n",
       "20 virginica            virginica   \n",
       "21 virginica            virginica   \n",
       "22 virginica            virginica   \n",
       "23 virginica            virginica   \n",
       "24 virginica            virginica   \n",
       "25 virginica            virginica   \n",
       "26 virginica            virginica   \n",
       "27 virginica            virginica   \n",
       "28 virginica            virginica   \n",
       "29 virginica            virginica   \n",
       "30 virginica            virginica   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "View(data.frame(predict(res, D_teach), D_teach$class))\n",
    "View(data.frame(predict(res, D_test), D_test$class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d194c-5f2a-4938-9fe2-ec6eebc33851",
   "metadata": {},
   "source": [
    "## Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e1207-78f1-4f99-8daf-275e21b10dcb",
   "metadata": {},
   "source": [
    "В настоящее время широкое применение в анализе данных находят методы Deep Learning, которые традиционно связывают с использованием нейросетевых технологий. \n",
    "\n",
    "Нейросети дают возможность получать достаточно точные прогнозные оценки там, где традиционные методы анализа оказываются ограниченными.\n",
    "\n",
    "В среде R  для этих целей можно использовать пакет neuralnet. \n",
    "\n",
    "При его подключении становится доступна функция neuralnet, которая имеет следующий формат:\n",
    "\n",
    "neuralnet(formula, data, hidden = 1, threshold = 0.01,\n",
    "  stepmax = 1e+05, rep = 1, startweights = NULL,\n",
    "  learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n",
    "  plus = 1.2), learningrate = NULL, lifesign = \"none\",\n",
    "  lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n",
    "  act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n",
    "  constant.weights = NULL, likelihood = FALSE)\n",
    "  \n",
    "где formula - символическое описание модели, которая строится нейросетью в процессе обучения;\n",
    "data – data.frame переменных, которые содержатся в формуле;\n",
    "\n",
    "hidden - вектор целых чисел, определяющий количество скрытых нейронов (вершин) в каждом слое (определяет архитектуру сети);\n",
    "\n",
    "threshold - числовое значение, определяющее порог частных производных функции ошибок в качестве критерия остановки;\n",
    "\n",
    "stepmax – максимальное количество шагов для обучения нейронной сети, достижение этого максимума приводит к остановке процесса обучения нейронной сети;\n",
    "\n",
    "rep  - количество повторов для обучения нейронной сети (фактически параметр мультистарта);\n",
    "\n",
    "startweights - вектор, содержащий начальные значения весов, если установлено значение NULL, то используется случайная инициализация;\n",
    "\n",
    "learningrate.limit - вектор или список, содержащий самый низкий и самый высокий предел скорости обучения, используется только для RPROP и GRPROP;\n",
    "\n",
    "learningrate.factor - вектор или список, содержащий коэффициенты умножения для верхней и нижней скорости обучения. Используется только для RPROP и GRPROP;\n",
    "\n",
    "learningrate - числовое значение, определяющее скорость обучения, используемую при традиционном обратном распространении, используется только для традиционного обратного распространения ошибки;\n",
    "\n",
    "lifesign - строка, определяющая, насколько подробно будет информация информация во время расчета нейронной сети, возможные значения 'none', 'minimal',  'full';\n",
    "\n",
    "lifesign.step - целое число, указывающее размер шага для печати минимального порога в режиме полного цикла;\n",
    "algorithm - строка, содержащая тип алгоритма для расчета нейронной сети. Возможны следующие типы: 'backprop', 'rprop +', 'rprop-', 'sag' или 'slr'. «backprop» относится к обратному распространению, «rprop +» и «rprop-» относятся к устойчивому обратному распространению с и без обратного отслеживания веса, в то время как «sag» и «slr» вызывают использование модифицированного глобально сходящегося алгоритма (grprop);\n",
    "err.fct - дифференцируемая функция, которая используется для вычисления ошибки, в качестве альтернативы можно использовать строки sse и ce, которые обозначают сумму квадратов ошибок и кросс-энтропию;\n",
    "act.fct - дифференцируемая функция, которая используется для сглаживания результата перекрестного произведения ковариации или нейронов и весов, кроме того, возможны строки, 'logistic' и 'tanh' для логистической функции и гиперболического тангенса;\n",
    "\n",
    "linear.output - если функцию активации act.fct не следует применять к выходным нейронам, значение параметра устанавливается в TRUE, в противном случае – FALSE;\n",
    "\n",
    "exclude - вектор или матрица, определяющая веса, которые исключаются из расчета (если задано как вектор, необходимо знать точное положение весов, матрица с n строками и 3 столбцами будет исключать n весов, где первый столбец обозначает слой, второй столбец - входной нейрон, а третий столбец - выходной нейрон веса);\n",
    "\n",
    "constant.weights - вектор, определяющий значения весов, которые исключаются из процесса обучения и рассматриваются как фиксированные;\n",
    "\n",
    "likelihood - если функция ошибок равна отрицательной функции логарифмического правдоподобия, будут вычислены информационные критерии AIC и BIC. \n",
    "\n",
    "Как видно из описания, большинство параметров имеют приемлемые значения по умолчанию, поэтому основными параметрами, подлежащими настройке являются:  formula, определяющая входные, выходные величины, data – определяющая набор исходных данных для обучения; hidden – задающий структуру нейросети.\n",
    "\n",
    "Рассмотрим использование нейросети сначала на простейших примерах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ecc061-8fce-46f6-8a36-48fc2650e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/mnt/38fd9072-993c-442e-b6f8-1d98878f17c7/juna/R/x86_64-pc-linux-gnu-library/3.6’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n",
      "also installing the dependency ‘Deriv’\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"neuralnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034946fd-f3ac-4ee6-88b7-23c0e65ec6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(neuralnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a36926a-482a-4e1c-96c9-78aa1ad9a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x<-1:100\n",
    "y<-sqrt(x)\n",
    "d<-data.frame(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95b96901-77fe-4cbb-b1a2-3b81cebc2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn<-neuralnet(y~x,data=d, hidden=c(5,4,3,5))\n",
    "plot(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0f9d43-62f7-4494-ae60-231c16116543",
   "metadata": {},
   "source": [
    "<img src=\"ris1.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fac9f0e0-2561-41cd-a0a5-b6d14a02018b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 1 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>predict.nn..xx.</th><th scope=col>X99.2</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>9.9548</td><td>9.95992</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 1 × 2\n",
       "\\begin{tabular}{ll}\n",
       " predict.nn..xx. & X99.2\\\\\n",
       " <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t 9.9548 & 9.95992\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 1 × 2\n",
       "\n",
       "| predict.nn..xx. &lt;dbl&gt; | X99.2 &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 9.9548 | 9.95992 |\n",
       "\n"
      ],
      "text/plain": [
       "  predict.nn..xx. X99.2  \n",
       "1 9.9548          9.95992"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx<-data.frame(99.2)\n",
    "View(data.frame(predict(nn,xx), sqrt(xx)))\n",
    "#compute(nn,xx)\n",
    "#sqrt(xx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b5854-39a8-4bb1-83ee-a57a11dad463",
   "metadata": {},
   "source": [
    "Используем нейросеть на примере данных о гибеле на Титанике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "572e2680-9867-4e2a-8a26-b8b3974d3906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 32 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>X</th><th scope=col>Class</th><th scope=col>Sex</th><th scope=col>Age</th><th scope=col>Survived</th><th scope=col>Freq</th></tr>\n",
       "\t<tr><th scope=col>&lt;int&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 1</td><td>1st </td><td>Male  </td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 2</td><td>2nd </td><td>Male  </td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 3</td><td>3rd </td><td>Male  </td><td>Child</td><td>No </td><td> 35</td></tr>\n",
       "\t<tr><td> 4</td><td>Crew</td><td>Male  </td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 5</td><td>1st </td><td>Female</td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 6</td><td>2nd </td><td>Female</td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 7</td><td>3rd </td><td>Female</td><td>Child</td><td>No </td><td> 17</td></tr>\n",
       "\t<tr><td> 8</td><td>Crew</td><td>Female</td><td>Child</td><td>No </td><td>  0</td></tr>\n",
       "\t<tr><td> 9</td><td>1st </td><td>Male  </td><td>Adult</td><td>No </td><td>118</td></tr>\n",
       "\t<tr><td>10</td><td>2nd </td><td>Male  </td><td>Adult</td><td>No </td><td>154</td></tr>\n",
       "\t<tr><td>11</td><td>3rd </td><td>Male  </td><td>Adult</td><td>No </td><td>387</td></tr>\n",
       "\t<tr><td>12</td><td>Crew</td><td>Male  </td><td>Adult</td><td>No </td><td>670</td></tr>\n",
       "\t<tr><td>13</td><td>1st </td><td>Female</td><td>Adult</td><td>No </td><td>  4</td></tr>\n",
       "\t<tr><td>14</td><td>2nd </td><td>Female</td><td>Adult</td><td>No </td><td> 13</td></tr>\n",
       "\t<tr><td>15</td><td>3rd </td><td>Female</td><td>Adult</td><td>No </td><td> 89</td></tr>\n",
       "\t<tr><td>16</td><td>Crew</td><td>Female</td><td>Adult</td><td>No </td><td>  3</td></tr>\n",
       "\t<tr><td>17</td><td>1st </td><td>Male  </td><td>Child</td><td>Yes</td><td>  5</td></tr>\n",
       "\t<tr><td>18</td><td>2nd </td><td>Male  </td><td>Child</td><td>Yes</td><td> 11</td></tr>\n",
       "\t<tr><td>19</td><td>3rd </td><td>Male  </td><td>Child</td><td>Yes</td><td> 13</td></tr>\n",
       "\t<tr><td>20</td><td>Crew</td><td>Male  </td><td>Child</td><td>Yes</td><td>  0</td></tr>\n",
       "\t<tr><td>21</td><td>1st </td><td>Female</td><td>Child</td><td>Yes</td><td>  1</td></tr>\n",
       "\t<tr><td>22</td><td>2nd </td><td>Female</td><td>Child</td><td>Yes</td><td> 13</td></tr>\n",
       "\t<tr><td>23</td><td>3rd </td><td>Female</td><td>Child</td><td>Yes</td><td> 14</td></tr>\n",
       "\t<tr><td>24</td><td>Crew</td><td>Female</td><td>Child</td><td>Yes</td><td>  0</td></tr>\n",
       "\t<tr><td>25</td><td>1st </td><td>Male  </td><td>Adult</td><td>Yes</td><td> 57</td></tr>\n",
       "\t<tr><td>26</td><td>2nd </td><td>Male  </td><td>Adult</td><td>Yes</td><td> 14</td></tr>\n",
       "\t<tr><td>27</td><td>3rd </td><td>Male  </td><td>Adult</td><td>Yes</td><td> 75</td></tr>\n",
       "\t<tr><td>28</td><td>Crew</td><td>Male  </td><td>Adult</td><td>Yes</td><td>192</td></tr>\n",
       "\t<tr><td>29</td><td>1st </td><td>Female</td><td>Adult</td><td>Yes</td><td>140</td></tr>\n",
       "\t<tr><td>30</td><td>2nd </td><td>Female</td><td>Adult</td><td>Yes</td><td> 80</td></tr>\n",
       "\t<tr><td>31</td><td>3rd </td><td>Female</td><td>Adult</td><td>Yes</td><td> 76</td></tr>\n",
       "\t<tr><td>32</td><td>Crew</td><td>Female</td><td>Adult</td><td>Yes</td><td> 20</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 32 × 6\n",
       "\\begin{tabular}{llllll}\n",
       " X & Class & Sex & Age & Survived & Freq\\\\\n",
       " <int> & <fct> & <fct> & <fct> & <fct> & <int>\\\\\n",
       "\\hline\n",
       "\t  1 & 1st  & Male   & Child & No  &   0\\\\\n",
       "\t  2 & 2nd  & Male   & Child & No  &   0\\\\\n",
       "\t  3 & 3rd  & Male   & Child & No  &  35\\\\\n",
       "\t  4 & Crew & Male   & Child & No  &   0\\\\\n",
       "\t  5 & 1st  & Female & Child & No  &   0\\\\\n",
       "\t  6 & 2nd  & Female & Child & No  &   0\\\\\n",
       "\t  7 & 3rd  & Female & Child & No  &  17\\\\\n",
       "\t  8 & Crew & Female & Child & No  &   0\\\\\n",
       "\t  9 & 1st  & Male   & Adult & No  & 118\\\\\n",
       "\t 10 & 2nd  & Male   & Adult & No  & 154\\\\\n",
       "\t 11 & 3rd  & Male   & Adult & No  & 387\\\\\n",
       "\t 12 & Crew & Male   & Adult & No  & 670\\\\\n",
       "\t 13 & 1st  & Female & Adult & No  &   4\\\\\n",
       "\t 14 & 2nd  & Female & Adult & No  &  13\\\\\n",
       "\t 15 & 3rd  & Female & Adult & No  &  89\\\\\n",
       "\t 16 & Crew & Female & Adult & No  &   3\\\\\n",
       "\t 17 & 1st  & Male   & Child & Yes &   5\\\\\n",
       "\t 18 & 2nd  & Male   & Child & Yes &  11\\\\\n",
       "\t 19 & 3rd  & Male   & Child & Yes &  13\\\\\n",
       "\t 20 & Crew & Male   & Child & Yes &   0\\\\\n",
       "\t 21 & 1st  & Female & Child & Yes &   1\\\\\n",
       "\t 22 & 2nd  & Female & Child & Yes &  13\\\\\n",
       "\t 23 & 3rd  & Female & Child & Yes &  14\\\\\n",
       "\t 24 & Crew & Female & Child & Yes &   0\\\\\n",
       "\t 25 & 1st  & Male   & Adult & Yes &  57\\\\\n",
       "\t 26 & 2nd  & Male   & Adult & Yes &  14\\\\\n",
       "\t 27 & 3rd  & Male   & Adult & Yes &  75\\\\\n",
       "\t 28 & Crew & Male   & Adult & Yes & 192\\\\\n",
       "\t 29 & 1st  & Female & Adult & Yes & 140\\\\\n",
       "\t 30 & 2nd  & Female & Adult & Yes &  80\\\\\n",
       "\t 31 & 3rd  & Female & Adult & Yes &  76\\\\\n",
       "\t 32 & Crew & Female & Adult & Yes &  20\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 32 × 6\n",
       "\n",
       "| X &lt;int&gt; | Class &lt;fct&gt; | Sex &lt;fct&gt; | Age &lt;fct&gt; | Survived &lt;fct&gt; | Freq &lt;int&gt; |\n",
       "|---|---|---|---|---|---|\n",
       "|  1 | 1st  | Male   | Child | No  |   0 |\n",
       "|  2 | 2nd  | Male   | Child | No  |   0 |\n",
       "|  3 | 3rd  | Male   | Child | No  |  35 |\n",
       "|  4 | Crew | Male   | Child | No  |   0 |\n",
       "|  5 | 1st  | Female | Child | No  |   0 |\n",
       "|  6 | 2nd  | Female | Child | No  |   0 |\n",
       "|  7 | 3rd  | Female | Child | No  |  17 |\n",
       "|  8 | Crew | Female | Child | No  |   0 |\n",
       "|  9 | 1st  | Male   | Adult | No  | 118 |\n",
       "| 10 | 2nd  | Male   | Adult | No  | 154 |\n",
       "| 11 | 3rd  | Male   | Adult | No  | 387 |\n",
       "| 12 | Crew | Male   | Adult | No  | 670 |\n",
       "| 13 | 1st  | Female | Adult | No  |   4 |\n",
       "| 14 | 2nd  | Female | Adult | No  |  13 |\n",
       "| 15 | 3rd  | Female | Adult | No  |  89 |\n",
       "| 16 | Crew | Female | Adult | No  |   3 |\n",
       "| 17 | 1st  | Male   | Child | Yes |   5 |\n",
       "| 18 | 2nd  | Male   | Child | Yes |  11 |\n",
       "| 19 | 3rd  | Male   | Child | Yes |  13 |\n",
       "| 20 | Crew | Male   | Child | Yes |   0 |\n",
       "| 21 | 1st  | Female | Child | Yes |   1 |\n",
       "| 22 | 2nd  | Female | Child | Yes |  13 |\n",
       "| 23 | 3rd  | Female | Child | Yes |  14 |\n",
       "| 24 | Crew | Female | Child | Yes |   0 |\n",
       "| 25 | 1st  | Male   | Adult | Yes |  57 |\n",
       "| 26 | 2nd  | Male   | Adult | Yes |  14 |\n",
       "| 27 | 3rd  | Male   | Adult | Yes |  75 |\n",
       "| 28 | Crew | Male   | Adult | Yes | 192 |\n",
       "| 29 | 1st  | Female | Adult | Yes | 140 |\n",
       "| 30 | 2nd  | Female | Adult | Yes |  80 |\n",
       "| 31 | 3rd  | Female | Adult | Yes |  76 |\n",
       "| 32 | Crew | Female | Adult | Yes |  20 |\n",
       "\n"
      ],
      "text/plain": [
       "   X  Class Sex    Age   Survived Freq\n",
       "1   1 1st   Male   Child No         0 \n",
       "2   2 2nd   Male   Child No         0 \n",
       "3   3 3rd   Male   Child No        35 \n",
       "4   4 Crew  Male   Child No         0 \n",
       "5   5 1st   Female Child No         0 \n",
       "6   6 2nd   Female Child No         0 \n",
       "7   7 3rd   Female Child No        17 \n",
       "8   8 Crew  Female Child No         0 \n",
       "9   9 1st   Male   Adult No       118 \n",
       "10 10 2nd   Male   Adult No       154 \n",
       "11 11 3rd   Male   Adult No       387 \n",
       "12 12 Crew  Male   Adult No       670 \n",
       "13 13 1st   Female Adult No         4 \n",
       "14 14 2nd   Female Adult No        13 \n",
       "15 15 3rd   Female Adult No        89 \n",
       "16 16 Crew  Female Adult No         3 \n",
       "17 17 1st   Male   Child Yes        5 \n",
       "18 18 2nd   Male   Child Yes       11 \n",
       "19 19 3rd   Male   Child Yes       13 \n",
       "20 20 Crew  Male   Child Yes        0 \n",
       "21 21 1st   Female Child Yes        1 \n",
       "22 22 2nd   Female Child Yes       13 \n",
       "23 23 3rd   Female Child Yes       14 \n",
       "24 24 Crew  Female Child Yes        0 \n",
       "25 25 1st   Male   Adult Yes       57 \n",
       "26 26 2nd   Male   Adult Yes       14 \n",
       "27 27 3rd   Male   Adult Yes       75 \n",
       "28 28 Crew  Male   Adult Yes      192 \n",
       "29 29 1st   Female Adult Yes      140 \n",
       "30 30 2nd   Female Adult Yes       80 \n",
       "31 31 3rd   Female Adult Yes       76 \n",
       "32 32 Crew  Female Adult Yes       20 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Tit<-read.csv(\"Titanic.csv\")\n",
    "View(Tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b711f058-8bbc-421a-b652-0f5366e1e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Формирование обучающей и тестовой выборок\n",
    "Titr<-data.frame(rank(Tit$Class), rank(Tit$Sex), rank(Tit$Age), rank(Tit$Survived), Tit$Freq)\n",
    "colnames(Titr)<-c('Class','Sex','Age','Survived','Freq')\n",
    "x<-c(1:32)\n",
    "y<-sample(x,floor(32*0.8))\n",
    "z<-setdiff(x,y)\n",
    "D_teach<-Titr[y,]\n",
    "D_test<-Titr[z,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2272b6d-1208-4a0e-a7f8-f6aacaa03666",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn<-neuralnet(Survived~Class+Sex+Age+Freq,data=D_teach, hidden=c(5,4,3,5), linear.output=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fe9e3e-89e4-4ea5-a4aa-f43b8779cd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 25 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>predict.nn..D_teach.</th><th scope=col>D_teach.Survived</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>31</th><td>24.500170</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>24.538982</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>15</th><td> 8.499951</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>24.502361</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>16.500439</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>2</th><td> 8.500033</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>20</th><td>16.499046</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>16</th><td> 8.500454</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>14</th><td> 8.499990</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>30</th><td>24.499903</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>29</th><td>24.499942</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>23</th><td>24.499980</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>24</th><td>16.500439</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>10</th><td> 8.501902</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>5</th><td> 8.501221</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>11</th><td> 8.499008</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>26</th><td>24.487650</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>13</th><td> 8.499904</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>24.499981</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>9</th><td> 8.500552</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>28</th><td>24.484004</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>12</th><td> 8.498607</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>16.499046</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>27</th><td>24.487656</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>1</th><td> 8.498320</td><td> 8.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 25 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & predict.nn..D\\_teach. & D\\_teach.Survived\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t31 & 24.500170 & 24.5\\\\\n",
       "\t19 & 24.538982 & 24.5\\\\\n",
       "\t15 &  8.499951 &  8.5\\\\\n",
       "\t17 & 24.502361 & 24.5\\\\\n",
       "\t8 & 16.500439 &  8.5\\\\\n",
       "\t2 &  8.500033 &  8.5\\\\\n",
       "\t20 & 16.499046 & 24.5\\\\\n",
       "\t16 &  8.500454 &  8.5\\\\\n",
       "\t14 &  8.499990 &  8.5\\\\\n",
       "\t30 & 24.499903 & 24.5\\\\\n",
       "\t29 & 24.499942 & 24.5\\\\\n",
       "\t23 & 24.499980 & 24.5\\\\\n",
       "\t24 & 16.500439 & 24.5\\\\\n",
       "\t10 &  8.501902 &  8.5\\\\\n",
       "\t5 &  8.501221 &  8.5\\\\\n",
       "\t11 &  8.499008 &  8.5\\\\\n",
       "\t26 & 24.487650 & 24.5\\\\\n",
       "\t13 &  8.499904 &  8.5\\\\\n",
       "\t21 & 24.499981 & 24.5\\\\\n",
       "\t9 &  8.500552 &  8.5\\\\\n",
       "\t28 & 24.484004 & 24.5\\\\\n",
       "\t12 &  8.498607 &  8.5\\\\\n",
       "\t4 & 16.499046 &  8.5\\\\\n",
       "\t27 & 24.487656 & 24.5\\\\\n",
       "\t1 &  8.498320 &  8.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 25 × 2\n",
       "\n",
       "| <!--/--> | predict.nn..D_teach. &lt;dbl&gt; | D_teach.Survived &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 31 | 24.500170 | 24.5 |\n",
       "| 19 | 24.538982 | 24.5 |\n",
       "| 15 |  8.499951 |  8.5 |\n",
       "| 17 | 24.502361 | 24.5 |\n",
       "| 8 | 16.500439 |  8.5 |\n",
       "| 2 |  8.500033 |  8.5 |\n",
       "| 20 | 16.499046 | 24.5 |\n",
       "| 16 |  8.500454 |  8.5 |\n",
       "| 14 |  8.499990 |  8.5 |\n",
       "| 30 | 24.499903 | 24.5 |\n",
       "| 29 | 24.499942 | 24.5 |\n",
       "| 23 | 24.499980 | 24.5 |\n",
       "| 24 | 16.500439 | 24.5 |\n",
       "| 10 |  8.501902 |  8.5 |\n",
       "| 5 |  8.501221 |  8.5 |\n",
       "| 11 |  8.499008 |  8.5 |\n",
       "| 26 | 24.487650 | 24.5 |\n",
       "| 13 |  8.499904 |  8.5 |\n",
       "| 21 | 24.499981 | 24.5 |\n",
       "| 9 |  8.500552 |  8.5 |\n",
       "| 28 | 24.484004 | 24.5 |\n",
       "| 12 |  8.498607 |  8.5 |\n",
       "| 4 | 16.499046 |  8.5 |\n",
       "| 27 | 24.487656 | 24.5 |\n",
       "| 1 |  8.498320 |  8.5 |\n",
       "\n"
      ],
      "text/plain": [
       "   predict.nn..D_teach. D_teach.Survived\n",
       "31 24.500170            24.5            \n",
       "19 24.538982            24.5            \n",
       "15  8.499951             8.5            \n",
       "17 24.502361            24.5            \n",
       "8  16.500439             8.5            \n",
       "2   8.500033             8.5            \n",
       "20 16.499046            24.5            \n",
       "16  8.500454             8.5            \n",
       "14  8.499990             8.5            \n",
       "30 24.499903            24.5            \n",
       "29 24.499942            24.5            \n",
       "23 24.499980            24.5            \n",
       "24 16.500439            24.5            \n",
       "10  8.501902             8.5            \n",
       "5   8.501221             8.5            \n",
       "11  8.499008             8.5            \n",
       "26 24.487650            24.5            \n",
       "13  8.499904             8.5            \n",
       "21 24.499981            24.5            \n",
       "9   8.500552             8.5            \n",
       "28 24.484004            24.5            \n",
       "12  8.498607             8.5            \n",
       "4  16.499046             8.5            \n",
       "27 24.487656            24.5            \n",
       "1   8.498320             8.5            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "View(data.frame(predict(nn,D_teach), D_teach$Survived))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a9e6016-4ded-4fe8-bded-02cddb80f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 7 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>predict.nn..D_test.</th><th scope=col>D_test.Survived</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>3</th><td>24.544327</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>16.500359</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>24.500025</td><td> 8.5</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>24.532381</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>24.501863</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>25</th><td> 8.967382</td><td>24.5</td></tr>\n",
       "\t<tr><th scope=row>32</th><td>27.716850</td><td>24.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 7 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & predict.nn..D\\_test. & D\\_test.Survived\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t3 & 24.544327 &  8.5\\\\\n",
       "\t6 & 16.500359 &  8.5\\\\\n",
       "\t7 & 24.500025 &  8.5\\\\\n",
       "\t18 & 24.532381 & 24.5\\\\\n",
       "\t22 & 24.501863 & 24.5\\\\\n",
       "\t25 &  8.967382 & 24.5\\\\\n",
       "\t32 & 27.716850 & 24.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 7 × 2\n",
       "\n",
       "| <!--/--> | predict.nn..D_test. &lt;dbl&gt; | D_test.Survived &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 3 | 24.544327 |  8.5 |\n",
       "| 6 | 16.500359 |  8.5 |\n",
       "| 7 | 24.500025 |  8.5 |\n",
       "| 18 | 24.532381 | 24.5 |\n",
       "| 22 | 24.501863 | 24.5 |\n",
       "| 25 |  8.967382 | 24.5 |\n",
       "| 32 | 27.716850 | 24.5 |\n",
       "\n"
      ],
      "text/plain": [
       "   predict.nn..D_test. D_test.Survived\n",
       "3  24.544327            8.5           \n",
       "6  16.500359            8.5           \n",
       "7  24.500025            8.5           \n",
       "18 24.532381           24.5           \n",
       "22 24.501863           24.5           \n",
       "25  8.967382           24.5           \n",
       "32 27.716850           24.5           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "View(data.frame(predict(nn,D_test), D_test$Survived))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc8500-943b-4e9a-8a8d-27215cd4defb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
